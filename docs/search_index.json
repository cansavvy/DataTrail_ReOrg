[["index.html", "DataTrail About this Course", " DataTrail January, 2023 About this Course This course is part of a series of courses for DataTrail. DataTrail is a no-cost, paid 14-week educational initiative for young-adult, high school and GED-graduates. DataTrail aims to equip members of underserved communities with the necessary skills and support required to work in the booming field of data science. DataTrail is a fresh take on workforce development that focuses on training both Black, Indigenous, and other people of color (BIPOC) interested in the data science industry and their potential employers. Offered by the Johns Hopkins Bloomberg School of Public Health, in partnership with local non-profits and Leanpub, DataTrail combines a mutually-intensive learning experience (MILE) with a whole-person ecosystem of support to allow aspiring data scientists and their employers to succeed. DataTrail uses mutually-intensive learning DataTrail joins aspiring data science scholars and expert-level data scientist mentors in a mutually-intensive learning experience (MILE). In the DataTrail MILE: Scholars engage in cutting-edge technical and soft skills training needed to enter the data science field. Mentors engage in anti-racism and mentorship training needed to be impactful mentors and informed colleagues on diverse data science teams. The social connections created along the way will fuel job opportunities for scholars and foster a more diverse, equitable, and inclusive climate at the mentors’ institutions. "],["intro-welcome-to-datatrail.html", "Chapter 1 Intro: Welcome to DataTrail 1.1 Learning objectives for this section", " Chapter 1 Intro: Welcome to DataTrail Hello and welcome! The goal of DataTrail is to help anyone with an internet connection and a computer learn to do data science. The program will start with the very basics of using a computer on the internet and work all the way up to doing data science and data analysis. We hope that by building this program we can help people get into the exciting tech world in one of the fastest growing and most satisfying jobs in the United States. There are only going to be more and more jobs asking for data science skills in the future. We believe that by making this career accessible to anyone we can have a positive impact on the world. 1.1 Learning objectives for this section In this section our goal is for you to be able to: Understand the components of this DataTrail course Describe what data science is Acknowledge the expectations and requirements for completing this course Navigate the basic use of your Chromebook and its operating system Setup the basic accounts you will need to complete this course 1.1.1 Course Details Before we jump into the content, we just wanted to orient you to how this course will be laid out: Sections - There are 8 major sections in this course that take you through another step of the data science process: Introduction to DataTrail (what you are looking at now) Forming Questions Getting Data Cleaning the Data Plotting the Data Sharing Results Building a resume Chapters - You’re looking at the first chapter here. It’s called “Welcome to DataTrail”. You can see a list of all the chapters (and the overall sections in bold) in this course in the left panel. The chapters will contain text and images to walk you through every chapters of each course. Quizzes - Most chapters (when looking at this in Leanpub) will have a quiz to evaluate your understanding of the material in that chapter. Successful completion of these quizzes is required for receipt of the certificate at the end of the DataTrail course. Quizzes often also have an interactive component called swirl which we will introduce you to. Projects - At the end of each section, there will be a project that will require you to turn in some code. The main objective of these projects is to have you try setting up an analysis. If you are at all struggling with completing these projects reach out. The main objective is that you attempt to use what we have learned in the section. If you get stuck, that is okay, just make sure to turn in the work that you have attempted. 1.1.2 What is data science? You might not think about data very much. Most people don’t. So why is data science such a popular and growing career? And what does data science even mean? One definition of data is any information that you can store on a computer. Examples of data that you produce all the time are text messages, Facebook posts, websites you visit, things you buy with a credit card, pictures of your car on speed cameras, and information you fill out in profiles for your work, school, or community organizations. If you can take a picture of it, write about it, make a video of it, or record it on audio - then it is probably data. All of these different kinds of data are collected and saved on a computer. It used to be that measuring and storing data was expensive and hard. Now it is easy and cheap. Governments, companies, organizations, and even individual people can now collect and store more data in a few days than the entire world collected over the last few centuries. Most of the time we don’t even think about the data we are collecting. We take pictures and post them to Facebook to show our grandparents, not because we want to analyze the data in the pictures. This is true for most of the data that we create and collect, both for ourselves and for companies. We don’t do it for the data - we do it because we want to record and share information about ourselves and the world. For example, companies do it so they can keep track of their customers, and governments do it because they want to keep records of who got parking tickets. But people started to figure out that you could use that data for other purposes. When you search for “symptoms of the flu” on Google, you are just looking for information because you are sick. But the data that you are searching for flu symptoms is valuable information for companies, doctors, and even scientists. We could use that data from you to do things like show you an ad for blankets or for flu medicine. We could also use data from you and everyone else who searched for flu symptoms to find out where there are lots of flu cases. Another example is social media. You might write a post on Facebook with pictures describing your child’s birthday party. You might do this so your child’s grandparents can see pictures of her birthday. But the information in that post can again be valuable for other people. We could figure out birthdays, hobbies, interests, and who knows who from Facebook posts and likes. That information can be valuable for showing ads, for suggesting other people you might know, or for studying how humans interact with each other on birthdays and holidays. Nonprofits use data science to better further their causes. For example, the Red Cross uses data science to find counties to target for smoke alarm installation campaigns. But to make data valuable we need to be able to study it and separate the interesting facts (called the “signal”) from the uninteresting information (the “noise”). One definition for data science is that. “Data science is asking a question that can be answered with data, collecting and cleaning the data, studying the data, creating models to help understand and answer the question, and sharing the answer to the question with other people.” The reason this field is growing so fast is that nearly every government, company, and organization is now collecting data. As the data have become cheaper and cheaper, the ability to analyze that data and find useful information has become a more and more valuable skill. But most people don’t have training or experience sifting through big piles of data to make interesting and valuable discoveries. The people who can do this well are called data scientists. They have a job that is exciting, interesting, and promises to be in high demand for years to come. 1.1.3 What is DataTrail? Data science is a fast-growing and exciting profession. But it can be a challenge to get into this career. Some of the biggest challenges are: Finding out that data science is even a real career Getting an education in data science can be costly and inconvenient You usually need a background in math, statistics, or computer science The equipment for data science can be costly and difficult to set up Many of the jobs are only available in major cities Most of the people who are currently data scientists have degrees in math, statistics, physics. They can afford computers that cost thousands of dollars and specialized computing software to help them do their jobs. They also mostly live in a few major cities like New York, San Francisco, Seattle, and Washington D.C. Many of these data scientists are former software engineers or other white-collar workers who moved into data science when they saw the demand for this kind of job. It is our goal with DataTrail to try to help people who would otherwise not have access to this exciting career to get into the career. To do that we need to remove some of the challenges above. So we designed this program to tackle some of the challenges that are preventing more widespread adoption of this career. DataTrail is being released as a set of online courses with a pay-what-you-can model. That means you can take the whole series of courses for free or for whatever cost you can afford. DataTrail is designed to be done entirely online using only tools you can access from a web browser. This means that you can do the entire program on a Chromebook - which you can get for as little as $150. DataTrail starts at the very basics of how to set up all of your accounts, which websites and apps to use, and straightforward projects that anyone can do. The only pre-requisites are high school math/reading and the ability to use a computer. DataTrail includes resources for finding, getting, and working at data science jobs. It also includes resources for finding and working at remote data science jobs that can be done from anywhere in the world. 1.1.4 Who is this program for? DataTrail is designed for people who have a high school education and know how to use a computer. Some people who we hope the program will be useful for are: High school students People who are working on or have completed a high school education Students at community colleges Older adults who want to learn something new But the program can be completed by anyone! We hope that it will be useful for anyone who wants to learn something new about data science. This program is also focused on people who want to learn to do data science. In some cases this program may not be the most efficient way to learn about data science. If you already have a background in statistics, math, or computer science and want to jump directly to more advanced topics we have already created a Data Science Specialization on Coursera just for you. There are many jobs that require people to understand or manage a data science project. If you are a leader or executive who just wants a high level overview of what data science is all about, we have also created an Executive Data Science Specialization. Our goal here is also to create a supportive and inclusive learning experience. Data science is frustrating and slow to learn. Often the best way is to learn from other people who have discovered similar solutions or made similar mistakes. Fortunately, there are communities in data science that are cheerful, friendly and willing to help new people get involved. Throughout the program we will introduce you to these communities and hope that you will also make an effort to help your fellow students as they discover this exciting field. 1.1.5 How the program is organized This course is designed to be used in many different ways so they can be useful for the most people possible. The courses and projects can be completed entirely online using nothing more than a web browser. To keep up on the latest information about the program, courses and more go to https://www.DataTrail.org/. 1.1.6 How DataTrail is graded Most chapters will have a short quiz at the end. To pass the course you need to get 70% of the quiz questions in the course correct. Additionally, if you are taking this course as a part of a datatrail cohort, you will be required to turn in your projects for credit. If you receive more than 90% of the points across all quizzes and turn in completed projects you will pass with honors. "],["finding-help.html", "Chapter 2 Finding Help", " Chapter 2 Finding Help In data science and computer-related work in general, it is common to ask for help multiple times per day. While sometimes we ask our colleagues for help in-person, most of the time we search the web. Throughout the coursework it may surprise you just how frequently other people have run into the same problem or had the exact same question you have. Often, there is an answer that was publicly shared previously on the Internet that can help answer the very question you’re asking. There are a number of websites and discussion boards where people frequently ask and answer questions. By knowing how to effectively search the web, you can easily find these answers. 2.0.1 Searching the Internet You’ve been working on the Internet through this coursework so far on the Chrome Browser? Within every Internet browser, you have access to web search engines. These are designed to find the most relevant answers to our question. The most common web search engine is Google. In fact, Google started as a web search engine before it developed any of the other many products it offers today. We can access Google by typing www.google.com in the search bar at the top of the Chrome browser. This will bring you to the Google homepage, where you will see a simple text box and a button called Google search. Google search On the search box, as you start typing your question you will see suggestions based on what you have written so far. This is called Google auto-complete. Here is an example where Google suggests a few common searches that start with “how to find help in”. Google auto-complete The auto-complete feature can be useful because it helps us refine our search query which will lead to more relevant results and answers. Throughout this course work, we’ll be using the R programming language to complete data analyses. Thus, you will often be searching for help related to the R programming language. So, in this example, let’s select “how to find help in r” and then click on the Google search button, we will get a list of websites that are most related to our question as shown below. Google search results Google highlights some of our key terms from our search in the search results list. For example, the word help is bolded twice on the first link title “R: Getting Help with R”. Each search result includes a short title, the web link, a short extract from the website, and some of our search terms (words) highlighted. Using this information we have to decide if our search was specific enough. For example, we could have searched “how to get help”. Google search would have had no way of knowing that we had an R question specifically. Alternatively, searching “how to get help for all the questions I’ve ever had or may have in the R programming language today or tomorrow” is also not ideal. Devising a search with the fewest words that help accurately answer your questions is the goal! We will cover different ways of finding help. Throughout this coursework, you’ll likely learn that part of being a data scientist means being good at Googling. Effectively searching the web is an important skill to have. 2.0.2 Search Guidelines The best way to get a response to your question is to be able to boil it down to relatively few words. Less is usually better…it’s also faster to type too! So, when you’re Googling things, keep a few things in mind: Use the fewest words possible - full sentences and correct grammar are not necessary when searching google Be Specific - include words that are important to your specific search Know specific websites where you can get help - while Google is generally a great place to start, sometimes it can be helpful to know specific websites where you can get help. StackOverflow, Basecamp, and the RStudio Community will likely be helpful places as you learn to program in R. These resources will be covered in detail in a future course; however, it’s good at this point to know they exist "],["understanding-your-chromebook.html", "Chapter 3 Understanding Your Chromebook 3.1 What is a Chromebook? 3.2 Chrome OS 3.3 Chrome Browser 3.4 Android Apps 3.5 Chromebook Shortcuts 3.6 Chromebook Security 3.7 Updating your Chromebook 3.8 Getting Help with a Chromebook", " Chapter 3 Understanding Your Chromebook This section is part of “DataTrail”; however, it is not a required. While not required for completion of the DataTrail course set on Leanpub, it may be very helpful to you! It’s not required because you don’t need a Chromebook to complete the courses. The only requirement for all of these courses is an Internet browser and connection to the web. However, if you have a Chromebook and want to get better acquainted with how it works, this section is for you! 3.1 What is a Chromebook? Chromebooks are a very specific type of computer produced by Google. Chromebooks aren’t exactly like normal computers and they have a few unique characteristics: They are usually very cheap They are designed mostly to use the web You don’t “install” any software on the computer itself Instead of “apps” and “software” you simply go to websites for your work A simple way to think about it is that a Chromebook is a computer that only lets you use an Internet browser like Chrome. You can’t really do much on the computer itself. Some people call this way of working - working only through the Internet - “cloud computing”. It’s called cloud computing because the computer you are using most of the time is not the one sitting in front of you. You are using the Internet to access tools and computers to do your work. But the physical computers doing the work are stored somewhere else - it could be nearby or on the other side of the globe. That is why people call the computers “in the cloud”. The goal of DataTrail is not that you have to use a Chromebook to finish the program, it is just that you could use a Chromebook to finish the whole program. You can finish the entire sequence of courses using any computer with an Internet connection and a web browser. Doing everything through a web browser has a couple of advantages for a training program like this: All Chromebooks have the same system so if you log out of one Chromebook and log onto another you will always see the same system (unless there has been an update to Chrome Operating System - “OS” - that affects all Chromebooks). Since everything is stored on the Internet (sometimes called “in the cloud”) then the Chromebook will always be set up with your preferences when you log off and log back on. Everyone is using the same software through the websites, so it reduces problems that come from different people using different versions of each computer program on different operating systems. Since Chromebooks are cheap and widely used, we expect many of the students who work on the class will use actual Chromebooks. In this chapter we will show you a little bit about how Chromebooks work, focusing on the Chrome OS operating system. If you are used to using a “normal” computer it can take a little getting used to to adapt to a Chromebook. A couple of things that you will likely do differently are: File storage: You won’t have much room for files on the Chromebook itself, so you won’t store them on your laptop, you will store them on “the cloud” in Google Drive or Dropbox. Software: You won’t install any software on your Chromebook. You will instead make accounts on different websites or “web apps” that let you use tools through your web browser. For example you wouldn’t install a Twitter app on your computer, you’d browse to the Twitter website to use that software. If you restrict yourself to only working in web browsers, doing everything will feel a little weird at first. It is sort of like writing a Haiku - a type of poem with a very specific set of rules. When you first start writing these poems it can be difficult to remember the rules. But after you get used to it the rules begin to come much more naturally. In a similar way, using a web browser for everything becomes second nature after a period of “learning the rules”. Using Chromebooks to do data science is a relatively new idea and is only possible in the last few years. But increasingly the tools for doing data science are available through the web. It is more and more common to have to use the Internet to rent computers, do analyses, or distribute your results. So by learning to do data science on a Chromebook, you are preparing for a future where most analysis is done online. 3.2 Chrome OS Your Chromebook is a little different than most normal Desktop computers. The main difference is that almost everything will be done in a Chrome web browser. For the most part, we won’t use any software that saves data directly to your computer with a few small exceptions. This doesn’t mean you are limited in what you can do as a data scientist. Rather, your work will be done in the “cloud”, rather than on your Chromebook directly. When using a “normal” computer you usually download software that makes it possible to edit files, make presentations, use your calendar, or use social media. On a Chromebook there are three different ways that you can add new software to use. Chrome Apps: Are apps that you “add” to Chrome. The user interface is just the Chrome Browser. Very often these apps are actually just a link to a website that lets you do something like manage your calendar or write a document. But sometimes they have other offline functionality. Chrome Extensions: These are actually pieces of software that modify the way the Chrome Browser itself works. An example would be an ad blocker that prevents ads on webpages you visit in Chrome from being shown. Android Apps: Many modern Chromebooks now support installing Android Apps. These are the same apps that you would get if you had an Android phone. These apps won’t run inside of a web browser, but will function just like an app on your phone. You can think of all three of these extensions to Chrome OS as “Apps”. Not all Chromebooks support Android Apps and we want anyone to be able to complete the DataTrail Program if they have a Chrome web browser and an Internet connection. So we will mostly focus on Chrome Apps and Extensions for this course. But if you have Android Apps that you like on your phone and have a Chromebook that supports Android Apps then you can install those apps as well as we discuss in the next section. 3.2.1 Chrome “Apps” Chrome Apps are software programs that run in the web browser. They can be as simple as a link to a website where the app runs or they can be actual pieces of software that download and run on your local Chrome computer. When you set up your Chrome OS account and log in, there will be a number of Chrome Apps that are available to you by default. Some of the most useful ones are: Google Docs: for creating, writing and editing formatted text documents (similar to Microsoft Word, but through the web browser) Google Slides: for creating, writing and editing presentations (similar to Microsoft Office, but through the web browser) Google Sheets: for creating, writing and editing spreadsheets (similar to Microsoft Excel, but through the web browser) Google Docs, Slides and Sheets are three Chrome Apps that come installed on your Chromebook We won’t go into too much detail about each of these apps here as we will cover them in a future class. For now we are just going to cover how to find these apps on your Chromebook, how to install and uninstall new ones, and how to organize them on your Chromebook. 3.2.2 Where do Chromebook Apps live? When you log into your Chromebook for the first time you can see icons for some apps at the bottom of the screen. This set of apps that are visible from the main screen are called your “Shelf”. The shelf is at the bottom of the screen Once you have installed a Chrome App or Android App you can find it by by clicking the Launcher button on the lower left hand side of the screen. If you don’t see the app among the first set of apps if you click on the arrow at the bottom of the screen you will be taken to a large list of apps installed on your Chromebook. You can click on the launcher button, then the arrow to see more apps Another way to open the screen with your apps is to click on the Launcher button on your keyboard. On most Chromebooks this is a button that looks like a magnifying glass. The keyboard launcher button looks like a magnifying glass Once you have found the App you want to open, click on it. When you click on a Chrome App the Chrome Browser will open and be directed to the website where that piece of software lives. For example if you click on the Google Docs icon (the blue piece of paper) then you will be directed to the website https://docs.google.com/ which lets you edit and work with your Google Docs. Clicking on the Google Docs icon will open Google Docs in the web browser. 3.2.3 The Shelf Your “Shelf” is the set of apps that are viewable on the bottom of your Desktop. You can use your Shelf to provide easy access to the Chrome Apps that you use most often. That way you don’t have to remember the exact web address for your most frequently used websites. You can “Pin” an app to the shelf by clicking with two fingers (sometimes called right clicking) on an app and then hovering over “Pin to Shelf” and letting go. For example you might want to pin your “Folder” with your local files to your Shelf so you can easily access it. Pin an app by right clicking You will then see the app on your Shelf so you can click on it to be taken directly to that Chrome App. The app is now pinned to your shelf. If you want to “Unpin” or remove an App from your Shelf you should right click on the app on the Shelf, hover over the Unpin command and let go. The App will then be removed from your shelf. The app is now pinned to your shelf. 3.2.4 Websites as Chrome Apps You can actually add any website you want to your shelf, not just the ones that have Chrome Apps. For example, we will be using RStudio Cloud for a lot of the work in this course. There is not a Chrome App for RStudio Cloud. But you can still add this website to your shelf. You can do this by first navigating to the website you want to add. Then clicking on the three dots in the upper right hand corner of the Chrome Browser. This will open up a menu and you can move your cursor until it hovers over More Tools. Then you can move your cursor over Add to shelf… and let go. Navigate to rstudio.cloud, move the cursor over More Tools, then Add to Shelf. This should then open up a window where you can name the website you are linking to. Uncheck the box that says Open as Window so that your Chromebook knows this is a website you want to open. Then click Add to put the icon onto your shelf. Uncheck Open as Window and click Add to add this website as an app. Once you are done you will see an icon for RStudio Cloud on your shelf so you will be able to quickly navigate to this web app in the future. The rstudio.cloud icon now appears on your shelf. 3.2.5 The Chrome Web Store and Chrome Apps Chrome Apps are pieces of software that can be run from the web browser. Sometimes you will need to download and install them on your Chromebook and sometimes they will just be an icon with a link to a particular website that lets you do some function. We already talked about how you can install any webpage as a Chrome App directly. But many Chrome Apps can also be found in the Chrome Web Store. This is a website that lets you search and find Chrome Apps for your Chromebook, similar to Google Play for Android Apps or the App Store for Apple apps. You can get to the Chrome Web Store by clicking on the launcher button, then finding and clicking the Web Store icon. Open the Chrome Web Store by clicking on the Web Store Icon. You will be taken to the Chrome Web Store in your browser. You can filter by whether they are free, built by Google, or are able to run offline. The first thing we will do is click on Apps to ensure that we are searching only among Chrome Apps. As an example we will find and install the StackEdit app, which is useful for writing markdown documents that you will use throughout the DataTrail courses. One nice feature of StackEdit is that it can be used even when your Chromebook is offline. To find the app, we will search for StackEdit and then click return. Search for stackedit on the Chrome Web Store. This will show two options one as a Chrome App and one as a Chrome Extension. We will select the app version and click Add to Chrome. This will bring up a dialogue box that asks if you would like to add Stackedit. Click on Add and the app will be installed on your Chromebook. Click add to Chrome and then Add to add the app to your Chrome browser. You can then click the Launcher button in the lower left to see that the app has been installed. StackEdit is now installed. If you click on the StackEdit icon you’ll be taken to the app, which you can see is also run in the Chrome web browser, just like any website you would visit. The difference is that this website will let you write and save Markdown files. StackEdit runs in the web browser. 3.2.6 Chrome Extensions Chrome extensions are software programs that modify the way that the Chrome browser itself works. You can find and install them through the Chrome Web Store just like you can with Chrome Apps but they look and function slightly differently. Sometimes these extensions may do something very simple (like show you a funny picture when you open a new tab) or they might do something very important (like managing your passwords). As an example we will install the Pocket Chrome extension. Pocket is an extension that lets you save a particular website or essay for reading when your Chromebook is offline. So if you know you will need to read a document you can save it to Pocket before you are traveling somewhere where you will not have Internet access and then retrieve and read it even when you are offline. You can open the Chrome Web Store just as we did previously and search for Pocket under the Extensions window. Once you have found the extension you want, you can click on the extension and it will open a page where you can click on Add to Chrome if you do that and click add the extension will be added. Click Add to Chrome to add the Pocket extension. Chrome Extensions often show up as a small icon in the top right hand side of the Chrome Browser. You can click on the icon to bring up the Pocket extension - which will also come up as a web page in your web browser. The Pocket extension comes up as a web browser. If you sign up for the Pocket extension you can then navigate to a web page that you are interested in saving for later and click the Pocket icon in the top right hand side of the screen. Click the Pocket icon to save the webpage This will save the page for later so that you can read it offline. The page is now saved for offline reading Extensions, like Chrome Web Apps, can improve the way that Chrome OS functions in the same way that installing software on your local computer works. Extensions and Apps help you use your Chromebook to accomplish tasks that would not otherwise be possible. 3.3 Chrome Browser A Chromebook is designed to primarily be an Internet browser. In other words, it mostly will only allow you to open up websites and navigate between them. Chromebooks use Chrome OS - this is like Windows on a Microsoft computer, Android on your phone, or OS X on your Apple computer. It is the main software that runs your computer. But unlike “normal” computers Chrome OS basically only allows you to use an Internet browser. Chromebooks use Google Chrome, Google’s Internet browser. When you are using a Chromebook, almost all of the time you will be interacting with the world through Chrome. When you have opened up your Chromebook you will see something like this. Chromebook opening screen The first thing you notice is that there is nothing on the “Desktop” and that all the “apps” are linked to from the bottom of the screen. This is called the “shelf” on a Chromebook. All the apps are basically just links to websites. To open a new Chrome Browser you can either click on the Chrome icon in the lower left of the screen or you can hold down the ctrl key and press the n key. We write this combination of keys as ctrl+n. If you do that you’ll see a new Chrome browser pop up. Chrome browser Here you can navigate to new websites and do all the usual things. At the bottom of the page are websites you visit frequently. In Chrome if you press the little square box (or boxes) in the top right hand side it will either maximize the size of a browser to fill the whole screen, or shrink it enough that you can drag the browser around. Chrome browser Click that box until the browser doesn’t fill the whole window. You can then create a second browser window by again using the ctrl+n command. Multiple Chrome browsers If you want to do more than one thing at once you can do it in multiple windows like this. But it is often easier to see what you are doing if you use tabs instead of windows. You can close one of the browser windows by clicking the X in the upper right hand corner or by holding down ctrl and pressing w. Closing a window With only one browser window open you can then open a new tab by holding down the ctrl key and pressing the t key (we will call this ctrl+t). Opening a new tab Now you can flip back and forth between tabs by clicking on the top of the tab in the browser. You can open more tabs by pressing ctrl+t again. You can close them by pressing the x in the top right corner of the tab or pressing ctrl+w. Closing a tab We’ll learn more about Chrome Apps later but for now if you press one of the other icons at the bottom of the screen then it will also open a Chrome window. The only difference is that it will go to a specific website for that icon. For example if you press the little blue paper icon you will open up a Google Doc. Open a Google Doc You can see that the Google Doc app is just another web browser but sent to a specific website - the website for Google Docs. Google Docs website Signing in to Chrome using your Gmail account automatically signs you in to all Google services including YouTube, Gmail, Google Drive, and others. This is particularly useful since you don’t have to sign in to websites that use Google services when you go to them with your web browser. For example if you click on the Gmail icon - the envelope - on your shelf in the bottom left corner of the screen you’ll be taken to the Gmail website and then logged in to your account. You are already logged into Gmail when you log into your Chromebook If you open a second tab by again clicking the Google Docs icon on your shelf in the bottom left corner of your screen, you’ll have two tabs open. You can separate the two tabs into two different windows by clicking on the top part of the Google Docs tab and dragging the Google Docs tab away from the Gmail tab. You can drag to separate tabs You can also put two windows together into a single window with multiple tabs. To do this, click on the tab at the top of the Google Docs window and drag it until it is close to the tab for Gmail. When you let go both tabs will go together into a single window. You can drag to combine tabs into a single window Finally, if you want to keep track of websites you use frequently you can use bookmarking. You can “bookmark” pages that you visit regularly and Chrome will keep track of those websites for you. It’s especially helpful if you don’t want to type the full address of a webpage, visit a page frequently, or are afraid you will forget about a page you will need later. To bookmark a webpage in Chrome, simply click on the star in the address bar or pressctrl+d on your keyboard. This will open the following window. Bookmark Menu If you click on “Done”, the bookmark will be saved to the Bookmarks bar. Next time you like to check the webpage you bookmarked without typing the address, you can click on the Chrome menu in the top-right corner, hover over Bookmarks, then click the bookmark you’d like to open. Bookmarks Those are the basics of how you manipulate windows and tabs in the Chrome browser. In the following lessons we’ll learn more about how you can use these windows to take advantage of the web through your Chromebook. 3.4 Android Apps Android is an operating system that was made available by Google and now powers most of the mobile phones in the world. Android is not the same thing as Chrome OS even though the two systems are both produced by Google. Until recently the only kind of apps you could install on a Chromebook were Apps and Extensions as we discussed in the last section. But Google has started to support the use of Android Apps on a small but growing number of Chromebooks. If you have one of these Chromebooks it is now possible to use the same apps you use on your phone on your Chromebook. Android Apps are now available on Chromebooks Chromebook Apps are stored in the same place on your Chromebook as Chrome Web Apps and Extensions. In fact, just by looking at the Shelf or your list of Apps you won’t be able to tell which are Chrome Apps and which are Android Apps. So to get started, we do the same thing as we did in the previous section, click on the Launcher button to show a list of potential apps. You can again click the arrow to view more apps if the Play Store does not appear among the first set of apps Find the Google Play App The first time you click on ‘Play Store’ will bring you to the Google Play Terms of Service. After reading the Terms of Service, if you agree to the Terms click on ‘Agree’ in the bottom right-hand side of the window. Google Play Terms of Service The following window will ask you to accept the Google Play Terms of Service. Here, you will need to click ‘ACCEPT’ to get to Google Play’s apps if you agree with the Terms of Service. You must accept terms of service to install Android Apps This will bring up Google Play where you can install Android Apps. This is the same place you would go if you have an Android Phone and you were going to install an App on your phone. The Google Play Store We will install the Basecamp app that can be used for communication with teams and is frequently used by data scientists. First we search for the app in the Google Play Store. Search for the Basecamp App When you find the Basecamp app you will be taken to a page where you can click Install to install the app. You may be asked to agree to terms and Google Play may ask you to enter payment details. However, if you are only going to use free apps you can find the small grey Skip box and skip over entering this information. Once you install the app it will appear as any other app on your Chromebook. Install the Basecamp App However one key difference is that if you click on the Basecamp icon and open the app then you will see that it does not open in a browser window. Click on the Basecamp icon to open Basecamp Android Apps are not Chrome Apps so are not accessed through the web browser. In this case, they will open in a separate window. Even though they don’t open in a web browser, many apps, including Basecamp, still require an Internet connection to be fully functional. Others may be more useful when used entirely offline. The app is working in a window, but not a Chrome Browser 3.5 Chromebook Shortcuts When we discussed the Chrome Browser we learned a couple of keyboard shortcuts. Keyboard shortcuts are combination of keys that make your Chromebook do certain functions. For example, if you hold the ctrl key and press the n key you get a new Chrome window. When we want to tell you to press this key combination we will use the symbols ctrl+n to describe what you are doing. In our Chrome Browser lesson we taught you the commands: ctrl+n - open a new Chrome window ctrl+t - open a new Chrome tab ctrl+w - close an open Chrome Window or tab ctrl+d - this will bookmark a page There are in fact a large number of keyboard shortcuts. Depending on what you do most often with your Chromebook some may be more or less useful to you. Here we will highlight a few that we use frequently and will come in handy for you during your data science career. To understand these key combinations first it is useful to know what some of the symbols on your Chromebook keyboard represent. The arrows in the top left hand corner take you back to the previous webpage you visited in a Chrome Tab and forward to the next page you visited. We will call them the forward and back keys. The back and forward keys are the arrows in the upper left of the keyboard. The curly key next to that refreshes the page you are currently visiting. Sometimes if you make an update to a webpage you are building and want to see the update you will need to refresh the page. We will call this the refresh key. The refresh key is the curly key in the upper left of the keyboard. The full screen key will take a tab you are working on and make that tab fill the whole screen. This will hide your shelf and hide the bookmarks and search bar at the top of your screen. When working on coding or when you feel you don’t have enough screen space this button can be helpful. You can shrink the browser window back to the normal size by pressing the full screen button again. We will call this the fullscreen key. The full screen key is the key with the window with two arrows in the upper middle of the keyboard. The button next to that is the show all windows button. This button will show you all the windows you have open, including Chrome Windows and any Android Apps you might have open. We will call this button the showwindows button. The show windows key is the key with the window and two lines next to it in the upper middle of the keyboard. The screen lock key in the upper right hand corner will lock your Chromebook when you aren’t using it and if you set up your settings to require a password, then you will need to re-enter your password to re-open your Chromebook. When working on data science projects where the data is sensitive it is a good idea to lock your Chromebook whenever you aren’t using it. We will call this the lock key. The lock key is the key with the lock in the upper right of the keyboard. You will recall that the Launcher key is the key with the magnifying glass on the left hand side of the keyboard. We will call this the launcher key. The launcher key is the magnifying glass on the right of the keyboard. 3.5.1 Taking Screenshots on a Chromebook Using these keys we can now consider some of the more useful keyboard shortcuts on your Chromebook. This will definitely be a partial list, you can learn about the rest of the shortcuts from the Chromebook help documentation. Taking screenshots on your Chromebook is very useful. You will use this when you are troubleshooting problems in the course frequently. There are two ways to take a screenshot. The first is to use the command ctrl+showwindows. If you use this key combination you will take a picture of everything that appears on the screen. This first will appear as a pop-up box in the lower right hand corner. The screenshot appears as a dialog in the lower right. You can also open the Files App and see that the screenshot has been stored on your computer. All screenshots you take will automatically stored in Files on your computer. The screenshot appears is stored in the Files App. If you click twice quickly on the screenshot file it will open and you can see it looks exactly like what was on your screen when you took the screenshot. The screenshot is a picture of whatever was on your screen. Sometimes, rather than taking a picture of the whole screen you will want to take a picture of just a small part of the screen. There is a second screenshot command that lets you do this. If you press ctrl+shift+showwindows then your screen will turn a shade darker and you will see your cursor is replaced with a bullseye icon. If you click and then drag, you can select only part of your screen to take a screenshot of. For example you could take a screenshot of just the upper right hand corner of the Files App. A targeted screenshot is also stored as a png in the Files App. This screenshot also will pop up in the lower right hand corner of your screen and then appear in your Files App. A targeted screenshot only takes a picture of part of the screen. 3.5.2 Zooming in and out Another set of useful shortcuts is to zoom your screen in and zoom your screen out. To zoom in, you can hold down ctrl and press the key that has the plus and equals on it (near the top right hand side of the keyboard). To zoom out, you can hold the ctrl key and press the key with the minus and underscore key on it (near the top right hand side of the keyboard). This will allow you to make the text on websites bigger and smaller to ease reading. For example on the website https://rstudio.cloud/ if you press ctrl+plus/equals four times you will zoom in 175%. If you use the zoom shortcut the text on a webpage increases in size. 3.5.3 Finding a word on a page Sometimes you will need to search a webpage for a specific word or phrase. To do this you can press ctrl+f and a box will pop up. You can then search for the specific word, for example we could search for the word “teach” on https://rstudio.cloud/. You can find a word or phrase on a page using the find shortcut. 3.5.4 Text formatting shortcuts Many of the text formatting shortcuts are nearly the same as on other types of computers. We will review them briefly here as they will be very useful when editing documents, presentations, and code. If you have used computers before you may already know these. ctrl+c - copies selected text ctrl+v - pastes the selected text ctrl+x - cuts the selected text and copies it so you can paste it elsewhere ctrl+z - undoes the previous action you did when editing a file One thing that is somewhat unusual about a Chromebook is that there is not a caps lock key. If you want to turn on caps lock you will need to use the command alt+launcher. To turn caps lock back off again you can simply click on the shift key. 3.6 Chromebook Security In some ways a Chromebook provides some security for you. Chrome OS has a number of security features that are constantly being improved by Google. We will discuss updating your Chromebook in a later section, but the updates ensure that at a basic level your Chromebook will have good security features. For example, data will be encrypted when passing through a web browser and each browser window will be sandboxed so a web app open in one browser window can’t access data from another window. But there are still some steps that you can take to make sure that your Chromebook is secure. This will be particularly important if your Chromebook is lost or stolen, but will also be useful if you share your computer with family or friends. Since a data scientist is often working with data that may be private or sensitive, it is important that if they lose their computer, they don’t risk losing the data they are working on. Setting your Chromebook up to be secure from the start is an important step for any data scientist. 3.6.1 Setting your Chromebook password One of the most important parts of securing your Chromebook is making sure that you have a good password. Good passwords are long, easy for you to remember, but hard for other people to guess. Don’t use simple passwords like “12345” or “password”, don’t use your name, your date of birth, or other details that would be easy to guess if someone saw your Facebook profile. Good passwords have a few important characteristics: They are long, ideally more than 10 characters They are easy for you to remember. They are not a commonly used password. One way to create relatively good passwords is to string together four or five random words. This approach was made popular by an xkcd cartoon. For example a password made with this approach would be “bikerainsmellblue”. This password has more than 10 characters, is easy for you to remember, and is unlikely to be used by a lot of other people. Try to pick words that aren’t related to each other. Then you only have to remember the four words to remember your password. xkcd suggested a good password combines four random words. Another way to create a password is to choose a line from a book or movie and choose the first one to three letters of each word. For example, you might use the line “Once upon a midnight dreary, while I pondered, weak and weary,” then you could change that to “onupamiddrwhipoweanwe”. You can then just remember the line that you have chosen and the fact that you used a certain number of letters from each word. Again, its a good idea to pick a line from a poem or a book that is less famous to do this. You can improve either of those password schemes by capitalizing some letters or turning words like “too” into numbers to make them more unique. The key is just to have a password that is not very common, easy to remember, and would be hard for other people to guess. 3.6.2 Requiring password to wake from sleep Now that you have set a good password you can make your Chromebook more secure by making sure that anyone will have to use the password to log in. To do this you need to again open up your personal settings by clicking on your personal avatar in the bottom of your screen and then click on the gear to open your personal settings. Open your personal settings Once you have your personal settings open you can click on the “Screen Lock” settings. Open the Screen Lock settings. You’ll be required to input your (hopefully good!) password. Input your password. Then you can turn on screen lock by clicking the option in the upper right hand corner of the screen for “Show screen lock when waking from sleep”. Click on option to Show screen lock when waking from sleep. Now if you close your Chromebook, put it to sleep, or log out, you will have to input your password to get access to your account. This will prevent other users from getting access to your Chromebook account even if they get a hold of the physical device. 3.6.3 Managing other people Another thing that you can do to make your Chromebook more secure is decide in advance who can log into your Chromebook. To do this, open your personal settings by clicking on the personal avatar at the bottom right hand side of your screen, then click the gear to open your personal settings. The click on the option to “Manage Other People”. Click on Manage Other People to manage users of your Chromebook. If you are the owner of the Chromebook you can then click on the dot next to “Restrict sign-in to the following users”. If you are the owner of the Chromebook will then be able to input which users can and can’t log on to your computer. Click on the option to restrict users. 3.6.4 Two step verification If you are dealing with very sensitive data or want to be more secure about who can access your account, you can turn on two step verification. All this means is that you can set it up so that entering your password is not enough to unlock your computer. You will also need to get a text message to your phone with a special code each time that you log in. This means that even if someone gets ahold of your Chromebook, they would also have to have your phone to be able to log into your account. Two step verification is often required for data scientists working in industry and is a good idea to prevent loss of your account information. To turn on two step verification first go to the website https://www.google.com/landing/2step and click on “Get Started” in the upper right hand corner of the screen. Go to the two step verification page and click Get Started. This will take you through the two step verification set up process. First you will again need to click “Get Started”. Click Get Started. Then you will be required to enter your password to confirm that it really is you trying to step two step verification. Enter your password. You will be asked to input your phone number. This is the phone number that will receive the text message every time you input your password and try to log in. This should be for your personal phone that you will have with you when you want to log in to your Chromebook. You won’t be able to log in without your phone after you set this up. Enter the phone number of the phone you will use for two step verification. Once you input your phone number, you will get a text message with a 6 digit number. Take this code and type it in to the next screen to confirm that you have the right phone number set up for two step verification. Enter your verification code to confirm your phone. After confirming both your password and your phone number you will be given the option to turn on two factor verification. You can do this by clicking “Turn On” on the next window. Turn on two factor authentication. Once this is turned on you can turn it back off by going through the same steps as before and then clicking “Turn Off” on the next screen. You can turn off two factor verification by clicking Turn Off. Finally, once you have enabled two step verification, you will need to have your phone with you every time you log in to your computer. This is good for security purposes, but will cause problems if you lose or replace your phone. Make sure you turn this feature back off if you are planning on getting a new phone. 3.6.5 Managing apps and devices You can monitor and manage which apps and devices have access to your account information from your account security page at https://myaccount.google.com/security . You should check this information periodically (every month or so at least) to see if there are any events or activity that you don’t recognize. If anyone has gotten access to your password or log in information, you will be able to see here when they have logged into your account. Your account security page. First you can see any security events that have occurred. Security events can be found by looking under “Device activity &amp; security events” in the section “Recent Security Events”. This will give you information on when your passwords are changed or when you have turned on or off two step verification. Your security events tell you about when your password or two step verification settings have changed. You can also see all the devices that have logged in to your Google account under “Device activity &amp; security events” in the section “Recently used devices”. You will see the Chromebook you are currently working on, but you will also see any Android phones you have logged in to or other computers where you signed into your account. Your recently used devices tells you what devices have logged into your account. As we have discussed in other sections, all of the software running on your Chromebook will be either Chrome Apps or Extensions or Android apps. For a lot of this software you will use Google to log in. That way you don’t have to have a separate password for each app and website. But each time you do this, you give the app a little bit of your information. When you stop using an app you may no longer want them to have access to your information. You can look and see which apps have access to your Google information by looking under “Apps with Account Access” in the section “Apps with Access to your Account”. You can see which apps have access to your account. If you click on a particular app you can take away the permission of that app to use your Google information. It is a good idea to check these apps from time to time and remove those that you no longer need, so they don’t have access to your Google information anymore. You can remove access for specific apps by clicking on them. 3.7 Updating your Chromebook In computing, there are often updates that are available. Updates can fix bugs, add new features, and/or improve security on your chromebook. Over time, your apps will need to be updated as well as your Chromebook’s operating system (OS) itself. To update your Chromebook you need internet connection. Every time you power on your Chromebook, it will check for and apply available updates. In fact, some updates to the Chrome OS happen in the background automatically while you’re using your device with the updates going into effect the next time you restart. However, should you ever need or want to manually update your Chromebook, you can do so using the following steps: 3.7.1 Steps to Update Step 1. Click on your username or photo at the bottom right-hand of the screen. Home Screen Step 2. A menu will display. On this menu, click on the settings button, which is the icon that looks like a gear turning. Settings Icon Step 3. A new window will appear. Here, you will click settings in the top left-hand portion of the screen. Settings Screen Step 4. On the menu that appears, click on ‘About Chrome OS’. About Chrome OS Step 5. From the ‘About Chrome OS’ window that is now displayed, click on ‘CHECK FOR UPDATES.’ CHECK FOR UPDATES Step 6. The progress of your update will be displayed in that window. Once the update has finished, a message will display letting you know that you are “Nearly up to date!” Nearly up to date! RESTART your device to finish updating To complete the update, you will have to restart your Chromebook. To do so, click ‘RESTART’ at the right of the window. RESTART to finish updating At this point, your screen will go black for a few seconds. Your Chromebook will then restart and you’ll be brought back to the login screen. Login back in here using your Google username and password. 3.8 Getting Help with a Chromebook Chromebooks have the advantage of being very simple. You are going to do almost everything through the Internet browser. This minimizes many issues that laptop computers had historically. However, there are still issues that you will run into from time to time. We’ll discuss them below and describe where you can find help for these problems. 3.8.1 Common Issues Common Chromebook Issues generally fall under one of the following categories: Operating Systems Issues Internet Connection Problems Google Account Login All of these issues are addressed among Google’s Chromebook help pages. Chromebook Support Page Below we’ll demonstrate how to step through finding answers among this documentation; however, a few good things to consider before going to the help documentation are: Did I type that web address correctly? Am I getting this error because of a mistake I made? If it is, fix that mistake and move on! Have I restarted recently (within the past week or two)? If not, now may be a good time to consider restarting to see if the issue still exists. If restarting doesn’t work or you can’t find a mistake, it may be time to consult the Chromebook Support documentation. We’ll walk through how to do that below. 3.8.2 Chromebook Support In addition to addressing common issues above, Google has a place where Chromebook users can go for support. At https://support.google.com/chromebook, you can search for answers to questions others have already asked or ask new questions. For example, if you were struggling to connect to the Internet on your Chromebook, you could first click on ‘Connect your Chromebook’ among the list of help topics. Connect your Chromebook This will expand the list to expose a number of topics of interest. If you were struggling to connect your Chromebook using Wi-Fi, you would click on ‘Connect your Chromebook to Wi-Fi’ Connect your Chromebook to Wi-Fi This would open a new web page with step-by-step instructions to help get you connected to wifi. Wi-Fi Connection Help Page If you don’t see what you’re looking for immediately among these topics, you can use the search bar at top. Search Bar By typing in the issue you are having in the search bar at the top, you can be directed to questions others have previously asked. For example, if you want to learn how to add an extension, you could type ’add extension in the search bar. As with most Google search bars, Google will try to provide helpful suggestions of topics you may be interested in. You can click on this if it matches what you’re interested in Add Extension Search Bar As before, a page with helpful steps to help solve your problem will show up. Adding Extensions Finally, if your topic isn’t here or doesn’t pop up, you can search additional questions asked in the ‘Help Forum.’ To access the help forum, click on ‘HELP FORUM’ at the top right hand of the page. Help Forum Button This will bring you to the Chromebook Central Help Forum. Chromebook Central Help Forum Always begin by searching for your topic to see if it has already been asked and answered using the search bar at the top. However, if you have a new question, you are welcome to start a new topic using the button at the top right. NEW TOPIC Clicking on the red ‘NEW TOPIC’ button will bring you to a new window where you can write out your specific question. Once you have asked your question as clearly and with as few words as possible, click ‘POST’ at the bottom right-hand of the screen so that others will be able to help you out by answering your question. New Topic Empty Box You’re now all up-to-date! "],["working-offline-vs-online.html", "Chapter 4 Working Offline vs Online 4.1 Where Are Files Stored?", " Chapter 4 Working Offline vs Online You may have heard the term cloud computing before, and data scientists often talk about working on the cloud. But what exactly is the cloud? Cloud storage refers to data or document storage on the Internet rather than on your personal computer. If you take pictures using your phone and then they are backed up on iCloud or Google Photos, you are using the cloud. Using the cloud for storage is like having an external hard drive (portable storage device) that you don’t ever see and can’t actually hold in your hands. Related to this, the term local means that it isn’t on the cloud but instead something stored on your computer or other device. 4.0.1 Cloud computing Cloud computing involves applications and software that run on shared data centers rather than running on the computer sitting in front of you. For data analysis, cloud computing has changed the way we think about working with data, especially when it comes to large datasets. A data analyst no longer needs to spend thousands of dollars to own high-capacity computers to deal with big data because the personal computer no longer has to do all the heavy lifting. Instead, a network of computers (from Amazon, IBM, or Microsoft among many others) will do the work instead. Your local computer will only need to run the interface software, which is often just your Internet browser. In future lessons we will study cloud-based data applications in more detail. 4.0.2 What are the advantages of using the cloud? A major advantage of the cloud is the ability to access your files everywhere, even if you don’t have your personal computer with you. Since storage and applications work over the Internet, they can be accessed from any computer with an Internet connection. An important advantage of cloud storage is that your files are safe even if your computer is lost or damaged. Because your files are not stored on your computer itself, they are safe and available even if your computer is stolen, you spill coffee on your keyboard, or there’s a natural disaster in your area. Moreover, most cloud storage services provide back-up services in case you delete files by mistake. These back-up services allow for accidentally deleted files to be recovered and restored. An advantage of cloud computing is an increase in computing power over what is available on your local machine. Remote machines that are used for cloud computing are more powerful than your personal computer and can do your data analysis much faster. Finally, working with the cloud puts the responsibility of maintaining software on the service provider rather than on you. When running software locally on your personal computer, you need to maintain applications by making sure that they still work and are up to date with the most recent versions. You must download and install the newest version of the software yourself or possibly even pay for a newer edition. With the cloud, service providers make sure the software is well-maintained and running optimally. 4.0.3 What are the disadvantages of using the cloud? Chromebooks are primarily meant to be used with an Internet connection. This is because most of the software you are using is through an Internet browser. So you will want to make sure you have Internet access when you are working on your Chromebook whenever possible. But sometimes you won’t have access to the Internet. Fortunately, some of the Chromebook Apps and functionality are available offline. You can use these to work when you can’t get an Internet connection. Then your changes will appear online when you reconnect. The most obvious disadvantage of working on the cloud is that you need an Internet connection to access storage and computing power. You cannot work “offline” away from the Internet. However, with wireless Internet service (wifi) available widely in libraries, coffee shops, and other public places, it’s possible to work on the cloud from almost anywhere! There are also concerns with the privacy and security of data that is stored remotely. Privacy and security are issues that must be addressed by both providers and users of cloud-based services. Service providers need to ensure that the files stored at their data centers are safe and secure. Users need to take advantage of authentication measures and use strong passwords to ensure that no one can gain access to their account. Most major cloud-based service providers do a good job in securing your data. Specifically, their infrastructure is set up so that you can avoid security issues by being serious about protecting your account access information through your choice of password and by using two-factor authentication for logins. Briefly, two-factor authentication is a way of proving your identity to a service provider in two steps. The first step that is by providing a password. The second step involves using a physical object in your possession, such as a phone, to prove your identity. For example, you may also need to enter a code that is sent to your phone during the login process. This means someone would need both your password and physical possession of your phone to access your account. It is good practice to chose two-factor authentication whenever it is offered by a service provider. Be careful when connecting to networks! It can be dangerous to connect to unknown or unverified networks. Only connect to networks that you recognize! 4.0.4 Google Drive As we discussed in a previous section, the Files App shows files both from your Google Drive and in a “Downloads” folder. All the files in your “Downloads” folder are saved directly on your Chromebook and are available even if you aren’t connected to the Internet. But you can also store files on your Google Drive. The advantage of Google Drive is that all your files are stored online. This means they won’t be lost if you lose your Chromebook and will be accessible from any Internet connected computer. However, sometimes when you don’t have access to the Internet you may want to access your files offline. You can make Google Drive files available offline on your Chromebook. To do so you need to open the Google Drive App on your Chromebook. You can find the Google Drive App by opening the apps using the launcher button and clicking on the Google Drive icon. To confirm that you are set to work offline open the Google Drive App Once you have opened Google Drive you can open the Settings in the upper right hand corner. Open the Google Drive Settings Then you can confirm that the box is checked next to the “Offline” setting. This means that all of your Google Slides presentations, Google Docs text files, and Google Sheets data files will be available offline. This will use up storage on your Chromebook, but will let you work even when your computer is not connected to the Internet. Confirm that the setting to work offline is turned on 4.0.5 Example: editing a text file offline One of the most common things you might want to do when you don’t have Internet access is to write in a text file. You can do this with a number of different apps. But the one we will use most frequently is Google Docs. You will learn more about Google Docs in a future course, but for now we will show you how offline editing works. First, click on the Google Docs icon to open up the Google Docs App. Note you may have to click on the launcher window to find this App if it is not pinned to your shelf. When Google Docs is open you can click on the plus sign to create a new document. Open Google Docs and create a new document Now you should add some text by typing in the Google Doc, this text will automatically be saved on the Google Doc online. Add some text to the Google Doc online To test offline editing you need to turn off wifi on your computer. You can do that by clicking the avatar picture in the bottom right hand side of the screen to bring up your global options. Then you can click on the wifi connection to open up the wifi options. Open your wifi options You can turn off wifi and your Internet connection by clicking the dot at the top of the wifi menu. Now your Chromebook is offline. Turn off wifi by clicking the dot. Now you can open the Google Docs App back up by clicking on the Google Docs icon at the bottom of your screen. You can still make edits to the document created while you are offline. You can still edit the Google Doc offline. When you are working offline the offline icon will appear next to the file name. The offline icon appears next to the file name. When you are editing a file offline you will see that it says “All changes saved offline” at the top of the window next to the file name. This means that changes to your online document won’t happen until you reconnect to the Internet. Changes are not saved to the file on the Internet until you reconnect. To see how this works you can open back up the wifi menu and turn the Internet connection back on by again going through the menu opened by clicking on your avatar on the lower right hand side of the screen. Turn the wifi back on. Right after you turn the wifi back on you will briefly see that it says “Saving” at the top of the document next to the file name. This means that the changes you have made to the file are being saved from the version on your Chromebook to the version on the Internet. Changes are saved when you go back online. In a similar way you can also edit presentations and spreadsheets offline using Google Slides and Google Sheets. We will cover those Apps more in a separate course. One thing that is worth noting is that if you work with files offline, then the version you change will update the version of the file online. So if you have shared a document with someone else, when you edit that document offline and re-connect, it might write over some of their changes to the file. 4.0.6 Offline Apps In addition to the main apps from Google, many other apps and extensions offer Offline functionality. Some will be fully functional when they are offline and some will only partially work. When you open the Google Web Store you can specifically search for Apps that have offline functionality. But you will need to review each App to figure out what is available when you are online or offline. You can search for apps that say they have offline functionality. 4.1 Where Are Files Stored? Chromebooks operate on the basic idea that you will store everything in the cloud. And, this set of courses is designed with this principle in mind. So most files will be stored online and used through your web browser. There is not a lot of room to store files on most Chromebooks, so we will want to store all of your documents, presentations, spreadsheets, and data on Google Drive or Dropbox. There is local storage on your Chromebook where you can place files. For example, we saw that when you take a screenshot with your Chromebook the file is saved onto your Chromebook directly, not onto the cloud. This can be particularly useful when people send you a file in an email and you want to eventually store it online. You might need to download it first to upload it again to one of the online file storage systems. 4.1.1 Where is the local storage? Files stored on your Chromebook can be found using the Files App. You can find them by first clicking on the Launcher circle at the bottom left-hand corner of your screen. To see all of the apps available on your Chromebook you will expand the selection from your Launcher by clicking on the ‘^’ (up arrow) symbol on your screen. Launcher Screen You can then find the Files App which looks like a blue circle with a white file folder on it. As we discussed in a previous section you can pin this app to your Shelf by clicking with two finders, then moving the cursor over Pin to Shelf and letting go. It is important to pin this app to your shelf since it will be one of the apps you use most regularly. Pin the Files App to your Shelf. If you click on the Files App you will see all of the files you have downloaded from the Internet, any screenshots you may have taken, and any other files you have stored locally on your computer. The Files App shows you all of your files. These files are all stored in a folder called “Downloads” if you click on the Downloads button you will only see the files that are actually stored on your Chromebook. These files will be available whether you are connected to the Internet or not. The Downloads folder shows you all of the files on your Chromebook. One challenge is that most Chromebooks don’t have much space on them to store data and files. You can check how much space is left on your Chromebook by clicking the three dots on the upper right hand side of the Files app. The Files options lets you see how much storage you have left. Despite the limitation on local storage you can also store some of your files and data on the cloud. We will talk about different options for this depending on whether you are storing code, data, or other files later. But in general, you have access to all of your files stored on the “cloud” here as well. For example, everything in Google Drive is also available from the Files App. These files aren’t saved to your computer, they are saved on the Internet, so you will need an Internet connection to be able to access them. To see these files click on “Google Drive” on the left hand side of the Files App. Google Drive files can be accessed from the Files App If you want to free up space on your Chromebook, one thing that you can do is transfer some of your files from your Downloads folder to your Google Drive. You can do that by clicking on a file in your Downloads folder, holding down, and dragging it to Google Drive. This will upload the file to the cloud. You can drag files from Download to Google Drive to upload them. Then if you delete it from your Downloads folder, it will only be stored on the web and will free up space on your Chromebook. To delete a file you can click with two fingers on the file. This will bring up options for different things you can do with the file. If you move your cursor down to delete and let go, you will be asked if you want to delete the file. If you say yes, the file will be permanently deleted from your Chromebook. Click with two fingers on a file and then move the cursor over delete to rename the file. You might also find that the names of files aren’t easy to follow. In a later section we will cover file naming for data science. But for now, it is useful to know how to change the name of a file. You start by again clicking with two fingers on the file. Then you can drag the cursor over “Rename” and let go. Click with two fingers on a file and then move the cursor over delete to delete the file. This will place your cursor on the file name which will now be editable. If you type in the new name and hit return the file will be renamed. Type in the new name and hit return, but don’t change the extension. Usually it is a good idea to leave the file extension the same even if you rename a file. The extension is all of the characters after the period that tells your computer what type of file it is - a code file, a data file, a text file, or something else. For example if your file is named “file.png” then the extension is “.png”. So you might change the file name to something like “new_file.png” but you would want to leave the “.png” at the end. "],["account-setup.html", "Chapter 5 Account Setup 5.1 Google Account Setup 5.2 Using Gmail for Email Communications 5.3 Google Drive 5.4 Other Accounts Setup", " Chapter 5 Account Setup Before we can get started doing fun things with data we need to make sure you are set up to use all of the different accounts that you will need throughout the course. We will tell you briefly what each of these accounts is used for and how to set it up now. If you don’t know what each of these accounts is for exactly, don’t worry! We will walk you through everything you need to know. 5.0.1 Choosing a Username Choosing an appropriate username is important. Some combination of your first and last name is a good idea. For example, if your name were Jane Doe, a username such as “JaneDoe” or “Jane.Doe” would work. If the first username you attempt is taken, you can try another, similar username. In this case, maybe try “JDoe”. But, be sure that whatever name you choose, you would be comfortable sharing it with your boss or family member. Usernames with nicknames or profanity are not a good idea. What to Avoid in Usernames 5.0.2 Using a Consistent Username Remembering different usernames for different accounts is difficult. It is best to make your life easy and use the same username whenever possible. We will make your life easy by using the Google Account you set up in the next section whenever possible. When it’s not possible to log in with Google, then we suggest you try to use the same username for each account. 5.0.3 Accounts To give you an idea of where we’re going, the first account (and arguably the most important account) you set up in the next lesson will be a Google account. After that we will walk you through the steps to get you set up with accounts on: LinkedIn - this is a site to share information about yourself with employers. Twitter - this is a social media site that we will use to share our data science products and get support from the data science community. Basecamp - this is a website where you will be able to chat online with your fellow students and instructors. RStudio Cloud - this is a website where you can use Rstudio, the main tool to learn data science. GitHub - this is a website where we will share the results of our data science projects with each other and the world. Accounts 5.1 Google Account Setup The first and most important account we need to set up will be a Google account. You will need a Google account to be able to use free Google products. These Google products, such as Gmail, Google Docs, Google Sheets, and Google Slides will be useful in many of the Data Science projects you complete. The Google account will also be useful for letting you get access to other websites we will use in the program. Google Products If you already have a Google account with an appropriate username that you would like to use throughout this course, you can skip the next section and move to the “Log off Guest Chromebook” section. However, it’s probably best to create a new account dedicated to all your Data Science accounts, many of which you will set up in the next lesson. 5.1.1 Getting a Google Login To get a Google account, you will first want to open a new tab in your current Chrome session. To do this, you’ll first click on the small gray box to the right of the tab you currently have open. Go to this Google Accounts Sign In Google Sign in Begin filling in this form. Google will alert you if the username you’ve chosen has already been taken. Once you’ve filled out all the blanks, click on “Next” at the bottom right. Recovery number You will then be asked to verify your account. To do so, ensure that a valid phone number for you is in the ‘Phone number’ box. Select whether you prefer to be contacted by “Text message (SMS)” or ‘Voice Call’. Click ‘Continue’ once the information has been entered. You will then be sent a verification code by text message or by phone call, depending on your choice to this question. Enter the verification code into the box on the screen and click “Continue”. You’ll also be asked to agree to Privacy and Terms. Scroll all the way down and click “I Agree”. Congratulations!! You now have a Google username and account! Be sure to remember your username and password! This will be used for your email address (Gmail) and all other Google products. Google Welcome! 5.1.2 Log Off Guest Chromebook At this point, you’ll want to log off the Chromebook you’re using as a Guest and sign in using your Google Username. To do so, click on ‘Guest’ at the very bottom right-hand of the screen. Click ‘Exit guest’ on the screen that pops up. This will log you off of the Chromebook so you can re-login in using your Google account. Chromebook Log Off 5.1.3 Re-Login using Google Account You will now be on the Chromebook login screen. To sign in using your Google account, click ‘Add person’ at the bottom of the login screen. Chromebook Add Person A ‘Sign in to your Chromebook’ screen will open. Chromebook Sign in Enter your new Google account name here. Click ‘Next’. Enter your password. Click ‘Next.’ You will now be logged on. Anytime you work on this Chromebook now, you will simply log in using your new Google account. 5.2 Using Gmail for Email Communications Communication with your clients or your employer is an important part of being a data scientist. E-mail, short for electronic mail, is an important part of the communication picture because of its flexibility. You can write messages of varying lengths, use formatting if you desire, and attach documents as needed. Depending on your particular work situation, email may be your main means of communicating about your work. When using a Chromebook, the email application of choice is Gmail. Gmail is a free e-mail service provided by Google that provides users with a gigabyte of free storage for messages and includes the ability to easily search for specific emails. Gmail also automatically organizes related emails into a conversational thread, meaning that a message and all replies to that message are stored together. Gmail has many of similarities with other web-based email clients, such as Hotmail and Yahoo! Mail. If you regularly use email through an internet browser, much of the following will be review. 5.2.1 Accessing your Gmail Inbox Gmail is included with your Google account, so you should already have access to Gmail through the account you set up in your first introductory course. Your email address will be username@gmail.com, where username is the username of your Google account. For example, if your user name is Jane.Doe then your email address would be Jane.Doe@gmail.com. This is one reason to choose an appropriate user name, since this email address will be visible to anyone you communicate with over email! You can access Gmail by going to the address “https://www.gmail.com” or by clicking on the start menu located on the lower left hand corner of your Chromebook and searching for “Gmail”. If you are not already logged into your account, you will see a prompt similar to the one here. Login screen After inputting your Google username and password you will be able to see your inbox. If you are already logged into your Google account, you will be brought immediately to your inbox without the need to first log in. Inbox Your inbox shows you all of the messages you’ve received through Gmail. Messages you have already read are shaded gray, while those you haven’t read yet are white. You can open a message to read it by clicking on the message stripe. By default, Gmail organizes your emails into three groups: Primary, Social, and Promotions. You can switch between these groups using the tabs at the top of the screen. Tabs The “Primary” tab will contain the majority of the email messages that you will care about reading and responding to. The “Social” tab will contain messages related to social media, such as messages from Twitter, LinkedIn, and Facebook. The “Promotions” tab will contain messages that include offers for purchasing products or signing up for services. Gmail uses machine learning to put new messages into the appropriate tab. As mentioned before, the “Primary” tab will be the main focus of using your email as a data scientist. One nice feature of Gmail is that it organizes related emails into a conversational thread. The message indicated by the red arrow is an example of a conversational thread, where an original message and all of its replies are grouped together. Inbox indicating thread By clicking on this message, you can see all of the individual messages that make up the email conversation, and can open each individual message by clicking on it. Example email thread 5.2.2 Composing and Sending Emails Let’s say you want to send an email to a client. The first step is to start a new message by pressing the “Compose” button on the top left corner of your inbox. Compose button A box will pop-up where you can enter the recipient’s email address (in the To field), the subject of the email, and the content of the email. New message You can send an email to multiple people by entering all the emails one after another in front of To. You can also add recipients in the Cc (carbon copy) or Bcc (blind carbon copy) fields by clicking on either of these links to the right of the To area. Recipients who are copied (Cc) will receive the email as if it is addressed to them; this option in usually used if you want them to be aware of the message but aren’t expecting them to take any action or reply. Recipients who are blind copied (Bcc) will receive the message, but their email addresses won’t be visible to other recipients of the email. This option is often used when sending an email to a long list of recipients where you don’t want each individual to see the other people on the email. You should also include a short informative subject in the Subject area so that the recipient will know what the contents of the email are about. This is also helpful later when you may want to search your email to find a particular message. There are also a few buttons in the menu bar at the very bottom of the email box that are very helpful. The attachment button, which looks like a paperclip, can be used to attach a file from your computer (such as an image, a document, or a video) to your email message. Attachment from computer You can also attach a file from Google Drive to your message by clicking on the Google Drive button, which looks like a triangle. Google Drive is Google’s file hosting service and you will learn more about it in a later lesson. Attachment from Google Drive You can send attachments up to 25 MB in size through Gmail. If you have more than one attachment, they can’t add up to more than 25 total. If your file is greater than 25 MB, you can instead upload the file to Google Drive and share a link to the file over email instead of including it as an attachment. Other options in this menu bar include inserting a photo (with the picture button), inserting a link (with the chain button), emojis (with the smiley face button), or even sending money (with the dollar sign button). Other menu options There are also many formatting options available for your email message, such as changing the font type and size, bolding text, including bullet lists, and more. These formatting options are similar to those available through Google Docs, which you will see in a later lesson, so we won’t spend time on them now. But you can find these options under the format button, which looks like an uppercase A with a line under it. Formatting options Finally, once you are ready to send your message, click the “Send” button at the bottom left of the message box. If you type an email but change your mind about sending it, you can delete the email by clicking on the trash can button in the bottom right corner of the new message box. Send or discard draft If you close the new message box without deleting it, it will automatically be saved as a draft message, and you can find it under the “Drafts” link in the list of link on the left-hand side of your inbox. Drafts 5.2.3 Replying to a message To read an email message, simply click on the message stripe. If you want to reply to the message, you can scroll to the bottom of the message to the reply box. To reply, simply click in the box and start typing your response. After writing your response to the email click the send button and you are done! Reply To forward the message to someone else, click on the “Forward” link and type the new recipient’s email address in the “To” area. You can then click in the message box to add a note to the forwarded message. If there is more than one other person on the email, you will also have any option to “Reply to all”, which means that your response will be sent to all the people who were on the original email. Use this option with caution, and only when you need all of the people to see your response. 5.2.4 Searching for specific messages To find a particular email message in your inbox, you can search using the search box at the top of the Gmail screen. Searching your Gmail inbox is very similar to searching the internet using Google. You can search by a word in subject of the email, the name of the person who sent the email, or the content of the email. For example, to find all messages from John Doe, you could type “from:john.everyday.doe@gmail.com” in this search box. To find messages that include the words “cancer data”, you could type “cancer data” in this search box. Search Box If you click the down arrow next to the search box, you will open a window that allows more specific options, such as searching by either sender or recipient, searching for specific words in the subject line, or searching by whether the message includes or doesn’t include certain words. You can also specify specific dates to search for messages. Advanced search 5.2.5 More specifics on using Gmail We have touched very briefly on some of the basics of using Gmail: accessing your account, sending and replying to messages, and searching for a particular message. There are lots of other features of Gmail that you may want to explore now or at a future time. There are many good beginner tutorials on using Gmail on YouTube.com. Google also has extensive information on getting started with Gmail, which can be found at the following web address: https://www.cloudskillsboost.google/course_templates/200 In this tutorial, you can find additional information about creating and sending email messages, organizing your inbox, searching your email, and more! 5.3 Google Drive Google Drive is a cloud storage service. It enables you to store files (including documents, data, images, videos, and presentation slides) online. You can also edit many of these files directly online, without needing to download them to your computer or upload them back to the cloud. One of the major benefits of Google Drive is the ability for multiple people to edit the same files at the same time. This makes Drive a useful tool for collaboration, with is very important to a data scientist. 5.3.1 Accessing your Google Drive account Like Gmail and Calendar, Google Drive is included with your Google Account. You can access your Drive account by going to the address “https://www.google.com/drive/” or by clicking on the start menu located on the lower left hand corner of your Chromebook and searching for “Drive”. You will see a welcome screen like this. Drive welcome screen Click the “Go to Google Drive” button to be taken to your Drive account. If you are already logged in to your Google account, you will go directly to Drive. If not, you will be prompted to log in with the username and password for your Google account. Here you can see Jane’s Drive directory. She currently has four documents stored on her Google Drive: a presentation in Google Slides, a dataset in Google Sheets, an image, and a document in Google Docs. To open any document from Drive, simply double-click on the document itself. Google Drive directory 5.3.2 Organizing files in Google Drive In Jane’s Drive account, you can see that she simply has each document shown individually without any file organization structure. This is fine when you have only a small number of documents. However, if you use Google Drive to store lots of documents you will need a file structure to keep things organized and to quickly find the documents you need. In general, we strongly suggest that you create folders to keep similar files together. In this course, we will simply go over the basics of organization in Google Drive. You will learn about good practices for organizing files for projects in a later chapter, which will include how to chose a folder structure and naming conventions for files and folders. First, let’s create a new folder. Click the “New” button in the top left corner of the Google Drive home page and then select “Folder.” A new window will open. Type the name of your folder, in this case “Cancer project,” and click the blue “CREATE” button. Create folder You can now see this new folder under the “Folders” section of your Google Drive home page. To access the files in this folder, you would double-click on the folder name. Folders section Now that you have created a folder, you can start adding files to it. Suppose you want to move the Google Slides presentation “Exploratory plots for cancer project” to this “Cancer project” folder. In the Google Drive home page find this presentation file and right click on it. A menu will open; choose the “Move to” option and then select the folder, “Cancer projects,” that you want to move it to. If don’t already have a folder, you can create one in the menu by clicking on the folder icon. Moving a File to a Folder on Google Drive You can also drag a file to a folder. Just click the mouse button to select a file and while keeping the mouse button pushed down move the file over to the folder and release the button. 5.3.3 Creating files in Google Drive So far we have only moved existing files to a folder. You can create a new file in Google Drive by clicking the “New” button in the top left corner of the Google Drive home page and then selecting the type of file you want to create. You can choose a “Google Doc” for a word-processing document, a “Google Sheet” for a spreadsheet, a “Google Slides” for a presentation, or other choices from the “More” menu. Once you’ve selected your file type, a new window will open with your new file. From here you can begin to edit your file; specific types of Google files will be covered in later lessons. Files in Google Drive are automatically saved to the folder you are in when the file is created. So if you create a new file from your home page, the file will be saved there. If you create a new file the “Cancer project,” the file will be saved to that folder. Remember that you can always move a file to a particular folder as shown before. Create file 5.3.4 Uploading files to Google Drive You can also upload files or folders from your computer to Google Drive. To do this, you again click the “New” button in the top left corner of the Google Drive home page and then select either “File upload” or “Folder upload.” Once selected, a window will open that will allow you to navigate to the file or folder you’d like to upload. Upload file or folder 5.3.5 Deleting Files and folders Deleting files and folders is easy on Google Drive. Simply right-click on the file you would like to delete and choose “Remove” from the menu. Be careful when deleting folders, though! If you delete a folder, all the files in it will be deleted as well. Delete file You can delete multiple files or folders at once by first selecting them all while holding the Ctrl key. Once selected, you can right-click and delete them with the “Remove” option on the menu. 5.3.6 File Recovery If you do accidentally delete a file or change your mind once it’s deleted, recovering a removed file on Google Drive is straight-forward. Deleted files are moved to the Trash folders within Google Drive. To recover a file, simply click on the “Trash” link on the left-hand menu. Trash folder In Trash you can see a list of all the files that you have previously removed. Right-click on the file you want to recover and select “Restore.” The file when they be returned to its original folder on Google Drive. If you’re sure that you won’t need the file ever again, you can clear some storage space by right-clicking on the file and choosing “Delete forever” from the menu. This option is only available within the “Trash” folder. Use this option with caution! By doing this your files will be deleted forever from your Drive and can no longer be recovered. 5.3.7 File Sharing One of the biggest advantages of Google Drive is the ability to share files and folders with other people. You can share individual files and folders that you store in Google Drive with anyone by using their email address (preferably their Gmail account). You can allow those people the permission to either edit, comment on, or only view the file. Permission to edit gives the person freedom to delete the file or change it in any way. Permission to view will only allow the other person to read the file, but they won’t be able to change it. Permission to comment allows the other person to insert comments on the file, but they won’t be able to edit the file itself. To share a file or folder, just right click on the file or folder and select “Share” from the menu. Sharing files and folders on Google Drive In the window that opens, enter the email address of the person you want to share the file/folder with and choose whether the person can edit, comment, or only view the content. If for some reason you want to stop sharing a file or folder that you have already shared, right click on the file and choose “Share” again. In the bottom right corner of the window that pops up, click on Advanced. There you’ll see a list of people with whom you’ve shared the file. To the right of each name or email there is an X. By clicking on it, you will remove access of that person to the file. Click “Save changes” when you’ve finished removing access. 5.3.8 Working Offline The main advantage of cloud storage like Google Drive is that all of your files are stored online and are accessible whenever you want them. But what do you do when you don’t have access to the Internet and want to access your files offline? If you know in advance you will be without internet access, you can easily make Google Drive files available offline on your Chromebook. To do so, follow these steps: First, when online, install the Google Docs Offline extension by clicking on ‘Add to Chrome’. If the button says “Added to Chrome,” you’ve already installed the extension. Google Drive extension for Chrome Next, go to Google Drive settings and in the “Offline” area, make sure the box is checked. It might take a few minutes for the offline access setting to turn on. Offline settings for Drive You are now ready to work on your files offline! Any changes will sync to Google Drive the next time your Chromebook connects to the Internet. 5.3.9 Storage limit Your Google Drive account comes with 15GB of free storage. If this is not enough space, you can pay for more space. But don’t worry, for the purposes of this DataTrail course and many other purposes, you shouldn’t need to buy any storage. Note that the free 15GB is shared among all Google services, including Gmail and Drive, so, part of your storage may be used by your email attachments. You can see how much storage you are using in the lower part of the left-hand menu on your home page. There is also a link here to purchase additional storage. Storage information 5.3.10 Security Google Drive encrypts your data before storing it. Encryption is any method that converts data to to encoded version for security purposes. Drive is encrypted using SSL, the same security protocol used on Gmail and other Google services. However, you’re still responsible for securing your Google account. Two-Step Verification (also known as two-factor authentication), provides an extra layer of security to your account. This requires you to enter your password and on top of that enter a code that is sent to your phone or email. You’ve learned about two-step verification in a previous lesson; we strongly recommend you use Google’s two-step verification to secure your files on Google Drive. Two-Step Verification setup 5.3.11 More specifics on using Google Drive In this lesson, we have talked briefly about how to create files and folders in Google Drive, as well as how to delete, recover, and share your files. There are many other features of Drive that you may want to eventually explore. There are many tutorials for using Drive on YouTube.com. Google also has extensive information on getting started with Drive on their G Suite Learning Center, which can be found at the following web address: https://www.cloudskillsboost.google/course_templates/199 5.4 Other Accounts Setup In addition to having a Google username, there are a number of other accounts to which you’ll need access. By the end of this lesson, you will have set up a number of different accounts. While this may seem like a lot of work now, it will get you set up with all the accounts you’ll need throughout this series of courses. The time you spend now will pay off later! Right now, we’ll walk through each one briefly, get you set up with an account, and discuss what the account is used for. 5.4.1 LinkedIn Account LinkedIn is a social networking site for employment. Think of it as Facebook for getting a job. It allows you to put your qualifications online (like an online resume), has a space where you can look for jobs, and can put you in contact with employers. Don’t worry about the details now. Through this program, you will have the chance to set up your LinkedIn gradually. For right now, we’re just worried about getting this account set up. To begin set up, you’ll go to the web address bar in your Chrome browser. You will type www.linkedin.com and hit ‘Enter.’ LinkedIn website This will bring you to LinkedIn’s login screen. Click on ‘Continue with Google’ to create your LinkedIn account with your Google/Gmail account. gmail account You will be asked to login to Gmail. gmail website You may be asked to verify that you are not a robot. gmail website You may also be asked a series of other questions about where you are located and whether you are looking for a job. Answer these questions about yourself. You now have a LinkedIn username and account! There is a lot more personalization you will want to do with your LinkedIn but this will be good for now! We will now go through similar processes for the other accounts needed in this program. 5.4.2 Twitter Account The second account you will need will be a Twitter account. You may already be familiar with Twitter; however, data scientists tend to use Twitter for work rather than socializing. Twitter is a social media platform where users can “post and interact with messages.” These messages are known as ‘tweets.’ Twitter is a great place to learn new things, connect with other data scientists, and to ask/answer questions quickly. You will need a professional Twitter account for our program. If you already have a Twitter account you use for personal tweets and communicating with friends you should still create a new, professional account where you will only post professional links and interact with other data scientists. To get a Twitter account, first type ‘www.twitter.com’ in the search bar at the top of your Chrome session. Twitter website To sign up, use your Google Account again Twitter sign in Continue to confirm your Google Account: Twitter sign in You will need to give it your phone number as well as your birthday to verify your identity. Then, this will bring you to a screen where you will choose a username. This will be what your Twitter ‘handle’ will be. For simplicity, it would be best for your Gmail username and Twitter handle to be the same (ie if your Gmail address is Jane.Doe@gmail.com, ‘Jane.Doe’ would be a great Twitter username). If that name is unavailable, choose a different, but appropriate and simple, Twitter username. Once you have chosen a Twitter username, click ‘Next’. Twitter username Twitter will ask you a series of questions to ask you your preferences. Because this will be your professional account, you may want to follow “data science” topics. Congrats! You now have a Twitter account! 5.4.3 Basecamp Account BaseCamp is a place where teams of people can easily communicate and work together on a project. As a data scientist, you are often working in a group on a project. Basecamp is a place where everyone working on that project can communicate. Basecamp is where communication throughout this course will happen. You will be able to ask questions, answer questions, and communicate with others on Basecamp about the things you are learning and the projects you are working on. To get a Basecamp account, you will first open a new tab in your browser by typing ctrl+t. Once you have a new tab open, you will type ‘www.Basecamp.com’ at the top of your browser in the web address bar. Then on the Basecamp home page, click on “Customer Login”. Basecamp Get Started! Choose “Sign in with Google”. We won’t be signing into any workspaces yet; however, later in the program, when we do, you will have an account! That’s all you need to do with Basecamp for now! 5.4.4 RStudio Cloud Account As a data scientist A LOT of your work will be done in something called RStudio. In this program, you will be learning the basics of the programming language, R. R is a free programming language for statistical computing and graphics. In other words, the code you write to work with data will all be done in R. RStudio Cloud is the place (or ‘platform’) where you will type this code and make basic plots. Luckily, RStudio Cloud makes it easy to sign up. You will first go to ‘rstudio.cloud’. Note: This web address does not start with ‘www.’ rstudio.cloud web address This will bring you to a screen where you can click on ‘Get Started.’ This will bring you to a login screen where, instead of typing in your information, since you already have a Google account, just click on ‘Sign up with Google.’ rstudio.cloud sign up with Google You will be prompted to choose which Google account you want to use. Choose your professional Google account. Then, you will be brought to a screen where you will have to enter a username. Again, for simplicity, try to use the same username across all accounts. rstudio.cloud choose account Then, click ‘Create Account’ and you’re all set! You now have an RStudio Cloud account! "],["forming-questions.html", "Chapter 6 Forming Questions 6.1 Learning Objectives", " Chapter 6 Forming Questions 6.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Recognize the general steps involved in a data science project Practice forming data science questions Understand the general categories for types of data science questions Look at data and ask questions based on its contents Run your first data science project "],["the-data-science-process.html", "Chapter 7 The Data Science Process", " Chapter 7 The Data Science Process As a data scientist, what steps are involved in answering a question with data? In this course, we aim to frame everything in the context of the data science process. In the following chapters and courses we will cover each part of this process more in-depth. 7.0.1 The Parts of a Data Science Project Data science process generally follow these steps: The first part of the a data science process is forming a question. Sometimes you’ll have a dataset in mind that you will explore to form this question, and other times, you will have a question that you will find some data to help you answer. Form a question - What is the question you hope to answer with data? Get the data - To answer this question you’ll need data! Clean the data - Datasets are almost never ready to analyze from the get go. As a data scientist, we’ll need to do some cleaning and set up steps to get the data where we need it before we can really dig into it. Explore and plot the data - After your data is clean, you’ll want to do some initial exploration. What do these data look like? Plots are a great way to visual your data and start to get an idea of how the data might relate to your initial question. Get stats - You can use statistics and models to try to use your data to get to the bottom of your question. Share results - Given everything you are seeing with your data, how do we interpret this? What’s our conclusion when it comes to our initial question? In the upcoming chapters and courses we will further dive into each of these steps! 7.0.2 A Data Science Project Example For this example, we’re going to use an example analysis from a data scientist named Hilary Parker. Her work can be found on her blog, and the specific project we’ll be working through here is from 2013 and titled “Hilary: the most poisoned baby name in US history”. To get the most out of this lesson, click on that link and read through Hilary’s post. Once you’re done, come on back to this lesson and read through the breakdown of this post. Hilary’s blog post 7.0.2.1 Form a question When setting out on a data science project, it’s always great to have your question well-defined. Additional questions may pop up as you do the analysis, but knowing what you want to answer with your analysis is a really important first step. Hilary Parker’s question is included in bold in her post. Highlighting this makes it clear that she’s interested in answer the following question: Is Hilary/Hillary really the most rapidly poisoned name in recorded American history? 7.0.2.2 Get the data To answer this question, Hilary collected data from the Social Security website. This dataset included the 1000 most popular baby names from 1880 until 2011. 7.0.2.3 Data Analysis As explained in the blog post, Hilary was interested in calculating the relative risk for each of the 4,110 different names in her dataset from one year to the next from 1880 to 2011. By hand, this would be a nightmare. Thankfully, by writing code in R, all of which is publicly available on a site called GitHub, Hilary was able to generate these values for all these names across all these years. It’s not important at this point in time to fully understand what a relative risk calculation is (although Hilary does a great job breaking it down in her post!), but it is important to know that after getting the data together, the next step is figuring out what you need to do with that data in order to answer your question. For Hilary’s question, calculating the relative risk for each name from one year to the next from 1880 to 2011 and looking at the percentage of babies named each name in a particular year would be what she needed to do to answer her question. Hilary’s GitHub repo for this project 7.0.2.4 Clean the data What you don’t see in the blog post is all of the code Hilary wrote to get the data from the Social Security website, to get it in the format she needed to do the analysis, and to generate the figures. As mentioned above, she made all this code available on GitHub so that others could see what she did and repeat her steps if they wanted. 7.0.2.5 Explore and plot the data In addition to this code, data science projects often involve writing a lot of code and generating a lot of figures that aren’t included in your final results. This is part of the data science process too. Figuring out how to do what you want to do to answer your question of interest is part of the process, doesn’t always show up in your final project, and can be very time-consuming. 7.0.2.6 Get stats That said, given that Hilary now had the necessary values calculated, she began to further analyze the data. The first thing she did was look at the names with the biggest drop in percentage from one year to the next. By this preliminary analysis, Hilary was sixth on the list, meaning there were five other names that had had a single year drop in popularity larger than the one the name “Hilary” experienced from 1992 to 1993. Biggest Drop Table In looking at the results of this analysis, the first five years appeared peculiar to Hilary Parker. (It’s always good to consider whether or not the results were what you were expecting, from any analysis!) None of them seemed to be names that were popular for long periods of time. To see if this hunch was true, Hilary plotted the percent of babies born each year with each of the names from this table. What she found was that, among these “poisoned” names (names that experienced a big drop from one year to the next in popularity), all of the names other than Hilary became popular all of a sudden and then dropped off in popularity. Hilary Parker was able to figure out why most of these other names became popular, so definitely read that section of her post! The name, Hilary, however, was different. It was popular for a while and then completely dropped off in popularity. 14 most poisoned names over time To figure out what was specifically going on with the name Hilary, she removed names that became popular for short periods of time before dropping off, and only looked at names that were in the top 1000 for more than 20 years. The results from this analysis definitively show that Hilary had the quickest fall from popularity in 1992 of any female baby name between 1880 and 2011. (“Marian”’s decline was gradual over many years.) 39 most poisoned names over time, controlling for fads 7.0.2.7 Interpret and communicate results The final step in this data analysis process was, once Hilary Parker had answered her question on her computer, it was time to share it with the world. An important part of any data science project is effectively communicating the results of the project. Hilary did so by writing a wonderful blog post that communicated the results of her analysis, answered the question she set out to answer, and did so in an entertaining way. Additionally, it’s important to note that most projects build off someone else’s work. It’s really important to give those people credit. Hilary accomplishes this by: linking to a blog post where someone had asked a similar question previously linking to the Social Security website where she got the data linking to a link about where she learned about web scraping 7.0.3 What you can build using R Hilary’s work was carried out using the R programming language. Throughout the courses in this series, you’ll learn the basics of programming in R, exploring and analyzing data, and how to build reports and web applications that allow you to effectively communicate your results. To give you an example of the types of things that can be built using the R programming and suite of available tools that use R, below are a few examples of the types of things that have been built using the data science process and the R programming language - the types of things that you’ll be able to generate by the end of this series of courses. 7.0.3.1 Prediction Risk of Opioid Overdoses in Providence, RI Masters students at the University of Pennsylvania set out to predict the risk of opioid overdoses in Providence, Rhode Island. They include details on the data they used, the steps they took to clean their data, their visualization process, and their final results. While the details aren’t important now, seeing the process and what types of reports can be generated is important. Additionally, they’ve created a Shiny App, which is an interactive web application. This means that you can choose what neighborhood in Providence you want to focus on. All of this was built using R programming. Prediction of Opioid Overdoses in Providence, RI 7.0.4 Other Cool Data Science Projects The following are smaller projects than the example above, but data science projects nonetheless! In each project, the author had a question they wanted to answer and used data to answer that question. They explored, visualized, and analyzed the data. Then, they wrote blog posts to communicate their findings. Take a look to learn more about the topics listed and to see how others work through the data science project process and communicate their results. Predicting movie ratings with IMDb data and R, by Dimiter Toshkov Where to Live in the US, by Maelle Salmon Sexual Health Clinics in Toronto, by Sharla Gelfand "],["types-of-data-science-questions.html", "Chapter 8 Types of Data Science Questions", " Chapter 8 Types of Data Science Questions The way to start a data analysis project is to start with a question. Asking questions is central to data science. In this chapter we will start to get comfortable with the types of questions that data science can answer. It’s very likely that in a data science project you will start out with one question that will give rise to many more questions! After you have asked your general question, the next step is to turn it into a data science question. This usually involves making the question more concrete, identifying what type of question you are asking, and identifying the parts of the data that you will use to answer the question. We will look at a few different types of questions that you might want to answer from data. There are four classes of data science questions we will talk about. Descriptive The goal of descriptive data science questions is to understand the components of a data set, describe what they are, and explain that description to others who might want to understand the data. This is the simplest type of data analysis. Examples of descriptive data science calculations are the average (also called the mean), variance, maximum, and minimum of the data set. Exploratory The goal of exploratory data science questions is to find unknown relationships between the different variables you have measured in your data set. Exploratory analysis is open ended and designed to find expected or unexpected relationships between different measurements. This step usually requires making a graph to visualize the relationships. Inferential The goal of inferential data science questions is to is to use a small sample of data to say something about what would happen if we collected more data. Inferential questions come up because we want to understand the relationships between different variables but it is too expensive or difficult to collect data on every person or object. In this case we can establish there is a relationship between two or more variables based on the small sample, but if we had more data we could be more confident about it. Predictive The goal of predictive data science question is to use data from a large collection to predict values for new individuals. This might be predicting what will happen in the future or predicting characteristics that are difficult to measure. Predictive data science is sometimes called machine learning. Types of Data Analysis The questions we will focus on are the types where we look for relationships between measurements or variables. But in these types of analyses we won’t be able to tell anything about what happens if you change one of the variables. To figure out what happens if you change a variable, you need a more advanced type of analysis. These analyses - causal and mechanistic - require data on specific types of problem and collected in special ways. For this class, the primary thing we need to be aware of is that just because two variables are correlated with each other it doesn’t mean that changing one causes a change in the other. One way that people illustrate this idea is to look at data where two variables show a relationship, but are clearly not related to each other. For example, in a specific time range, the number of people who drown while falling into a pool is related to the number of films that Nicholas Cage appears in. These two variables are clearly unrelated to each other, but the data seems to show a relationship. We’ll discuss more later in this class Spurious Correlation 8.0.1 Examples of data science questions Let’s begin with some example data science projects, how they are asked, and how they are approached. In each example, we explain the problem as a data science question and guess what sort of analysis is fit for approaching the problem. 8.0.1.1 Example 1: Credit card fraud detection If you have a credit card, every time you charge something, the bank keeps a record of that charge. This can be useful for you when you want to keep track of your finances. But banks use the information for other purposes as well. If you lose your credit card and someone starts using it to buy things for themselves this is called credit card fraud. By collecting data from everyone, credit card companies are able to predict potential fraudulent transactions before consumers notice anything. So the question is: “Can we predict which credit card charges are fraudulent?”. But this is not a data science question yet. To ask this like a data scientist we might ask, “Can we use the time of the charge, the location of the charge, and the price of the charge to predict whether that charge is fraudulent or not?”. Since we are interested in predicting whether a charge is fraud or genuine, this will be a predictive analysis. Problem: Detecting whether credit card charges are fraudulent. Data science question: Can we use the time of the charge, the location of the charge, and the price of the charge to predict whether that charge is fraudulent or not? Type of analysis: Predictive analysis 8.0.1.2 Example 2: Analysis of YouTube comments Any text you can find on the web can be considered to be data. You can use this data to understand how people behave on the web. For example, when companies like YouTube want to understand whether their users are behaving nicely or badly, they might look at the comments they leave. So the question might be, “Are the comments on our platform mostly nice or mostly mean?” To write this in terms of a data science question you might ask, “Are the words that people use in their comments more frequently positive words (great, awesome, nice, useful) or negative words (bad, stupid, lame, awful)?” This is a question you could answer by collecting information on words and labeling them with whether they are nice or mean. This is an example of descriptive analysis since once we have the data, the analysis boils down to comparing the number of positive comments to negative comments. Problem: Understanding whether users are nice or mean on YouTube Data science question: Are the words that people use in their comments more frequently positive words (great, awesome, nice, useful) or negative words (bad, stupid, lame, awful)? Type of analysis: Descriptive analysis 8.0.1.3 Example 3: Sesame Street and kids’ brain development Sesame Street is a children’s entertainment and educational program. Scientists might be interested in whether watching Sesame Street helps brain development in kids. To turn this into a data science question we need to make it more specific and focused on data. So we might convert this to a data science question like, “Can we compare children who watch Sesame Street and those who don’t to see whose test scores are higher?” A complication is that it might be difficult and expensive to get parents to have their children participate in the study. So we might take a small sample of students and measure their test scores and find out whether they watch TV. Since we want to say something about all children, but have only measured data on a few, this would be an inferential analysis. Problem: Does Sesame Street affect kids’ brain development? Data science question: Is there a relationship between watching Sesame Street and test scores among children? Type of analysis: Inferential analysis "],["how-to-learn.html", "Chapter 9 How to Learn", " Chapter 9 How to Learn In the last lesson we walked through a few interesting data science projects. Eventually, using the foundational skills learned, with practice on your own, and with other skills you pick up along the way, you’ll be completing your own, equally-awesome data science projects. However, what many people don’t tell you early on is that that path will be paved with a lot of failure. This isn’t a bad thing! Data scientists fail all the time. They write code that produces an error they have to figure out. And they regularly have to abandon projects that aren’t going to work out. Failure is part of the process. Failure Even when a project is successful, know that there was failure on the way to success! The problem is that what you see in a final blog post or a product put out by data scientists at a company is the final product. This product may be something that is functional, really important, or even beautiful. What you don’t see is all the failure that happened on the way to getting the end product. Data science projects can be a lot like social media accounts. On social media, it’s easy to only show the good stuff about one’s life. For data science projects, the end product of a data science project may be awesome, so the user will only see the good stuff. But, there’s a lot of struggle and failure that went into creating the awesome end product! success requires failure In fact, that pathway to success in data science is always full of failure. And, often, failure followed by figuring out why you just failed is a great way to learn. That doesn’t make failure easier. It will be frustrating from time to time, and figuring out why something isn’t working can be hard. That’s ok! Know that you’re not alone. Even experienced data scientists who have built really cool stuff experience lots of failure along the way. process can be difficult 9.0.1 Learning How To Learn In addition to learning the basics of data science in this course set, we also want you to learn how to learn. First and foremost, the best way to learn data science is by doing it. Throughout these lessons, copy the code you see in the lessons and try it out on your own. If you get an error, that’s ok! Google that error and try to learn from this error! In fact, we’ve got a whole lesson in this course on how to Google and a lesson in a later chapter on how to get help for questions when you’re programming. But, there’s more to learning how to learn than getting good at searching on the Internet (although, that is important!) 9.0.1.1 The Mindset Your mindset is very important to learning how to learn. Your goal should be to answer an interesting question. Your objective is not to memorize a bunch of functions. It’s to use those functions to do something interesting. The path to accomplishing that goal may be circuitous. You may take a few steps backward and experience a setback or two before moving forward. That’s ok! mindset 9.0.1.2 The Path When carrying out a data science projects, there is always more than one way to solve a problem. Your path may be different than someone else’s path. In fact, while you may not know R code yet, the following four lines of code all produce the exact same output: # Example 1 mtcars %&gt;% tidyr::gather(key = variable, value = value) # Example 2 tidyr::gather(mtcars, key = variable, value = value) # Example 3 mtcars %&gt;% tidyr::gather(key = variable) # Example 4 mtcars_long &lt;- tidyr::gather(mtcars, key = variable) Any one of these would be a reasonable approach. We use this example to explain that there is more than one way to approach and to answer a question! Your path may be different than someone else’s. Your approaches may not be identical. And, that is more than ok! path 9.0.1.3 Asking For Help Data science is best done as a part of a community! Asking each other questions and brainstorming with others is ideally how data science projects are completed. In fact, this is how most companies work. You are part of a team of data scientists working and learning together. Working together and asking questions: Makes projects turn out better! Helps everyone learn more! Helps everyone have a better, less frustrating time! No one knows everything and that is okay and expected. We’ll point out where to find help when you’re stuck throughout this course set. However, it may not be obvious when to ask for help. While this is not a hard and fast rule, if you’ve been trying to find the answer to something you’re stuck on for half an hour and cannot figure it out, it may be time to post your question online for someone else to answer or to reach out directly to someone to get your question answered. During the half hour when you’re trying on your own, you should: Google for the answer with various different wording attempts. If it’s a coding question, you should try running code to test to see if the fixes from Google fix your problem. If you’re getting error messages, paste those messages into Google and search. If after trying all of these things you’re still stuck, then you should ask for help every time. Rather than give up or get overly frustrated because you’re stuck, ask questions! Ask Questions "],["rstudio-and-projects.html", "Chapter 10 RStudio and Projects", " Chapter 10 RStudio and Projects Remember R is the main software that we are going to use to analyze data in this class. R is one of the two most popular languages for data science. We will learn a lot more about it throughout the courses, but here we are just going to use it to take a peak at the data you have created. R is a piece of software that is used for running computer code. RStudio is a company that makes a piece of software that works with R. RStudio makes it easier to create, save, share, and work with R code and data sets. RStudio is also useful for organizing projects and writing, organizing, and sharing your data science work. If you have a more traditional laptop you can download and install R and RStudio on your laptop. However, the DataTrail Program will use RStudio through a web-based version of their software called RStudio Cloud. 10.0.1 Logging in to RStudio Cloud To use this software, open your web browser and navigate to the website RStudio Cloud. Navigate to RStudio Cloud in your web browser You should see a screen that looks like this. You can click the button in the top right to log in. Log in to RStudio Cloud When you click Log In you will be offered options for Logging in, for our class you will log in with your Google Account so click on that option. Choose to log in with Google Then you will be asked to “Choose an account”. Click on the Google account you have set up for this course. Choose your Google account You should now see a list of your projects. This is a list of the instructor’s projects, your list will be different. Choose your Google account 10.0.2 Creating a new project RStudio projects are where you will do your data science magic for this course and beyond! To get started working on a new project, click on the “New Project” blue icon toward the top right. If you click the arrow next to New Project, you will notice there are multiple different options for creating a new project. For now choose New Rstudio Project, and we will discuss and use those other options later. Create a new project By starting a new project you’ll be brought to a screen where three spaces are available. RStudio Cloud Project If you click on the name of the project - currently it will be Untitled Project - then you can rename it. Click to change the title of the project To make things simple let’s call it my_first_project. You will call it that by typing the name into the box for the project name. Rename the project my_first_project If you want to see all the projects you have you can click on the words Your Workspace at the top left of the screen. Click on Your Workspace to see all of your projects If you want to return to a project, just click on the project name, for example by clicking on my_first_project. Click a project name to return to your project. Projects can be organized into Spaces. A space is just a place that lists out multiple projects. You can click on the name of the DataTrail space in the upper left to see your projects there. Click a project name to return to your project. If you want to return to your original space, click again in the top left hand on the menu bar, then click on Your Workspace to return to your main set of projects. 10.0.3 The Tour Now that we have a project. Return to your my_first_project so we can get oriented to the RStudio space. We will go through each of the regions and describe some of their main functions, so follow along with each step and make sure you understand the function and how to access each part of RStudio Cloud on your own. But, it would be impossible to cover everything that RStudio can do, so we urge you to explore RStudio Cloud further on your own too! To access the fourth space shown in the figure below, you’ll have to start a new file. To do so, you’ll click on File, hover over New File from the drop-down menu that appears, and then click “R Script” from the drop-down menu. Open up a new file To open up a new file, you’ll click on File, hover over New File from the drop-down menu that appears, and then click “R Script” from the drop-down menu. This will open up a new R Script, which is currently called “Untitled,” which you can see on the tab at the top left of the quadrant has just appeared. RStudio’s quadrants 10.0.3.1 The menu bar In addition to the four main quadrants, there is also a menu bar. The menu bar runs across the top of your screen and should have two rows. The first row should be a fairly standard menu, starting with “File” and “Edit.” Below that, there is a row of icons that are shortcuts for functions that you’ll frequently use. The commonly used options of the main menu bar To start, let’s explore the main sections of the menu bar that you will use. The first is the File menu. Here we can open new or saved files, save our current document, or close RStudio. As we saw earlier in this lesson, if you mouse over “New File”, a new menu will appear that suggests the various file formats available to you. R Script and R Markdown files are the most common file types for use, but you can also generate R notebooks, web apps, websites, or slide presentations. If you click on any one of these, a new tab in the “Source” quadrant will open. We’ll spend more time in a future lesson on R Markdown files and their use. The File menu The Session menu has some R specific functions, in which you can restart, interrupt or terminate R - these can be helpful if R isn’t behaving or is stuck and you want to stop what it is doing and start from scratch. The Session menu The Tools menu is a treasure trove of functions for you to explore. For now, you should know that this is where you can go to install new packages and set your options and preferences for how RStudio looks and functions. For now, we will leave this alone, but be sure to explore these menus on your own once you have a bit more experience with RStudio and see what you can change to best suit your preferences! The Tools menu 10.0.3.2 Console When you first opened R, you were presented with the console. This is where you type and execute commands, and where the output of these commands is displayed. The console To execute your first command, at the &gt; prompt, try typing 1 + 1. Then, hit enter. You should see the output [1] 2 below your command. 10.0.4 Source: script editor panel However, often you want to write code and save it so that you can open the code again and re-run it later. This saved file with code in it is referred to as a script. When you want to write code and save it in a script, you’ll do this in the Source panel. To get started in your script file, copy and paste the following into your Source quadrant (top-left). example &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), nrow = 4, ncol = 2) To run this code, you can’t just hit enter (as you were able to do in the Console). Hitting enter will just bring your cursor to the next line in the script. Instead, with your cursor in the line of code you want to run, you can click on “Run” at the top right of your script file. This will execute the code in the Console. Alternatively, to run code, with your cursor on the line of code you’d like to run, you could hit ‘ctrl + enter’ to run that line of code. This will save you a lot of time as you start writing a lot of code and analyzing data. Practice this keyboard shortcut now! What this code does is create an object (we’ll define what that is soon!) called ‘example’ that has the numbers 1 through 8 in four different rows and two different columns. To see what this object looks like, we’ll take a look at the environment quadrant of RStudio Cloud. 10.0.4.1 Environment (&amp; History) To view this object we’ve just created, you’ll first want to ensure that the object was created. In the Environment quadrant, you should see that ‘example’ is now there. The object was created! The environment quadrant Then, just click anywhere on the “example” line, and a new tab on the Source quadrant should appear, showing the matrix you created. Your newly made object, opened in a new tab of the source panel RStudio Cloud also tells you some information about the object in the environment, like whether it is a list or a data frame or if it contains numbers, integers or characters. This is very helpful information to have as some functions only work with certain classes of data. We’ll get into the details of all this later, but for now, knowing that this information is in the Environment tab is enough. The quadrant has two other tabs running across the top of it. We’ll just look at the History tab now. Your history tab should look something like this: The history tab Here you will see the commands that we have run in this session of R. If you click on any one of them, you can click “To Console” or “To Source” and this will either rerun the command in the console, or will move the command to the source, respectively. From History to Source Do so now for your View(example) command and send it to source by clicking “To Source”. Sending ‘View(example)’ from History to Source This line of code is now in your Source document. When you save this document, you’ll also have this line of code saved for future use. 10.0.4.2 Saving Files Now that you’ve created a script with code in it, you likely want to save it. To do so, you’ll want to click on the save icon. Save Icon In RStudio Cloud this will open a Save File window. Save File Window Type in the name you want for this file. For now, let’s call it ‘R_basics.R’. It’s important that it ends in a .R since that is the file type we are working with, but we’ll return to this concept later. Click the Save button. The file name ‘R_basics.R’ will now show up in the tab at the top of the R Source quadrant. 10.0.4.3 Making a new folder Now let’s create a new folder for storing our data that we’ll need. In the bottom right quadrant of your console, click the New Folder icon. New Folder We’ll name this folder “data” by typing it in the box and clicking “OK”. To move in and out of folders, you can click on folder names in the Files pane. 10.0.5 Files/Help/Plots/Packages/Viewer 10.0.5.1 Files You can also see where this file is saved using the fourth and final quadrant in RStudio Cloud that we’ll discuss. In this final quadrant you’ll see five tabs: Files, Plots, Packages, Help, and Viewer. Files, Plots, Packages, Help, Viewer In Files, you can see everything in your RStudio Cloud project by moving around. You should now be able to see the data folder you just created. code directory in Files tab Clicking on that folder, and create another folder within it called raw_data raw_data folder in Files tab Note that you can see on the top of this bar what folder this folder is in. This is called a file path and we’ll get into this more later. Let’s create some fake data to store in this raw_data folder. Click the New Blank File button and create a Text file that is called hello.txt and save it to this raw_data folder. After you save a file in a folder, if you realize it’s not where you wanted it, you do have the option to move it around. To do so, click on the check box of the file you want to move, and click on the “More” icon to expose options. Click through these to move your file to where you actually wanted it. The “More”” icon 10.0.5.2 Plots In the Plots tab, if you generate a plot with your code, it will appear here. You can use the arrows to navigate to previously generated plots. The Zoom function will open the plot in a new window, that is much larger than the quadrant. Export is one way to save the plot. (Saving plots will be discussed in more detail in a future lesson.) The broom icon clears all plots from memory. The plots tab 10.0.5.3 Packages The Packages tab will be explored more in depth in the next lesson on R packages. Here you can see all the packages you have installed, load and unload these packages, and update them. The packages tab 10.0.5.4 Help The Help tab is where you find the documentation for your R packages and various functions. In the upper right of this panel there is a search function for when you have a specific function or package in question. Navigating this tab will be discussed in more detail in a later lesson in this course. The help tab 10.0.6 Swirl Throughout the DataTrail curriculum, we’ll be using something called Swirl modules to practice the R code learned in many of the lessons. These modules can be run in R. To make sure that you’re comfortable using Swirl, we’ll go through the steps on where to go to run Swirl and how to work through a module. This will be important as many of the quizzes accompanying these lessons will require you to use Swirl. Follow the steps in this section of the lesson to get started with your first Swirl module! Following the steps we just did to open a new project, open up a new project in R and call it swirl_practice_yourname but replace yourname with your own name. In this new project, run the following code to install swirl: install.packages(&quot;swirl&quot;) Any time you are within this space and supposed to complete a swirl module you’ll start by first loading the swirl package (it has already been installed in that space for you) and running the command swirl(): Run the following code to start up swirl. Note that lines that start with # aren’t actually run by R. ## load package library(swirl) ## start swirl swirl() As a reminder, to run code, with your cursor on the line of code you’d like to run, you can hit ‘ctrl + enter’ to run that line of code. Similarly, if there are multiple lines you want to run, you can highlight the lines you want to run and again hit ‘ctrl + enter’ to run those lines of code. This will bring up a prompt asking you what swirl should call you. Type your first name as a response here and hit “enter.” starting swirl Swirl will often send you some text to read. Always read the text as this text will help explain the background information you need or will provide you with information you need to answer the question. At this point, swirl is explaining that when you see ..., that’s when you should press “enter” to continue. When you see &gt; or a list of options (like 1:, 2:, 3), that lets you know swirl is looking for something from you! When you see &gt; that’s a prompt letting you know swirl is expecting you to write some code. When you see a list of options, those are the possible answers to a question you’re being asked. In these cases, you’ll want to select the number corresponding to the right answer. For this practice in swirl, select 1, 2, or 3 and press enter. Getting started in swirl You’ll then be given a number of options that you can use within swirl whenever you see the &gt; prompt. Read the list here, but know that info() gives you this list of options again, main() returns you to swirl’s main menu, and bye() saves your progress but exits swirl. swirl menu options After this, you will be shown a list of courses. The list will be longer than what you see here, but we’re showing this simple example to demonstrate that if you wanted to start on the course “DataTrail Introduction to R”, you would type 1. You’ll be told which course to select throughout the course set. Selecting a course Note that for each quiz question you complete in swirl, upon completion, you’ll receive a code. This code is to be entered as the answer to the quiz question on Leanpub. That’s a basic introduction to using swirl. You’ll have lots of quiz questions that require you to use swirl in this course, so be sure to walk through this introduction on RStudio Cloud now and get comfortable navigating within swirl. "],["your-first-data-science-project.html", "Chapter 11 Your First Data Science Project", " Chapter 11 Your First Data Science Project We are using this definition of data science. “Data science is asking a question that can be answered with data, collecting and cleaning the data, studying the data, creating models to help understand and answer the question, and sharing the answer to the question with other people.” The first step in any data science project is to come up with a question. In this first project, we will go through a question step by step to see what a data science workflow might look like! 11.0.1 Starting up this project Go to the DataTrail workspace. Go to the Data Trail Project list. Go to the DataTrail_Projects project in this list. Click on it to start this assignment. For all future projects you will return to this same place to keep working on your projects. When you first start up your DataTrail_Project, you might see some red text about packages loading, but that this is normal. For this project, go to the 01_Forming_Questions folder. Click on the file first_project.Rmd to open this file. 11.0.2 Your objectives! To complete this project you’ll need to do a few things within this file. Go through this notebook, reading along. Run each code chunk as you come across it by clicking the tiny green triangles at the right of each chunk. You should see the code chunks print out various output when you do this. Add a new code chunk to this project that prints out the phrase “I completed my first data science project”. See the very top of this notebook for an example of how to to do this. At the very top of this file, put your own name in the author: place. Currently it says \"DataTrail Team\". Be sure to put your name in quotes. In the Conclusions section, write up responses to each of these questions posed here. When you are satisfied with what you’ve written and added to this document you’ll need to save it. In the menu, go to File &gt; Save. Now the nb.html output resulting file will have your new output saved to it. Open up the resulting first_project.nb.html file and click View in Web Browser. Does it look good to you? Did all the changes appear here as you expected. Upload your Rmd and your nb.html to your assignment folder (this is something that will be dependent on what your instructors have told you – or if you are taking this on your own, just collect these projects in one spot, preferably a Google Drive)! Pat yourself on the back for finishing up your first project! "],["getting-data.html", "Chapter 12 Getting Data 12.1 Learning Objectives", " Chapter 12 Getting Data 12.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Run basic commands in R Manipulate R objects and understand the differences between R object types Create a data frame Install and use R packages Read in data from a CSV file Create an RMarkdown file Understand the basics of conducting data science in an ethical manner Get a dataset from an online repository and read it into R "],["what-is-r.html", "Chapter 13 What is R", " Chapter 13 What is R 13.0.1 Introduction to R R, most simply, is a programming language. Just like there are many different spoken languages throughout the world, there are many different programming languages. Similar to how each spoken language is used by a subset of the humans on this Earth, each programming language was created for a different group of people who code. You may have heard of other programming languages, such as C++, Java, or HMTL previously. These are all enormously popular programming languages, but each has what it does best along with its own disadvantages. For example, if you’re interested in building software that runs really quickly, you may learn C++. If you want to build and edit websites, you would maybe start by learning HTML. And, Java may be most helpful if you want to build video games. R Similarly, R has its strengths and weaknesses. R was designed to be helpful to those interested in statistical computing and graphics. That said, in its simplest form, R is a calculator. If you type ‘3 + 7’ into the R console and hit enter, R will tell you the answer to that math problem is ‘10.’ R is a calculator However, R is much more than just a calculator. It also has the ability to work with data, such as the information in spreadsheets. It’s able to tell you how many rows are in your column. It’s able to find the average age of individuals across a data set. It’s able to create plots to show you how many males or females are included in your data set. And beyond data summary, you can run statistical analyses, write your own software, and carry out complicated analyses start to finish in R. So, while it is a calculator, it is much more than a calculator. It is a place where you can do all of your data analysis. RStudio makes the process of doing an analysis in R easier. 13.0.2 RStudio RStudio is a free, integrated development environment (IDE) for R. Generally, IDEs are software applications that allow software developers to program more efficiently, putting everything the programmer needs in one place. With regards to RStudio specifically, RStudio has a space for the programmer to code, a separate space for that code to run (the Console), a place to see all the objects created in the current session (the Workspace), and a place to see Plots that have been generated. All of these spaces are viewable in a single window, simplifying programming and data analysis. Those who work at RStudio seek to develop tools that support analysts to perform trustworthy and high quality analysis. Their singular goal is to make your life programming in RStudio easier! RStudio IDE 13.0.3 RStudio Cloud RStudio Cloud is a version of RStudio that can run in the cloud. This means that regardless of what computer you’re on, you can access the analysis you were doing previously in your RStudio Cloud session. Other than that, it has many of the same features and is being developed by the same group of people who developed the version of RStudio you download and use on your individual laptop. This means that in RStudio Cloud, like in RStudio, you have four main components, each of which is visible in the same window. To review from the introductory lesson of this series of courses, RStudio Cloud has the following four main components: Scripting - where you write your code Console - where your code runs Environment - where you can see what objects have been created during your analysis Files - where you can see all the files that are part of your project RStudio Cloud There are additional features that you can play around with; however, one important feature to note is that whenever you create a plot in RStudio or RStudio Cloud, it will be visible in the plots tab at the bottom right-hand of your screen. RStudio Cloud plots 13.0.4 Basic History Knowing the background of a programming language often helps to add some context. So, very briefly, R first appeared in 1993 and was developed to be very similar to another programming language, S. R was initially written by Ross Ihaka and Robert Gentleman in the Department of Statistics at the University of Auckland in New Zealand. Since its inception, many people have contributed code and improvements to R. And, since 1997, the “R Core Team” is responsible for all modifications to the language. R is an open source language. This means that the language is free to use and the source code is available to the general public. As for RStudio, it was first released in 2011. It was founded by J.J. Allaire, who is the company’s current CEO. RStudio Cloud, the cloud-based version of RStudio, was first released for alpha testing (meaning it would have bugs and things that still needed to be fixed and will likely be updated significantly in the coming years) in 2017. R Basic History 13.0.5 Learning R Learning R can be difficult and frustrating. If you get stuck, you’re definitely not alone! The Internet and conversations with others can be very helpful to you! frustration is normal We just wanted to take a second to remind you that getting frustrated is normal and failure is expected. The goal here is to learn how to use R, not to memorize functions (there are too many functions and new ones being made all the time for this to even be possible!). failure is inevitable So, try things out on your own. Try to work through error messages when you’re stuck. But, if you can’t figure it out, ask questions of others who have more experience than you! try first; then ask questions "],["objects-in-r.html", "Chapter 14 Objects in R", " Chapter 14 Objects in R 14.0.1 What is an object? An object in R is something that contains information. In R there are a number of basic classes and types of objects. Classes - tell us what kind of data is being stored? Is it a number? A category? A word? Types - tell us what shape the data are being stored as. Is it a table? A series of items together (called a vector)? Or something with a different structure all together. 14.0.2 Storing objects In R, as with all programming languages, it is important to be able to store objects that we create so that we can use them in later code. The process of storing an object is called assignment, and it entails giving an object a name. For example, the following code creates an object called min_age and stores inside that object the value 21. It is equivalent to saying that min_age is equal to 21. min_age &lt;- 21 The &lt;- operator is called the assignment operator. After you run this code in R, you will see the object min_age show up in your Environment panel. Assignment Now if we tell R min_age it will print back to us the number 21. Having this minimum age variable stored in an object can be useful later if we have data where we only want to keep individuals who exceed this minimum age. 14.0.3 Printing objects Often we will want to print the contents of an object to see the information it contains. We can do this by clicking in the Console in RStudio Cloud (bottom left corner). The R prompt is indicated by the &gt; in the Console. This indicates that R is ready to accept a command from you. If we simply enter the number 21 at the R prompt, the 21 object will be printed, but you will not see an object come up under the Environment pane (top right corner). &gt; 21 [1] 21 &gt; min_age &lt;- 21 &gt; min_age [1] 21 The 1 in square brackets that gets displayed in the printed output is simply an index that is provided for convenience of reading in case the object contains several values. It indicates that the number 21 is the first number in this object. It also happens to be the only number in this object. Throughout this curriculum, when we display code without the &gt; indicating that we are not at the R prompt in the Console, we are emphasizing only the R command. When we display code with the &gt; indicating that we are at the R prompt in the Console, we want to emphasize the commands and how the output is displayed on the screen. 14.0.3.1 Classes of objects Five classes that you’ll be working with commonly are: Character - e.g. \"sharon\" \"marcus\" Integer - e.g. 1, 4 (whole numbers) Numeric - e.g. 1.2, 3.4, 4 Logical - only two options: TRUE or FALSE. Factor - red, red, blue, yellow (categories that could be repeated). We will describe each of these classes in more detail but first, why are these important anyway? 14.0.3.2 Why are classes important? Let’s discuss an example. If we asked: “What is Blue divided by 2 equal?” You would rightly tell us that question doesn’t even make sense and you can’t answer it. But if we asked: “What is 4 divided by 2 equal?”, then you could easily tell us 4/2 = 2. In the same way, R will throw errors at us if we ask it nonsensical questions or things that its not built to do. Because of this, we need to keep an eye on what kind of data classes we have, and what kinds of things we are asking R to do to make sure that those things make sense together. 14.0.4 Character Character objects in R can be created by surrounding a string in either double quotes or single quotes as in the following two examples. “This is a character object.” ‘This is also a character object.’ The example below shows how to store the above sentence character object in an object named my_char. my_char is a character vector of length 1. my_char &lt;- &quot;This is a character object.&quot; We can create a character vector named my_char_vec with multiple character objects using the concatenate function, c(). While we’ll discuss functions more in later lessons, the word concatenate means to link things together in a series, so this function links pieces of information together: my_char_vec &lt;- c(&quot;char object 1&quot;, &quot;char object 2&quot;) This character vector contains two different pieces of information. In R, the number of pieces of information in a vector is referred to as that vectors length. Thus, this vector is of length 2. 14.0.5 Integer Integers are whole numbers, such as 1, 23, or 1000. 1.2 is not an integer, as it contains a fraction of a number. Integer objects in R can be created by specifying an integer number followed by the letter “L”. The following creates an integer object called num and stores the value 1. num &lt;- 1L Without the letter “L”, the number will be recognized as a more general, numeric object (discussed below). We can create an integer vector with multiple items using the c function, the concatenation function. The following creates an integer vector of length 3 with the numbers 1, 10, and 3. num_vec &lt;- c(1L, 10L, 3L) We can also create an integer vector with the colon operator. The colon operator specifies to include all numbers between the value before the colon and the value after the colon. The following command creates an integer vector with the numbers 2, 3, 4, and 5. num_vec2 &lt;- 2:5 If we create longer vectors and print the output, we can see the use of having the square bracket indices at the beginning of the lines of the printed output. In this last example, we see that 4 is the first number in the vector, and 12 is the ninth number in the vector, as specified by the 9 in brackets to start the second line of output. &gt; 4:16 [1] 4 5 6 7 8 9 10 11 [9] 12 13 14 15 16 14.0.6 Numeric Numeric objects in R represent real numbers and are created by simply entering a number. Thus, while 1.2 is not an integer, it is a real number. Thus 1.2 could be stored as a numeric but not an integer. num1 &lt;- 1 num2 &lt;- 1.2 We can create a numeric vector with multiple items using the c function. num_vec &lt;- c(1.2, 9.8) As discussed previously, we can also use R as calculator. At the prompt, we can enter mathematical expressions without assignment to display the results as a calculator would. The operators for addition, subtraction, multiplication, division, and exponentiation in R are +, -, *, /, and ^ respectively. &gt; 1+5 [1] 6 &gt; 2-3 [1] -1 &gt; 4*2 [1] 8 &gt; 4/5 [1] 0.8 &gt; 3^2 [1] 9 14.0.7 Logical Logical objects in R represent true or false conditions and can be created by typing “TRUE” or “FALSE”. check_condition &lt;- TRUE check_condition &lt;- FALSE We can create a logical vector with multiple items using the c function. check_condition &lt;- c(TRUE, TRUE, FALSE) 14.0.8 Factor Factor objects contain information for categorical variables (e.g. color, shape), where there are a number of possible values the object can take, but these values are limited. For example, a categorical variable could include the colors of the rainbow. Here, values could be red, orange, yellow, green, blue, indigo, or violet. Thus, values could be one of seven different colors, but the categorical variable is limited to one of these seven values. To simplify this example and make factors explicitly clear, the following colors object is a character vector containing five pieces of color information. There are only two unique colors present: red and blue. These unique colors are called the levels of a factor. colors &lt;- c(&quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;) To create a factor object out of this character vector we can use the factor function or the as.factor function. Let’s try both and look at the objects created. &gt; colors_factor1 &lt;- factor(colors, levels = c(&quot;red&quot;, &quot;blue&quot;)) &gt; colors_factor1 [1] red red blue red blue Levels: red blue &gt; colors_factor2 &lt;- as.factor(colors) &gt; colors_factor2 [1] red red blue red blue Levels: blue red When we used the factor function we also specified the levels to be red and blue. The order of the levels we specified is important: first red, then blue. We can see that when we print this object the levels are listed in the order we specified. A quick way to create a factor object is with the coercion function as.factor. When we print this object, the levels are opposite to what we specified when we used the factor function because by default, the levels are specified in alphabetical order. Here the first level is blue and the second is red. The ordering of levels will be important in future courses when we cover data tidying, plotting, and statistical modeling. One last topic to cover with factors is labeling. We can control the displayed labels of a factor with another option with the factor function. This need often arises if we want to create a factor object from an integer object or from a character object with labels that we don’t like. In the example below, we see that we originally had ozone information encoded with integers. When we use the factor function to make a corresponding factor object, we specify both the unique levels present in the integer object and the desired labels with a character vector. The order of the specified levels should correspond to the order of the specified labels. The two examples, ozone_factor and ozone_factor2, create the same labeling of the original integer vector, but the order of the levels is different between the two approaches. In the first approach, the first level is low, the second is medium, and the third is high, which is the most natural ordering. In the second approach, the first level is medium, the second is low, and the third is high. &gt; ozone_levels &lt;- c(1,2,1,3,1,1) &gt; ozone_factor &lt;- factor(ozone_levels, levels = 1:3, labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) &gt; ozone_factor [1] low medium low high low low Levels: low medium high &gt; ozone_factor2 &lt;- factor(ozone_levels, levels = c(2,1,3), labels = c(&quot;medium&quot;, &quot;low&quot;, &quot;high&quot;)) &gt; ozone_factor2 [1] low medium low high low low Levels: medium low high 14.0.9 Data frames Now that we’ve covered common basic data classes, we will now discuss data frames. Data frames are a more complex data type than the simple vectors than we’ve seen so far. Data frames organize data into a rectangular format where each column corresponds to a single variable and each row corresponds to an observation. So a row of a data frame contains an observation’s values for all variables. An example of a data frame is shown below: Data frame example We see along the columns different variables related to car properties, and each row gives information on those properties for a particular car model. Every column in a data frame is a simple vector of values all from the same class. Most often, the data that we work with can be represented with data frames. You will learn more about working with data frames in subsequent chapters. 14.0.10 Missing values The last topic that we should discuss in our introduction to R objects is missing values. During nearly any type of data collection, there is information missing for one or more variables. Thus, it is important to understand how R handles missing values. Most missing values that you will deal with are encoded with NA in R. Below are some examples of creating objects of the various basic types we discussed above that contain missing values. &gt; char_vec &lt;- c(NA, &quot;two&quot;, &quot;four&quot;) &gt; char_vec [1] NA &quot;two&quot; &quot;four&quot; &gt; num_vec &lt;- c(1L, 10L, NA, 3L) &gt; num_vec [1] 1 10 NA 3 &gt; num_vec &lt;- c(1.2, 9.8, NA) &gt; num_vec [1] 1.2 9.8 NA &gt; logi_vec &lt;- c(TRUE, NA, FALSE, FALSE) &gt; logi_vec [1] TRUE NA FALSE FALSE &gt; factor_vec &lt;- as.factor(c(NA, &quot;apple&quot;, &quot;banana&quot;)) &gt; factor_vec [1] &lt;NA&gt; apple banana Levels: apple banana Another missing value that can arise in R is NaN which stands for “not a number.” This can arise in mathematical calculations, such as 0 divided by 0. &gt; 0/0 [1] NaN 14.0.11 Determining the class and shape of an object In this lesson so far we have discussed how to create the five main classes of objects in R; however, we haven’t yet described how to determine the class of an object once its been stored. To do so, there are three different strategies we’ll show you. You can use the function class() and specify the class of the object within the parentheses: &gt; min_age &lt;- 21 &gt; class(min_age) [1] &quot;numeric&quot; &gt; min_age &lt;- 21L &gt; class(min_age) [1] &quot;integer&quot; &gt; colors &lt;- c(&quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;) &gt; class(colors) [1] &quot;character&quot; &gt; colors_factor1 &lt;- factor(colors, levels = c(&quot;red&quot;, &quot;blue&quot;)) &gt; class(colors_factor1) [1] &quot;factor&quot; As you can see, the class of the object specified within the parentheses is the class of that object. You can use a function str() which will tell you its structure. This will tell you more about the shape of an object, how long or wide it is, etc. str(colors) &gt; chr [1:5] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;red&quot; ... Lastly, you can always look at your Environment panel and sometimes click on an object to see more about it. This strategy doesn’t require you to run any command, but sometimes it won’t have information that using str() or class() would tell you. 14.0.11.1 Types of objects We store these different classes of objects in different ways. The ways this information is stored is referred to as the type of object. When talking about objects in R, it may be helpful to think of actual objects in every day life for comparison. For example, think of three objects: a bucket, a pot you would cook with, and a backpack. These three objects are clearly designed for and carry out different purposes. The bucket may be used to carry water to clean your floor, the pot to cook pasta, and the backpack to carry notebooks, but, we can agree that they are all objects, just different types of objects. The water, pasta, and notebooks would be the information contained in the object. In this real-life example, the “class” of the information may be “liquid”, “food”, and “paper”. Objects in real-life analogy The simplest type of object in R is called a vector, which is an object that can contain multiple items. Generally, each individual vector can only contain objects of the same class, but a certain type of vector, called a list, can contain objects of different classes. You will learn about lists in a later lesson. For now, it’s not important to understand the details of that last paragraph, but it is important to know that there are different types of objects and that these objects each hold information of a specific class. 14.0.12 Summary In this lesson, we’ve discussed that within R information can be assigned to objects. We’ve covered the five main classes of objects in R and have started to touch on the different types of objects in R, but will discuss this in greater detail in later lessons in this course. We’ve discussed how to create each class of object in R as well as each class’ unique properties. Finally, we discussed how to determine the class of an object in R using the function class(). "],["working-with-logicals.html", "Chapter 15 Working with Logicals", " Chapter 15 Working with Logicals Earlier in this course, you learned that one of the basic classes of objects in R is the class of logical objects which contain TRUE and FALSE values. Logicals come up very frequently in data management and analysis because they form the basis of conditional operations (if a condition is met, perform a task) and are instrumental in data exploration, visualization, and analysis. In this lesson, we will cover the tools you will need to work with logical values in R. As you work through this lesson, you’ll be inundated with TRUE and FALSE a lot. That is because there are only two options when it comes to logicals. However, these are incredibly important and helpful class of objects. So, take your time to understand each example. Copying and pasting the code into your own RStudio, running it, and spending time to understand the output will really help you understand how to work with logicals! 15.0.1 Logical operators One of the most common ways to create and combine logical objects is to use logical operators. Broadly speaking, operators are symbols that indicate some action. We introduced arithmetic operators in an earlier lesson for performing routine arithmetic calculations. There was + for addition, - for subtraction, * for multiplication, / for division, and ^ for exponentiation. Logical operators in R perform actions relating to logic checking and include the following: !: the “not” operator &amp;: the “and” operator |: the “or” operator (Shift + backslash() ) ==: the “equals” operator !=: the “not equal” operator &gt;: the “greater than” operator &gt;=: the “greater than or equal to” operator &lt;: the “less than” operator &lt;=: the “less than or equal to” operator %in%: the “contained in” operator Logical operators are used with one to several R objects in order to create logical objects. These logical objects are the result of checking conditions, and they store the answers to yes/no questions that you may ask throughout your work. Let’s look at several examples. We have data on ages of some students, and we have stored this information in the ages object. We have information on a common age cutoff that is applied to all students for a particular activity. This is stored in the common_cutoff object. For another activity, we have individualized age cutoffs for each student. This is stored in the indiv_cutoffs object. Let’s ask several yes/no questions relating to this data. ages &lt;- c(12, 17, 16, 13, 14) common_cutoff &lt;- 13 indiv_cutoffs &lt;- c(12, 12, 14, 14, 14) Do the students’ ages equal the cutoff? To answer this, we would use the “equals” operator ==. (Note: the equals operator requires two equals signs (==). You’ll recall that a single equals sign (=) is used for object assignment and is equivalent to &lt;- . Whenever you want to ask if two things are equal be sure you have both equals signs in your code!) Here, each number in the ages object is compared to 13. Only the fourth student meets this condition. &gt; ages == common_cutoff [1] FALSE FALSE FALSE TRUE FALSE The output from this code prints “TRUE” for the individual (the fourth person) who meets this condition. Do the students’ ages equal the individualized cutoffs? Here, each number in the ages object is compared to the corresponding number in the indiv_cutoffs vector. Only the first and fifth students meet this condition. &gt; ages == indiv_cutoffs [1] TRUE FALSE FALSE FALSE TRUE This is obvious in the output from R, where the first and the fifth values are TRUE, while the rest are FALSE. Usually cutoffs are a bound rather than a specification of an equality, so we may instead ask if the students are older than the cutoff by using the “greater than” operator &gt;. &gt; ages &gt; common_cutoff [1] FALSE TRUE TRUE FALSE TRUE &gt; ages &gt; indiv_cutoffs [1] FALSE TRUE TRUE FALSE FALSE Are they at least as old as the cutoff? We can answer this with the “greater than or equal to” operator &gt;=. &gt; ages &gt;= common_cutoff [1] FALSE TRUE TRUE TRUE TRUE &gt; ages &gt;= indiv_cutoffs [1] TRUE TRUE TRUE FALSE TRUE If the cutoffs are upper bounds instead of lower bounds, we can answer similar questions as above using the “less than” &lt; and “less than or equal to” &lt;= operators. &gt; ages &lt; common_cutoff [1] TRUE FALSE FALSE FALSE FALSE &gt; ages &lt; indiv_cutoffs [1] FALSE FALSE FALSE TRUE FALSE &gt; ages &lt;= common_cutoff [1] TRUE FALSE FALSE TRUE FALSE &gt; ages &lt;= indiv_cutoffs [1] TRUE FALSE FALSE TRUE TRUE So far we have treated the common cutoff and the individualized cutoffs separately, and we have thus only used one logical operator at the time. We can use several logical operators simultaneously to answer more complex yes/no questions. Are the students older than the common cutoff and the individualized cutoffs? We can combine the “greater than” operator with the “and” &amp; operator. &gt; ages &gt; common_cutoff &amp; ages &gt; indiv_cutoffs [1] FALSE TRUE TRUE FALSE FALSE Are the students older than the common cutoff or the individualized cutoffs? We can combine the “greater than” operator with the “or” | operator. &gt; ages &gt; common_cutoff | ages &gt; indiv_cutoffs [1] FALSE TRUE TRUE FALSE TRUE Are the students older than the common cutoff but not the individualized cutoffs? We can answer this with the “not” operator or without it by reasoning through with the inequalities. In using the “not” operator, it is a good idea to wrap the condition that you are negating in parentheses to enhance clarity and avoid errors. &gt; ages &gt; common_cutoff &amp; !(ages &gt; indiv_cutoffs) [1] FALSE FALSE FALSE FALSE TRUE &gt; ages &gt; common_cutoff &amp; ages &lt;= indiv_cutoffs [1] FALSE FALSE FALSE FALSE TRUE When working with complex logical expressions, it can help to store different parts of the expression in their own objects. In reproducing the example above, we have stored the result of the logical operation dealing with the common cutoff in the meets_common_cut logical object. We have also stored the result of the logical operation dealing with the individual cutoffs in the not_meets_indiv_cut logical object. These two objects can be combined at the end in a more readable expression. &gt; meets_common_cut &lt;- ages &gt; common_cutoff &gt; not_meets_indiv_cut &lt;- !(ages &gt; indiv_cutoffs) &gt; meets_common_cut [1] FALSE TRUE TRUE FALSE TRUE &gt; not_meets_indiv_cut [1] TRUE FALSE FALSE TRUE TRUE &gt; meets_common_cut &amp; not_meets_indiv_cut [1] FALSE FALSE FALSE FALSE TRUE Although these examples have all used numbers, logical operators can also be used for character and factor objects. Let’s start with character objects. For comparing character objects, you will primarily use the “equals” == and “not equal” != operators. For example, we have a character vector of colors. Are the colors “red”? &gt; colors &lt;- c(&quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;) &gt; colors == &quot;red&quot; [1] TRUE TRUE FALSE FALSE FALSE Are the colors not “blue”? &gt; colors != &quot;blue&quot; [1] TRUE TRUE TRUE TRUE FALSE Here it is useful to introduce the “contained in” operator %in%. This operator checks if the elements in the left hand object are contained in the right hand object. Are “red” and “purple” contained in this set of colors? The length of the output is the same as the length of the left hand side. We ask about two colors, “red” and “purple”, and we see that “red” is contained in the colors object but “purple” is not. &gt; c(&quot;red&quot;, &quot;purple&quot;) %in% colors [1] TRUE FALSE If we had reversed the command, we would instead be asking, “Are the colors in the colors object contained in the red and purple set?” Only the instances of “red” will be marked as TRUE. &gt; colors %in% c(&quot;red&quot;, &quot;purple&quot;) [1] TRUE TRUE FALSE FALSE FALSE When dealing with logical operations with factors, we can only use the “equals” == and “not equal” != operators. Usually we will want to compare factor objects with values of their labels. Let’s look at logical operations for the following factor object containing height category information. &gt; height_factor &lt;- factor(c(2,1,2,3,1), levels = 1:3, labels = c(&quot;short&quot;, &quot;average&quot;, &quot;tall&quot;)) &gt; height_factor [1] average short average tall short Levels: short average tall Although we create this factor object from integers, comparing it to the value 1 will not give desired results. The intention in comparing it to the integer 1 is to mark the short individuals with TRUE. We can do this by either coercing the factor object to an integer object with as.integer or by comparing the factor to the string label “short”. &gt; height_factor == 1 [1] FALSE FALSE FALSE FALSE FALSE &gt; as.integer(height_factor) [1] 2 1 2 3 1 ## coerce object to be an integer &gt; as.integer(height_factor) == 1 [1] FALSE TRUE FALSE FALSE TRUE ## compare to label directly &gt; height_factor == &quot;short&quot; [1] FALSE TRUE FALSE FALSE TRUE When we coerce the object to be an integer, we get the expected output. The second and final outputs are TRUE, corresponding to the values of “1” in the height_factor object. The output is the same for when the labels are directly compared. The output here returns TRUE for any places in the height_vector object where the factor label is (equal to) “short”. 15.0.2 Logical functions So far we have used logical operators to ask yes/no questions on a unit-by-unit basis. That is, asking the question for each data observation. This has given us TRUE/FALSE answers for each unit. We might also want to summarize the results of these multiple responses with questions such as “Do all units meet the condition?” or “Do any (at least one) units meet the condition?” For the first question, “Do all units meet the condition?”, we can use the all function. The all function takes a logical object as input and returns TRUE if all values in the logical object are TRUE, and it returns FALSE otherwise. Are all student ages equal to the individual cutoffs? Are all ages greater than or equal to zero? &gt; all(ages == indiv_cutoffs) [1] FALSE &gt; all(ages &gt;= 0) [1] TRUE For the second question, “Do any units meet the condition?”, we can use the any function. The any function takes a logical object as input and returns TRUE if at least one of the values in the logical object is TRUE, and it returns FALSE otherwise. Are any of the student ages equal to the common cutoff? Are any ages greater than 100? &gt; any(ages == common_cutoff) [1] TRUE &gt; any(ages &gt; 100) [1] FALSE Often we will want to combine the asking of yes/no questions with “who” and “how many” questions. Who meets the condition? How many units meet the condition? For the first question, “Who meets the condition?”, we can use the which function. The which function takes a logical object as input and returns the indices of TRUE values. In this example, we see that the first and second colors are the ones that are contained within the red and purple set. &gt; colors %in% c(&quot;red&quot;, &quot;purple&quot;) [1] TRUE TRUE FALSE FALSE FALSE &gt; which(colors %in% c(&quot;red&quot;, &quot;purple&quot;)) [1] 1 2 To answer, “How many units meet this condition?”, we can make use of the sum and mean functions. The idea here is that logical values have a correspondence with the integer values of 0 and 1. TRUE values correspond to 1, and FALSE values correspond to 0. Thus when we create a logical object, we can use sum to count the number of TRUE values, and we can use mean to compute the fraction of TRUE values. ## assign logical to ages that are greater than ## or equal to indiv_cutoffs &gt; meets_indiv_cut &lt;- ages &gt;= indiv_cutoffs &gt; meets_indiv_cut [1] TRUE TRUE TRUE FALSE TRUE ## sum that object &gt; sum(meets_indiv_cut) [1] 4 ## get the mean of that object &gt; mean(meets_indiv_cut) [1] 0.8 Here, the sum of the meets_indiv_cut is 4. When you sum a logical, R returns the number of TRUE responses. Similarly, when you take the mean() of an object of class logical, you get the proportion of responses that were TRUE. Here, that’s 4 out of 5, or 0.8. 15.0.3 Summary This lesson walked you through how to work with operators and logical objects. This will be incredibly helpful as you start to manipulate and clean data. Having a thorough understanding of this class of objects and how to work with them will serve you well going forward. "],["data-frames-1.html", "Chapter 16 Data Frames 16.1 Exploring data frames", " Chapter 16 Data Frames A majority of the time that you are working with data in R you will have it in the form of a data frame. Data frames can contain different types of variables in a rectangular format, much like how spreadsheets are. A data frames’ size can be described in terms of rows (across) and columns (up/down). Rows are often called observations and columns are called variables. ottrpal::include_slide(&quot;https://docs.google.com/presentation/d/1Q47qnIkVzE-JzCEE5Lm54P6yqReg09QJdr7kiFyCbGc/edit#slide=id.g313d649efe_0_38&quot;) *Much of this chapter is paraphrased or inspired by content from the Childhood Cancer Data Lab. 16.1 Exploring data frames In R there are built in data frames that you can use to play with. In this lesson, we will use some of these to get comfortable with data frames. As a data scientist, one of your main skills will be constantly looking at and evaluating your data! In RStudio, you can see your data frame by clicking on the name of your data frame in the Environment pane. Some useful functions for exploring our data frame include: head() to see the first 6 rows of a data frame. Additional arguments supplied can change the number of rows. tail() to see the last 6 rows of a data frame. Additional arguments supplied can change the number of rows. names() to see the column names of the data frame. nrow() to see how many rows are in the data frame ncol() to see how many columns are in the data frame. Try out these functions to explore your data frame by putting the iris data frame in them. head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa We can additionally explore overall properties of the data frame with two different functions: summary() and str(). The summary() function gives us a summary of each variable in the data frame. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## str() function gives us information about the structure and variables included in the iris data frame. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 16.1.1 Subsetting data frames There are multiple ways you can subset your data frame if you want to only look at certain parts of it. 16.1.1.1 $ for extracting columns Each column in this data frame can behave as a vector and we can easily pull it out from the data frame by using a $. iris$Species ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica Note that $ work for any R objects that have column names. But don’t work if there aren’t column names. When you look at a data frame you will see that there are names to the columns. Try using the colnames() function to explore the column names of the iris data frame. colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; 16.1.1.2 Brackets for indexing Brackets can be used to index data frames or other objects. In the case of data frames, you can subset data frames like below: In this example, this will extract the second row’s data and the 1st column. iris[2, 1] ## [1] 4.9 Subsetting always goes [row, column]. If we want the whole row or whole column to be extract we can leave the other spot blank like this: In this example, the first row and all the columns of that row will be extracted. iris[1, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa In this example, all rows of the 3rd column will be extracted. iris[, 3] ## [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4 ## [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2 ## [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0 ## [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0 ## [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0 ## [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3 ## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0 ## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 ## [145] 5.7 5.2 5.0 5.2 5.4 5.1 16.1.2 First plot! Now that we have some basics down, let’s preview how we can easily make visuals with R. “A picture is worth a thousand words” is also true when it comes to data! In this quick plot example, we can show how Sepal Length is related to Sepal Width by extracting the data using $. Additionally we can color code the data points by Species by using the col argument. plot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species) Don’t worry too much about the specifics of plotting. We will go into much more detail about how to make visuals of your data later. This is just to illustrate how R makes data visuals easy to make! 16.1.3 What about matrices and tibbles? At this point, we should mention some cousins of the data frame: matrices and tibbles. All three of these types of objects: data frames, matrices, and tibbles contain two dimensions: rows and columns. And often you may be able to handle them similarly. But, they do have some differences that are worth knowing about! 16.1.3.1 What’s a matrix? A matrix is like a data frame but it needs to contain all the same data type. So it can be all numeric, all character or etc. A data frame by contrast can have one column that is string, another that is factor, and so on. Let’s experiment with what this means in practical terms and start out by making a very small data frame. df &lt;- data.frame(num = 1:4, y = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;)) df ## num y ## 1 1 red ## 2 2 blue ## 3 3 green ## 4 4 purple str(df) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ num: int 1 2 3 4 ## $ y : chr &quot;red&quot; &quot;blue&quot; &quot;green&quot; &quot;purple&quot; We’ve made our data frame with two types of data: a numeric (integer) column and a character column. Let’s see what happens if we coerce this to a matrix a_matrix &lt;- as.matrix(df) a_matrix ## num y ## [1,] &quot;1&quot; &quot;red&quot; ## [2,] &quot;2&quot; &quot;blue&quot; ## [3,] &quot;3&quot; &quot;green&quot; ## [4,] &quot;4&quot; &quot;purple&quot; str(a_matrix) ## chr [1:4, 1:2] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;red&quot; &quot;blue&quot; &quot;green&quot; &quot;purple&quot; ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:2] &quot;num&quot; &quot;y&quot; Notice that this whole matrix is now a character but still does have two dimensions! 16.1.3.2 What’s a tibble? A tibble is super similar to a data frame. It’s the tidyverse brand of data frame. According to the people who develop the tidyverse: Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). They are similar enough, that for most instances, especially when you are working with tidyverse functions, you can treat them the same. However if you get a peculiar error when trying to use a tibble with a non-tidyverse function, you may need to use the as.data.frame() function to coerce your tibble to a data frame. But you shouldn’t encounter this problem too often. "],["basic-commands-in-r.html", "Chapter 17 Basic Commands in R 17.1 A brief intro to the apply family of functions", " Chapter 17 Basic Commands in R Now that we’ve covered some essentials about R objects, we’ll go over some basic commands that will be helpful in working with data. 17.0.1 Functions In working with data, we will be making substantial use of functions. Functions in R carry out some task. We’ve already introduced you to a handful of functions like class(), head(), str(), getwd(), to name a few. Functions are always a word (or set of words connected by underscores or periods followed by a set of parentheses, so the general structure of a function in R would look something like this: function(input) function_name(input) Input to a function in R is known as an argument. Functions require at least one argument, but can require multiple different arguments, depending on the function. At this point you may be saying, “but when we used getwd() we didn’t specify an argument!” - And you’d be correct! We didn’t specify one, but the sneaky thing about arguments is that often if you don’t specify one, that means one is automatically being chosen for you (called a default). To see all about a function’s arguments and its defaults, see its help page by using ?function. To visually understand the anatomy of a function call (a term that describes the using of a function), let’s look at the following example: mean(x, trim = 0.1) We have an object x that should contain numbers, and we want to compute the mean of these numbers with the mean function. As stated above, all of the information inside the parentheses are function inputs (also called arguments), and they are separated by commas. In this command, I have supplied the object x and an additional argument trim that I set to be 0.1. The trim argument calls for a number between 0 and 0.5 and specifies the fraction of the observations in x to trim from the upper and lower ends of the data. Here, by including the trim argument, I am specifying that I want to take the mean of the middle 80% of the data. 17.0.2 What is this object? If someone were to write down a mystery noun for us to guess, our first question would likely be: “Is it a person, place, or thing?” When working with R objects, we will initially want similar types of information. Here we will go over some functions that can help in this regard. As we covered before, the class() function returns the class of an R object. This is useful for determining if an object is an atomic vector, list, or some other type of object. If it is an atomic vector, this function tells you the type. &gt; x &lt;- 1:10 &gt; class(x) [1] &quot;integer&quot; &gt; y &lt;- c(1.1,2.2) &gt; class(y) [1] &quot;numeric&quot; &gt; class(mtcars) [1] &quot;data.frame&quot; As discussed, The str() function stands for “structure”, and it returns a description of the structure of an object. It tells you the class of an object, its size, and a preview of different components of the object. For example, when we call the str() function on a data frame object (mtcars), we see that its class is data.frame(), it has 32 rows and 11 columns, and a preview of each of the 11 columns, including the class of each column. In this example, all of the columns are numeric variables relating to features of different models of cars. &gt; str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 17.0.3 How big is this object? After we determine generally what an object is, it is useful to know how much information it contains, how big it is. The dim() function returns the dimensions of a rectangular object, such as a matrix or a data frame. The output is an integer vector with two components: first is the number of rows (which can also be obtained with nrow()), and second is the number of columns (which can also be obtained with ncol()). We saw previously that the str() function provides the same information and more, so why would we use these functions instead? The str() function provides this information by printing it to the screen for us to visually see, but it does not extract this information directly. If we need to use the dimensions later in the analysis as a variable, these functions provide a direct way to store this information. &gt; dim(mtcars) [1] 32 11 &gt; nrow(mtcars) [1] 32 &gt; ncol(mtcars) [1] 11 The length() function returns the number of items in a vector object. We talked about this briefly last lesson that the number of things in your object is referred to as its length. Here, we can quickly calculate the length of an object by calling the length() function. &gt; x &lt;- c(1, 10, 3) &gt; length(x) [1] 3 17.0.4 Are there named features of this object? Another way to explore an object in R is to see what components it has. In R, these components are designated with names. The names() function can be used to get and set the names of an R object, most often an atomic vector or a list. For example, we can create an R object called prize_money that contains the prize money for first, second, and third places: prize_money &lt;- c(1000, 500, 250) If we want to label this vector with the prizes, we can use names() combined with the assignment operator &lt;- and a character vector of labels: names(prize_money) &lt;- c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;) Later in our work, if we want to remind ourselves of the labels, we can use the names() function by itself, which will print the names for the object. &gt; names(prize_money) [1] &quot;first&quot; &quot;second&quot; &quot;third&quot; Note that in many situations, it will be better practice to encapsulate the above information in a two-column data frame instead of a named vector as below. prize_info &lt;- data.frame( money = c(1000,500,250), place = c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;) ) This is more convenient for further work if you have other objects that have information on first, second, or third placing, but not prize money information. You’ll learn more about these concepts when you learn about “tidy data” in a later chapter. The colnames() and rownames() functions act analogously to the names() function but are used for the column labels and row labels of a matrix or data frame. The numbers in square brackets at the beginning of the lines of printed output indicate the index of the first observation on the line. So for the row names, we can see that “Duster 360” is the seventh element. &gt; colnames(mtcars) [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; &gt; rownames(mtcars) [1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; [4] &quot;Hornet 4 Drive&quot; &quot;Hornet Sportabout&quot; &quot;Valiant&quot; [7] &quot;Duster 360&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; [10] &quot;Merc 280&quot; &quot;Merc 280C&quot; &quot;Merc 450SE&quot; [13] &quot;Merc 450SL&quot; &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; [16] &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; &quot;Fiat 128&quot; [19] &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; &quot;Toyota Corona&quot; [22] &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; &quot;Camaro Z28&quot; [25] &quot;Pontiac Firebird&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; [28] &quot;Lotus Europa&quot; &quot;Ford Pantera L&quot; &quot;Ferrari Dino&quot; [31] &quot;Maserati Bora&quot; &quot;Volvo 142E&quot; colnames() and rownames() functions 17.0.5 What does this object look like? Sometimes we may just want to see the information contained in an object. Here we will discuss functions that allow you to see parts of objects. The print() function displays the entire contents of an object. print(mtcars) Recall that in R, the Console is where commands can be typed and entered for R to run. When R is ready to accept a command, a greater than sign will be displayed. An alternative to calling the print function is to simply type the name of the object in the Console and press enter. In general printing an entire object is not advisable just in case the object is quite large. In this case your screen would overflow with text! mtcars printing objects’ contents to the screen Safer alternatives to printing are the head and tail functions. The head function displays the beginning of an object. By default, it shows the first 6 items. If the object is a vector, head shows the first 6 entries. If the object is a rectangle, such as a matrix or a data frame, head shows the first 6 rows. The tail function is analogous to head but for the end of the object. head() and tail() can be used to see a portion of the data The summary function computes summary statistics for numeric data and performs tabulations for categorical data, which are called factors in R. The summary() function summarizes data The unique() function shows only the unique elements of an object. For vectors, this returns the set of unique elements. For rectangles such as matrices and data frames, this returns the unique rows. This function is useful if we want to check the coding of our data. If we have sex information, then we expect the result of unique to be two elements. If not, there is likely some data cleaning that must be done. The unique() function is also useful for simply exploring the values that a variable can take. In the example below, we can see that in the mtcars data frame, there are only cars with 6, 4, and 8 cylinders. Note that to extract the column corresponding to cylinders, we used a dollar sign followed by the column name: $cyl. This is an example of subsetting that you will learn in later lessons. The unique() shows unique elements of an object 17.0.6 Errors, Warnings, and Messages In R, there are three types information that R may return to you to your screen to provide you with additional information. These come in the form of errors, warnings, and messages. While they will often look similar to one another, it’s important to understand the difference between them. The most serious of these messages is an error message. Errors indicate that the code you tried to run did not run successfully. If you receive an error message, you should carefully look back at your code to see what went wrong. Error messages cannot be ignored as they indicate that there was no way for the code to run. Something has to be fixed before moving forward. For example, the code here produces an error, since mtca is not a data frame or object in R. errors Warnings are generally less serious than error messages. They are generated when the code executes (meaning, it runs without producing an error and stopping), but produces something unexpected. Warning messages should always be read, and then you, the person writing the code, has the option to decide whether or not the code that has generated the warning needs to be re-written. For example, the log function is only defined for numbers greater than zero. If, in R, you try to take the log of a negative number, you get an output (NaN): log(-1) This output means the code executed (there was no error), but you also get a warning letting you know that NaNs were produced. If you meant to take the log of a negative number, you would leave the code as is. However, if you did not intend to do this, the warning message helps clue you into the fact that you may want to revisit your code. warnings Last but not least, messages, in general, are simply there to provide you with more information. They do not indicate that you have done anything wrong. For example, if you were to run a function that creates a directory if it does not yet exist, the function may provide you a message informing you whenever a new directory has been created. This message would just be there to provide you with more information. No further action is generally necessary when a message is provided. messages Note that all three are in the same font and same color, so they’ll look similar in your RStudio Cloud console. Over time, you’ll get more comfortable dealing with and understanding the difference between the three. For now, be sure that if you get an error, your code did not execute successfully. Go back and find what caused the error. 17.1 A brief intro to the apply family of functions *This section is repurposed from the Childhood Cancer Data Lab’s section on apply functions.. In base R, the apply family of functions can be an alternative methods for performing transformations across a data frame, matrix or other object structures. One of this family is (shockingly) the function apply(), which operates on matrices. A matrix is similar to a data frame in that it is a rectangular table of data, but it has an additional constraint: rather than each column having a type, ALL data in a matrix has the same type. Let’s make a data frame with some data. df &lt;- data.frame(x = 1:4, y = 5:8, z = 10:13) Converting it to a matrix will require us to make them all the same type. We can coerce it into a matrix using as.matrix(), in which case R will pick a type that it can convert everything to. What does it choose? a_matrix &lt;- as.matrix(df) # Explore the structure of the `gene_matrix` object str(a_matrix) ## int [1:4, 1:3] 1 2 3 4 5 6 7 8 10 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:3] &quot;x&quot; &quot;y&quot; &quot;z&quot; Now that the matrix is all numbers, we can do things like calculate the column or row statistics using apply(). The first argument to apply() is the data object we want to work on. The third argument is the function we will apply to each row or column of the data object. The second argument specifies whether we are applying the function across rows or across columns (1 for rows, 2 for columns). # Calculate row means row_means &lt;- apply(a_matrix, 1, mean) # Notice we are using 1 here # How long will `gene_means` be? length(row_means) ## [1] 4 Now let’s investigate the same set up, but use 2 to apply over the columns of our matrix. # Calculate column means col_means &lt;- apply(a_matrix, 2, mean) # Notice we use 2 here # How long will `col_means` be? length(col_means) ## [1] 3 Although the apply functions may not be as easy to use as the tidyverse functions, for some applications, apply methods can be better suited. In this course, we will not delve too deeply into the various other apply functions (tapply(), lapply(), etc.) but you can read more information about them here. "],["getting-help-in-r.html", "Chapter 18 Getting Help in R", " Chapter 18 Getting Help in R In an earlier chapter, we discussed some basic guidelines for carrying out an effective Google Search. Now that you’ve been introduced to the basics of R, we wanted to guide you to some incredibly helpful resources that can help you work through issues when you’re trying to write your own R code. 18.0.1 R Help: ? To access documentation directly within RStudio, you can type a question mark followed by the function, dataset, or object within the R Console directly. The output for this documentation will display in the Help window at the bottom-right hand of RStudio. For example, earlier in this course, you were introduced to the function summary() in R. If you later can’t remember what this function does, but you can remember the function, you can always type ?summary in your Console. The following will display in the Help tab: ?summary documentation In this documentation we see: A general description of what the summary() function can be used for What package the function is from – “base” means it’s part of the base installation in R, rather than from a specific package The syntax you should use to carry out this function Some sample code As you work in R, you’ll find that some documentation is more helpful than others. Some packages and functions have incredibly helpful documentation pages. Others are less helpful. A thing to remember is that humans are responsible for writing this documentation. It’s great when someone who’s really great at documenting software or who has the time to do so write great documentation pages. However, for those times when the documentation proves less than helpful, there are other places you can look to for help. 18.0.2 Debugging code It is likely that after looking at R documentation, you still will have questions, or might not know exactly how to fix your code. For more tips on debugging your code, go paraphrase this guide originally written by the Childhood Cancer Data Lab. 1) Carefully read any and all error messages This may seem like a silly thing to include as a tip, but it’s very easy to gloss over an error message without actually reading it. Often, R may be telling you exactly what is wrong, but if you don’t take the time to understand what the error message means, you will have trouble fixing the error. Error messages often refer to R terms (e.g.. “argument”, “directory”) so if you need a refresher on what some terms mean, we recommend going through one of the intro to R tutorials we recommend. Secondly, realize that just because you don’t receive an error message, doesn’t mean that your code did what you intended it to. You also will need to carefully review your code (and your results) to try to find “silent” bugs (situations where R did exactly what you asked, but you didn’t get what you intended). 2) Identify which line and phrase of code is the source of the error. If you ran many lines of code, you may not know which part of your code is the origin of the error message. Isolating the source of the error and trying to better understand your problem should be your first course of action. The best way to determine this, is by running each line, and each phrase by itself, one at a time. Chunk-out your code and test the individual bits of code. Do you have a lot of lines of code, a lot of arguments, or multiple functions at once? Try each piece by itself to narrow down what piece appears to be the origin of the problem. 3) Be sure that the code you think you have run has all successfully run and in order. It could be that the problem with your code isn’t that it doesn’t work as it is written, but that you didn’t run it or didn’t run it in the correct order. This should be one of the first things you check, while checking that the objects that you believe should be in your environment, are in your environment. It’s also good practice to be periodically quitting your current R session and starting a new one, in addition to clearing your R notebook output. If you are encountering problems and haven’t refreshed your R session, you may want to do that before further troubleshooting. In the course of troubleshooting, you will want to re-run all of your code, perhaps many many many times in order to get to the bottom of the problem. 4) Google your error message The main advantage to Googling your errors is that you are likely not the first person to encounter the problem. Certain phrases and terms in the error message will yield more fruitful search results than others. 5) Look at the documentation for a function to make sure you are using it correctly Once you’ve better determined the origin of the problem, you should use whatever documentation is available to you regarding the problematic code. When using a new function from a package you are unfamiliar with, it’s worthwhile to skim through the documentation so you know how to properly use the functions. For base R functions, Tidyverse functions, and some Bioconductor packages, the documentation will give you a lot of the information you need. However, you will also likely find that not all documentation is thorough or clear. As we discussed, objects have structures and types. Having input that doesn’t match the requirements that a function has can be a common source of errors. Pay special attention to what the documentation says about what kind of input and output the function is designed to use. Use the RStudio help bar Here’s a screenshot from the help window in RStudio. Note that here we searched for the levels function. R documentation includes information about what the expected arguments are as well as examples of how to use a function. Note here that this documentation tells us that the input for x is probably a factor. For Bioconductor package functions, look at their page on bioconductor.org. The documentation on Bioconductor pages have information that can be valuable for troubleshooting. Vignettes can have good example workflows to get started with (can use the browseVignettes function for RStudio to open them). In addition, every Bioconductor package has a PDF reference where all the functions and objects for that package are described. They can take some getting used to, but generally can have helpful information. 6) Google it again Because it’s unlikely your first attempt at Googling will lead you straight to an answer; this is something you should continue to try with different wordings. Through trial and error, and also Google algorithms learning about what you look for, your search results can eventually lead you to helpful examples and forums. 7) Ask for help! Asking for help is a skill in itself! As we’ve mentioned, data science is best performed as a part of a community. But how do you ask for help in a way that most effectively helps you as well as the person who is trying to help you? Let’s discuss that! 18.0.3 Tips on asking questions! In the introduction to this course, we discussed that asking questions is central to data science and it’s also a crucial part of learning programming! When you are working on something, you should first ask these questions to yourself. But data science is best done as a community effort, so before you reach the point of being crazily frustrated, ask a peer or a more experienced coder for help! When you are troubleshooting code there are few things you should ask yourself (and tell others you are asking). Having this information together will help you reach a solution more efficiently! 18.0.3.1 Things to include in your request for help: What is the goal of this code? What are you trying to do? Is this the best or only way to do that? Often times in R there are many ways to achieve the same outcome, so it may be worth considering if there’s another solution that bypasses the problem you are having. What is the code that is producing this error? What part of the code is sending the error you are seeing? Try Chunking your code – trying each part piece by piece, to zero in on the problematic part. What is the error or problematic outcome? What are you seeing that is making you think something is wrong? Is it an error message? if so copy and paste that error message verbatim. Is it data that’s formatted not how it should be? Show what that looks like either with a screenshot or example. Can someone reproduce the error I’m getting with the code and data I’m giving them? Ideally you can share code that someone else can easily copy and paste and run in their own R console. Try to recall all that’s necessary to reproduce the error that you are getting. If the code that you are running is too long and intensive to send, could you send a smaller toy example that illustrates the same concept and error that you are seeing? This is called a reprex or reproducible example. Read this blog for more about reprex’s. 18.0.4 Summary In this lesson we discussed documentation, how to debug code and how to ask for help. All of these items can be tools in your tool belt for when your R programming adventures are not going as you hope. Remember that learning to program can be frustrating but with time and asking for help, you will get there! 18.0.5 Additional Community Resources The R community is a welcoming and helpful community. Many people go years as R users before they learn what a rich community there is out there. We’re hoping to change that by introducing new users to a number of resources that will help them feel more welcome to the large community of individuals using R! Below is a short list of ways to connect with the R community: R for Data Science Learning Community - a welcoming and helpful community for those new to R and data science Twitter - Using or searching the hashtag #rstats can be incredibly helpful and can connect you to others who use R ROpenSci - an online community of developers developing tools for open science R Project Help in R Tidyverse Help Documentation - more details on creating a reprex "],["managing-files.html", "Chapter 19 Managing Files 19.1 Managing Files in R 19.2 Managing Files in the Terminal", " Chapter 19 Managing Files 19.1 Managing Files in R In this lesson, we will continue our tour of ways in which you will be managing files as a data scientist. In the previous lesson, you learned about the Terminal and its command-based interface to the file system on RStudio Cloud. In this lesson, you will learn in small part about the R programming language and in large part about specific tools within R to manage files. You will learn much more about R for its capabilities in data science work in future courses. 19.1.1 Why manage files in R? As a data scientist, you will find it very useful to automate the management and organization of files. For some tasks with larger scale data projects, it will be easier to work with files programmatically in R rather than at the Terminal. Why? Because R is programming language, you will be able to manage files in complex situations where your actions are dependent on lot of other information. For example, if you are working with hundreds of different files, moving them to a location that depends on information in the files is much easier with a programming language such as R rather than at the Terminal. Also, in order to completely document your work in going from raw files to finished products, a good approach is to combine your data work in R (to be covered in later chapters) with your file organization work in R. 19.1.2 Navigating to R in RStudio Cloud When you open a project in RStudio Cloud, you will see a Console pane that is immediately next to the Terminal pane. Usually this pane is automatically open when you open a project. The &gt; that you see in the R console is called the R prompt. It is analogous to the Terminal prompt in that it is where you can type and enter R commands. Locating the Console in RStudio Cloud 19.1.3 Important functions 19.1.3.1 What is my working directory? Recall from the previous lesson that a working directory is the folder that you are currently in within a file system. We discussed that the working directory within the Terminal may be different from the working directory within R, and both of these may be different from the Files pane in RStudio. To determine your working directory in R, you can use the getwd() function, which stands for “get working directory.” When you type getwd() at the R prompt and hit enter, you will see the absolute path to your working directory displayed in quotes on a new line. As with working in the Terminal, knowing the working directory in R is important because when you run code that refers to other files, R will search for those files relative to this directory. getwd() is analogous to pwd in the Terminal. getwd() 19.1.3.2 Set the working directory In Terminal you learned about the cd command to change the current (working) directory. In R, use the setwd() function, which stands for “set working directory.” Inside the parentheses, type an absolute or relative path in quotes. Just as in the Terminal, tab completion can save typing time and help prevent incorrect spelling. When you hit tab after typing part of a path, RStudio provide a drop down menu of files and folders that fit what you have typed. You can select between them with the arrow keys or your mouse and hit enter to autocomplete. Note also that the absolute path to the current working directory is displayed in the status bar beneath the Console tab. The working directory is not automatically updated or reflected in the Files pane however. Additionally, the Console and the Terminal do not talk to one another. When you set your working directory in one, it does not change the working directory in the other. This is important to keep in mind when you’re working. In fact, the working directory in the Terminal, Console, and Files tab can all be different. setwd() 19.1.3.3 What is in this folder? In Terminal you learned about the ls command for listing the contents of a directory. In R, use the list.files() function. Without anything in the parentheses, R will list the files in the current working directory. Otherwise, you can include a relative or absolute path in quotes. In the example below we see that using no path gives the same result as specifying the current directory with a period. When we list the files in the raw_code directory, four files are listed. The numbers in the square brackets just help us count in the displayed output. An option when using list.files() is to type full.names = TRUE after the initial path and a comma. Doing this displays the full relative paths to those files. Note that if the path ends in a forward slash, there will be two forward slashes before the filename in the output. This is not a problem, but if you prefer to not have two forward slashes, omit the trailing forward slash when you type the path. list.files() 19.1.3.4 Create a file In Terminal you learned about the touch command to create a blank file. In R, use the file.create() function. In parentheses, provide a path to the file to create. If this is successful, R will display TRUE and FALSE otherwise. You can verify that the file has been created by using list.files(). As in Terminal, you can quickly cycle through recent commands using the up and down arrow keys. file.create() 19.1.3.5 Moving and renaming a file In Terminal you learned about the mv command for moving and renaming files. In R, use the file.rename() function. In the parentheses, you provide two paths separated by a comma. The first path is the path to the origin file. The second path is the path to the destination file. In the first example, we move the data3.txt file and keep the same name by specifying a different sequence of folders at the beginning of the second path and keeping the same file name. In the second example, we move and rename simultaneously by specifying a different sequence of folders and a different file name. If the renaming is successful, R will display TRUE and FALSE otherwise. file.rename() 19.1.3.6 Copy a file In Terminal you learned about the cp command for copying and renaming files. In R, use the file.copy() function. This function works similarly to file.rename() in that you have to supply the path to the original file first and the path to the destination second. If the copy is successful, R will display TRUE and FALSE otherwise. The first example copies the data3_renamed.txt file to the data3.txt file. In the second example, we try to do this again but fail because data3.txt already exists, and R will not overwrite a file by default. In example 3, we specify overwrite = TRUE after the two paths and a comma to explicitly overwrite the data3.txt file. file.copy() 19.1.3.7 Remove a file In Terminal you learned about the rm command for deleting files. In R, use the file.remove() function. In the parentheses, provide a path to the file you want to delete in quotes. If the deletion is successful, R will display TRUE and FALSE otherwise. file.remove() 19.1.3.8 Does a file exist? In Terminal you used the ls command to list the contents of a directory for verifying what files and folders are present. If you want to check whether a particular file exists, you can use the file.exists() function in R. In the parentheses, provide a path to the file of interest. In the first example, we show the usage for a single file. For data1.txt, R displays TRUE because this file exists. In the second example, we show the usage if you want to check the existence of multiple files. The multiple files are specified in what is called a character vector. The two paths are separated by a comma in the c() function which is the concatenation function. In this case, R displays TRUE then FALSE because data2.txt exists but data4.txt does not. You will learn much more about character vectors and the c() function in later chapters devoted to R. Note that the other functions we covered in this lesson (except getwd() and setwd()) can also be used on multiple files by supplying character vectors of paths instead of single paths. You will have a chance to try these out when you learn more about R and begin working on projects. file.exists() In this lesson, you learned about R functions that are analogous to Terminal commands for managing files. As you work on projects, you’ll gain an appreciation for the benefits of managing files using a programming language. 19.2 Managing Files in the Terminal As was discussed in the first lesson of this course, one of the most important aspects of being a productive data scientist is staying organized. And a big part of staying organized is in managing your files: knowing where they are located and manipulating them with ease. In these next few lessons, we will be getting you fluent in orienting yourself in file systems in two areas: the Terminal and R/RStudio. In this lesson, we will focus on working with the Terminal. 19.2.1 What is the Terminal? The Terminal is an interface to the operating system of a computer. That is, it provides a way for you to type commands in order to perform actions on a computer. For example, there are commands to create new files and folders and to open files and folders. On your computer, you are familiar with using your mouse to perform such actions. When you are working with data on RStudio Cloud, however, you will not be able to use your mouse for everything that you’ll want to do. It will be important for you to become comfortable with using the Terminal as a place for purely text-based commands. In RStudio Cloud, the Terminal is located in the tab next to the Console. Locating the Terminal in RStudio Cloud When you click on this tab, you will see a pane that looks as follows: The Terminal prompt You will always see a string of text at the beginning of the line. This is called the Terminal prompt. To the right of the dollar sign, you will be entering your commands. The Terminal is going to be extremely important for efficient management of your files. You will also use it extensively when we discuss version control systems and GitHub because it is the primary interface for working with those tools. 19.2.2 Why manage files in the Terminal? As we’ve discussed, the Terminal is a command-based interface to a computer operating system. Let’s say that you wanted to make a copy of a folder. Whether on RStudio Cloud or on Google Drive, you would need to go through a series of clicks and typing to find that folder, copy it, and rename it. Within the Terminal, this can all be achieved with a single command. This also allows you to have this command written down so you can more easily repeat it later. 19.2.3 File system example and vocabulary When you are using your mouse to point and click to a file on your computer you are starting from a particular location and may jump within a series of folders to get to the destination file you are looking for. The series of folders it takes to get to the file is called a file path. A file path is a string that specifies a sequence of folders to traverse in order to end up at a final destination. File paths can be relative or absolute. To make an analogy, if someone asked you directions to a particular building, the directions you give would be tailored based on where this person located. In other words your directions would be relative to their location. Relative paths In the same way, your computer can be given absolute directions to a file - basically the directions with absolute directions or they can be relative to where you are calling the command in the computer. So in our above analogy, if you are trying to direct someone to somewhere on Johns Hopkins’ campus with a file path: An absolute file path would be: /Earth/North America/United States/Maryland/Baltimore/Johns Hopkins University/Street Name/Building number Whereas if the person was already in Baltimore, a relative file path would be: Johns Hopkins University/Street Name/Building number To know your location within a file system is to know exactly what folder you are in right now. The folder that you are in right now is called the working directory. In the above analogy a person being located in Baltimore would be their working directory. In a path, folder names are separated by forward slashes / Note that a relative directory may be different between different apps: RStudio versus Terminal versus something else. The end of a path string may be a file name if you are creating a path to a file. If you are creating a path to a folder, the path string will end with the destination folder. Returning to computer files. In your Terminal you can see your working directory at the top of the Terminal window or at the beginning of the terminal prompt. Knowing this, this can tell you how you need to change the command you are entering. Let’s say you want to list, using the ls command, a file called file.txt. Relative path Alternatively, you can specify an absolute path. An absolute path starts at the root directory of a file system. The root directory does not have a name like other folders do. It is specified with a single forward slash / and is special in that it cannot be contained within other folders. In the current file system, the root directory contains a folder called cloud, which contains a folder called project which contains the folder data_analysis_project, which has the file.txt file we are looking for. 19.2.4 Important Terminal Commands Now that you have some vocabulary, we can delve into details about creating and using paths with different Terminal commands for managing files. 19.2.4.1 Where am I right now? If you want to know your current working directory with the Terminal, you can look at the top of the Terminal window OR you can use the pwd command by typing pwd at the Terminal prompt and hitting Enter. This stands for “print working directory”, and it prints the absolute path to your current location. In the example below we are in the project folder within the cloud folder. We can determine that this is an absolute path because it starts with a forward slash /. pwd command 19.2.4.2 What is in this folder? If you want to know what files and folders are contained within your current directory, you can use the ls command. This stands for “list”, and it lists the files and folders within the current directory. If you just use the ls command without any options, the contents will be displayed horizontally across lines. If you add the -lh option with a space after the initial ls command, the output is displayed more nicely. The l part displays the results in a longer form with more information than just the file name. The h part displays the file sizes in a human-readable format. The most important pieces of information are in the last three columns, which display the file or folder size, the date the file or folder was last modified, and the name of the file or folder. You can also list the contents of a specific folder by specifying an absolute or relative path after the main ls command. ls command When you use ls without specifying any path, it displays the contents of the current working directory. You can get the same results by specifying a period . for the path, as shown below. The period stands for the current working directory. ls command 19.2.4.3 Change directory If you would like to change your current working directory, you can use the cd command. This stands for “change directory” and moves you to the folder that you specify with a path after cd. When you type these paths, it will be incredibly useful to use the tab completion feature of Terminal. With tab completion, you can partially type part of a folder or file name and hit tab to automatically fill in the remainder of the name. This is super handy because file paths have to be typed exactly right for them to work. Same capitalization, spaces, punctuation, etc. So if you use tab completion to complete a file path, you know it will be typed exactly right! For example, if we are typing out a file path within a command and want to type out the folder for data_analysis_project, if we have cd cloud/project/da typed out, and then we click the tab it will autocomplete the rest of the folder name: data_analysis_project. cd command If you try to change directory to a nonexistent folder, you will get an error message that looks as below: Error with the cd command You can avoid error messages like this more often if you use tab completion! There’s one last important note about file paths. As we’ve mentioned, data science is best done as a team! This means you will often share your code and data with others. Absolute file paths are not transferrable to other computers! Do not write absolute file paths in your code because they will not work anywhere else! Try to stick to relative file paths (and we will practice this idea more later so you know how to do this). 19.2.4.4 Cycling through previous commands Often when working in the Terminal, you will want to run the same or similar commands that you have run earlier. For example, you may want to change directories, list files, and then repeat this process as you change to other directories. You can avoid typing commands over and over by using the up and down arrow keys at the Terminal prompt. Hitting the up arrow key once pulls up the last command that you entered. Hitting the up key twice pulls up the command you entered two times ago. Now hitting down will pull up your last command again. You can keep hitting up and down to cycle through your previous commands. Try this out when you are typing at the Terminal! 19.2.4.5 The wildcard operator In several different Terminal commands, it can be useful to specify only part of a file or folder name. For example, if we go to the raw_code folder and use the ls command, we see that there are 4 files. In larger projects, there may be many more files and we might not want to view all of them. We can specify certain patterns of files to list after the main ls command using part of the file name combined with the * wildcard operator. This operator matches any number of characters. So to list only files that start with “clean”, we can use ls -lh clean*. To only list files that contain “dataset1”, we can use ls -lh *dataset1* to match any characters on either side of the “dataset1” pattern. This will be useful when you work more with GitHub, starting in the next course. Wildcard operator 19.2.4.6 Copying files and folders Copying files and folders will come up often during your work. You may want to copy a messy code file to a new file to begin cleaning it up. It may be useful to do this for an entire folder. To copy in Terminal, we can use the cp command. To copy one file, the syntax is cp path_to_original_file path_to_new_file The path to the original file and the path to the new file can be absolute or relative paths. In the path to the new file, you can specify a different file name to rename it. In the example below, the first command cp clean_dataset1.R ../final_code/ copies the clean_dataset1.R file to the final_code folder and keeps the same name. The second command cp clean_dataset1.R ../final_code/clean_dataset1_renamed.R specifies the relative path and a new file name. When we list the contents of the final_code folder, we see both the originally named file and the renamed file. Copying a single file We can combine copying with the wildcard operator to copy multiple files. This does not rename the files. We can also copy multiple files into a directory by naming the files explicitly. That is cp analyze* ../final_code/ achieves the same as cp analyze_dataset1.R analyze_dataset2.R ../final_code/. Copying multiple files To copy a folder, we must specify the recursive option -r in order to copy all of the files and folders inside the one being copied. In the example below, using cp final_code/ publication_code doesn’t work, but cp -r final_code/ publication_code does work. Note that a final forward slash at the end of final_code and publication_code is optional. In these examples, you will see the trailing forward slash when tab completion was used to speed up typing. Copying a folder 19.2.4.7 Moving files and folders In the process of organizing your files, you will undoubtedly need to move files and folders to different locations. To move files and folders in Terminal, we can use the mv command. This command also renames files and folders. Similar to the copy command, the syntax for moving one file is mv path_to_original_file path_to_new_file Just as with copy, the path to the original file and the path to the new file can be absolute or relative paths. In the path to the new file, you can specify a different file name to rename it. In the example below, the first command mv ../final_code/clean_dataset1_renamed.R . moves the clean_dataset1_renamed.R file to the current working directory (indicated by the period at the end of the command) and keeps the same name. The next two commands mv clean_dataset1.R tidy_dataset1.R and mv clean_dataset2.R tidy_dataset2.R rename the files in the working directory to start with tidy instead of clean. When we list the contents of the working directory, we see both the moved file and the two renamed files. Moving a single file We can combine moving with the wildcard operator to move multiple files. This does not rename the files. We can also move multiple files into a directory by naming the files explicitly. This is achieved with mv tidy_dataset1.R tidy_dataset2.R ../raw_code/. Moving multiple files To move a folder into another folder, we use the same syntax for moving a single file: mv path_to_folder_being_moved destination_path In the example below, we achieve this with mv raw_code/ publication_code/. We can also rename a folder by specifying a new name for the destination path. In the example, we achieve this with mv publication_code/ pub_code. Moving and renaming a folder 19.2.4.8 Deleting files and folders To delete files and folders in the Terminal, we can use the rm command, which stands for remove. To remove a single file, the syntax is as follows: rm path_to_file_to_delete To remove multiple files, you can specify paths to multiple files separated by spaces or use the wildcard operator. Examples are shown below. Deleting files To delete a folder, we must specify the recursive option -r in order to recursively delete all of the files and folders inside the one being deleted. This is exactly as we had to do with copying a folders. In the example below, using rm final_code/ doesn’t work, but rm -r final_code/ does work. Deleting a folder Be very careful when deleting files and folders because this action is irreversible! In particular, rm * will delete all files in the current working directory. If you are using the wildcard operator, test your wildcard pattern with an ls command before deleting anything. 19.2.4.9 Creating files and folders To create an empty file in the Terminal, we can use the touch command with touch path_to_file. The file will be empty, but it serves as a placeholder in case you decide to later open and edit the file. Creating a new file To create a new directory in the Terminal, we can use the mkdir command, which stands for “make directory.” After mkdir, type the path to the folder that we want to create. Creating a new directory 19.2.5 Additional Resources The ls command The cp command The mv command The rm command "],["r-packages.html", "Chapter 20 R Packages", " Chapter 20 R Packages Now that we’ve looked at R and RStudio and have a basic understanding of how they work together, we can get at one thing that makes R so special: packages. 20.0.1 What is an R package? So far, anything we’ve played around with in R uses the “base” R system. Base R, or everything included in R when you download it, has rather basic functionality for statistics and plotting but it can sometimes be limiting. To expand upon R’s basic functionality, people have developed packages. A package is a collection of functions, data, and code conveniently provided in a nice, complete format for you. At the time of writing, there are just over 17,600 packages available to download - each with their own specialized functions and code, all developed for a specific but different purpose. For a really in depth look at R Packages (what they are, how to develop them), check out Hadley Wickham’s book from O’Reilly, “R Packages” Side note: A package is not to be confused with a library (these two terms are often conflated in colloquial speech about R). A library is the place where the package is located on your computer. To think of an analogy, a library is, well, a library… and a package is a book within the library. The library is where the books/packages are located. Packages are what make R so unique. Not only does base R have some great functionality but these packages greatly expand its functionality. And perhaps most special of all, each package is developed and published by the R community at large and deposited in repositories. 20.0.2 What are repositories? For R packages, a repository is central location where many developed packages are located and available for download. The repositories for R packages are also places where information and code are stored. (In fact, as you’ll see below, GitHub repositories are one of the main repositories for R packages!) There are three big repositories for R packages: 1. CRAN (Comprehensive R Archive Network): R’s main repository (&gt;12,100 packages available!) 2. BioConductor: A repository mainly for bioinformatic-focused packages 3. GitHub: A very popular, open source repository (not R specific!) – we will talk about and use GitHub more later! Take a second to explore the links above and check out the various packages that are out there! 20.0.3 How do you know what package is right for you? So, you know where to find packages… but there are so many of them, how can you find a package that will do what you are trying to do in R? There are a few different avenues for exploring packages. First, CRAN groups all of its packages by their functionality/topic into 35 “themes.” It calls this its “Task view.” This at least allows you to narrow the packages you can look through to a topic relevant to your interests. Second, there is a great website, R Documentation, which is a search engine for packages and functions from CRAN, BioConductor, and GitHub (ie: the big three repositories). If you have a task in mind, this is a great way to search for specific packages to help you accomplish that task! It also has a “task” view like CRAN, that allows you to browse themes. More often, if you have a specific task in mind, Googling that task followed by “R package” is a great place to start! From there, looking at tutorials, vignettes, and forums for people already doing what you want to do is a great way to find relevant packages. 20.0.4 How do you install packages? Great! You’ve found a package you want… How do you install it? 20.0.4.1 Installing from CRAN If you are installing from the CRAN repository, use the install.packages() function, with the name of the package you want to install in quotes between the parentheses (note: you can use either single or double quotes). For example, if you want to install the package ggplot2, you would use: install.packages(\"ggplot2\") Try doing so in your R console! This command downloads the ggplot2 package from CRAN and installs it onto your computer. If you want to install multiple packages at once, you can do so by using a character vector (we’ll get back to exactly what that means in a later lesson in this course!), like: install.packages(c(\"ggplot2\", \"devtools\", \"lme4\")) If you want to use RStudio’s graphical interface (meaning you would point-and-click more than you would type into the console) to install packages, go to the Tools menu, and the first option should be “Install packages…” If installing from CRAN, select it as the repository and type the desired packages in the appropriate box. You can install through the console interface using the above commands or using the Install Packages menu option Select the appropriate repository and type in your desired packages 20.0.4.2 Installing from Bioconductor The BioConductor repository uses their own method to install packages. While you get started and learn to code in R, you will likely not be installing packages from Bioconductor; however, if you later on work in biology-focused fields, you’ll want to know about Bioconductor. So, we’ll cover this now so you know about Bioconductor, even if you don’t install most of your packages from this repository right now. First, to get the basic functions required to install through BioConductor, use: install.packages(\"BiocManager\") This makes the main install function of BioConductor, BiocManager::install(), available to you. Following this, you call the package you want to install in quotes, between the parentheses of the BiocManager::install command, like so: BiocManager::install(\"GenomicFeatures\") 20.0.4.3 Installing from GitHub This is a more specific case that you probably won’t run into too often as you just get started working in R. As packages are developed, the code is frequently put into a GitHub repository. At this point, as long as the repository is public, anyone can install the package in RStudio. However, most developers, once a package is complete, will submit it to CRAN (the first repository discussed above), making it stably available to all R users. Thus, most of the packages discussed throughout these courses will be available from CRAN. However, newer packages that are still under active development will likely have to be installed directly from GitHub. In the event you want to do this, you first must find the package you want on GitHub and take note of both the package name AND the author of the package. Check out this guide for installing from GitHub, but the general workflow is: install.packages(\"devtools\") - only run this if you don’t already have devtools installed. If you’ve been following along with this lesson, you may have installed it when we were practicing installations using the R console. library(devtools) - more on what this command is doing immediately below this. install_github(\"author/package\") replacing “author” and “package” with their GitHub username and the name of the package. 20.0.5 Loading packages Installing a package does not make its functions immediately available to you. You need to tell R in what library to find those functions. There’s two ways to do this: Use the library() function - Let’s say I want to use the write_csv() function from the readr function. I need to first run library(readr) before I can use the write_csv() function that is from that package. library(readr) write_csv(iris, &quot;iris.csv&quot;) Use the :: notation - The only downside from the strategy above is that I might only need to use one function from a package and loading in the whole package would be unnecessary. In this case I can also just use :: to tell R what package a function is from: readr::write_csv() (If I use this, I do not have to run library(readr)). So in this case the code above would be this instead: readr::write_csv(iris, &quot;iris.csv&quot;) Another note about ::, sometimes multiple packages might have functions called the same thing. For example, there is a function called filter in a package called dplyr as well as in the base package stats. If you load a library, one or the other function will be used preferentially. This may not be clear to you or others which function is being used. If you use :: there is no confusion. This being said, writing :: a billion times in your code may also not be ideal if you are using one package heavily. In this case you may want to use the library() function. In summary, you can use either strategy depending on what your situation calls for! But you must use at least one of the strategies or else R will tell you an error message like this: Error in function_name() : could not find function &quot;function_name&quot; This is R saying “I don’t know where to find this function!”. This means you’ve done neither the :: or the library() for the function you are using and you need to do that! 20.0.6 Dependencies There is an order to installing packages - some packages require other packages to be loaded first. These other packages are known as dependencies. When you install a new package, that new package’s manual/help pages will automatically determine what packages this new package is dependent upon and will install those packages as well. If you want to load a package using the RStudio interface, in the lower right quadrant there is a tab called “Packages” that lists out all of the packages and a brief description, as well as the version number, of all of the packages you have installed. To load a package just click on the checkbox beside the package name. Find the package you want to load from the list of installed packages and check the box to load it 20.0.7 Updating, removing, unloading packages Once you’ve got a package, there are a few things you might need to know how to do: 20.0.7.1 Checking what packages you have installed If you aren’t sure if you’ve already installed a package, or want to check what packages are installed, you can use either of: installed.packages() or library() with nothing between the parentheses to check! In RStudio, that package tab introduced earlier is another way to look at all of the packages you have installed. 20.0.7.2 Updating packages Like your projects in GitHub, packages are version controlled. As updates are made to packages, the version on the package will change and be updated. To see if packages you’ve installed need an update, use the function old.packages(). This will identify all packages that have been updated since you installed them/last updated them. To update all packages, use update.packages(). If you only want to update a specific package, just use once again install.packages(\"packagename\") Within the RStudio interface, still in that Packages tab, you can click “Update,” which will list all of the packages that are not up to date. It gives you the option to update all of your packages, or allows you to select specific packages. If all of your packages are up to date you will get the message “All packages are up to date.” Using the Update menu, you can select all or some of the packages you have installed that you can update You will want to periodically check in on your packages and check if you’ve fallen out of date - be careful though! Sometimes an update can change the functionality of certain functions, so if you re-run some old code, the command may be changed or perhaps even outright gone and you will need to update your code too! 20.0.7.3 Unloading packages Sometimes you want to unload a package in the middle of a script - the package you have loaded may not play nicely with another package you want to use. To unload a given package you can use the detach() function. For example, detach(\"package:ggplot2\", unload=TRUE) would unload the ggplot2 package (that we loaded earlier). Within the RStudio interface, in the Packages tab, you can simply unload a package by unchecking the box beside the package name. 20.0.7.4 Uninstalling packages If you no longer want to have a package installed, you can simply uninstall it using the function remove.packages(). For example, remove.packages(\"ggplot2\") (Try that, but then actually re-install the ggplot2package - it’s a super useful plotting package!) Within RStudio, in the Packages tab, clicking on the “X” at the end of a package’s row will uninstall that package. 20.0.7.5 Sidenote: How do you know what version of R you have? Sometimes, when you are looking at a package that you might want to install, you will see that it requires a certain version of R to run. To know if you can use that package, you need to know what version of R you are running! One way to know your R version is to check when you first open R/RStudio - the first thing it outputs in the console tells you what version of R is currently running. If you didn’t pay attention at the beginning, you can type version into the console and it will output information on the R version you are running. Another helpful command is sessionInfo() - it will tell you what version of R you are running along with a listing of all of the packages you have loaded. The output of this command is a great detail to include when posting a question to forums - it tells potential helpers a lot of information about your OS, R, and the packages (plus their version numbers!) that you are using. In the output from sessionInfo(), you’ll note that the end of each package’s name has an underscore followed by a series of numbers. Those numbers indicate the packages version. For example, the version of ggplot2 installed in this session is version 2.2.1 (read version two point two point 1). This number will change (increase) every time developers make changes to this package. sessionInfo() shows you packages and versions 20.0.8 Using the commands in a function In all of this information about packages, we haven’t actually discussed how to use a package’s functions! While functions are discussed in greater detail in a later lesson in this course, for now, know that to use the contents of a package, you’ll use functions. First, you need to know what functions are included within a package. To do this, you can look at the man/help pages included in all (well-made) packages. In the console, you can use the help() function to access a package’s help files. Try help(package = \"ggplot2\") and you will see all of the many functions that ggplot2 provides. Within the RStudio interface, you can access the help files through the Packages tab (again) - clicking on any package name should open up the associated help files in the “Help” tab, found in that same quadrant, beside the Packages tab. Clicking on any one of these help pages will take you to that functions help page, that tells you what that function is for and how to use it. Once you know what function within a package you want to use, you simply call it in the console like any other function we’ve been using throughout this lesson. Once a package has been loaded, it is as if it were a part of the base R functionality. If you still have questions about what functions within a package are right for you or how to use them, many packages include “vignettes.” These are extended help files, that include an overview of the package and its functions, but often they go the extra mile and include detailed examples of how to use the functions in plain words that you can follow along with to see how to use the package. To see the vignettes included in a package, you can use the browseVignettes() function. For example, let’s look at the vignettes included in ggplot2:browseVignettes(\"ggplot2\") . You should see that there are two included vignettes: “Extending ggplot2” and “Aesthetic specifications.” Exploring the Aesthetic specifications vignette is a great example of how vignettes can be helpful, clear instructions on how to use the included functions. 20.0.9 Summary In this lesson, we’ve explored R packages in depth. We examined what a package is (and how it differs from a library), what repositories are, and how to find a package relevant to your interests. We investigated all aspects of how packages work: how to install them (from the various repositories), how to load them, how to check which packages are installed, and how to update, uninstall, and unload packages. We took a small detour and looked at how to check what version of R you have, which is often an important detail to know when installing packages. And finally, we spent some time learning how to explore help files and vignettes, which often give you a good idea of how to use a package and all of its functions. If you still want to learn more about R packages, here is a great resource: Introduction to R Packages from Ken Rice and Timothy Thornton. 20.0.10 Additional Resources “R Packages”, by Hadley Wickham CRAN (Comprehensive R Archive Network): BioConductor: GitHub: Introduction to R Packages, from Ken Rice and Timothy Thornton "],["r-markdown.html", "Chapter 21 R Markdown 21.1 Introduction to Markdown", " Chapter 21 R Markdown We’ve spent a lot of time getting R and RStudio working, learning about projects and functions - you are practically an expert at this! There is one last major functionality of R/RStudio that we would be remiss to not include in your introduction to R - R Markdown! 21.0.1 What is R Markdown? R Markdown is a type of document that allows you to generate fully reproducible reports. In these documents, text , code, and the results of the code are all combined in one place. In fact, these lessons are written using what you’ve already learned about Markdown and all the R code you’ve recently mastered! To refresh your memory, this is how we use plain text in Markdown formatting: Markdown review Throughout this lesson we’ll remind you of what you learned in the previous lesson on Markdown and discuss all the new things you’ll be able to do with R Markdown documents! R Markdown documents generally take one of two file extensions: .Rmd, .rmd. If a file ends with either of these two file extensions (ie Project_Analysis.Rmd), then you know it’s an R Markdown document. While this type of file is a plain text file, it can be rendered (“Knit”) into HTML pages, PDFs, Word documents, or slides! We’ll get into exactly what that means in just a second! .Rmd to PDF 21.0.2 Why use R Markdown? One of the main benefits is the reproducibility of using R Markdown. Since you can easily combine text and code chunks in one document, what this means for a data science project is that you can easily integrate an introduction about what your project question is and where your data came from and the code that you are running, the results of that code, some pretty plots and figures, and your conclusions all in one document. Sharing what you did, why you did it and how it turned out becomes so simple - and that person you share it with can re-run your code and get the exact same answers you got. That’s what we mean about reproducibility. In addition to being reproducible, there will be times that you’re working on a project that takes many weeks or months to complete. In these cases, you want to be able to see what you did a long time ago (and perhaps be reminded exactly why you were doing this). By using an R Markdown document, you’ll be able to see exactly what you did previously, what code you used AND the results of that code! Another major benefit to R Markdown is that since it is plain text, it works very well with version control systems, such as git and GitHub. It is easy to track what character changes occur between commits; unlike other formats that aren’t plain text. For example, in one version of this lesson, I may have forgotten to bold this word. When I catch my mistake, I can make the plain text changes to signal I would like that word bolded, and in the commit, you can see the exact character changes that occurred to now make the word bold. Check out this video that the RStudio developers released about R Markdown and what it is! 21.0.3 Getting started with R Markdown The best way to follow along for the rest of this lesson is to open up RStudio Cloud, and follow along on your own step-by-step. In the process, you’ll get to generate your first R Markdown document! Generating and working with RMarkdown documents is incredibly easy when working within RStudio (or RStudio Cloud). To get started in RStudio Cloud, go to File &gt; New File &gt; R Markdown.. . R Markdown… If a window pops up specifying that you need to install and update a few packages before using R Markdown, click “Yes” to install those updates. At this point, you will be presented with the following window: R Markdown You’ll want to add a Title to this document and put your name in the Author box. Title and Author information filled out When you are done entering this information, click OK, and a new .Rmd document will open with a little explanation on R Markdown files. R Markdown document There are three main sections of an R Markdown document. The first is the YAML at the top, bounded by the three dashes. This is where you can specify details like the title, your name, the date, and what kind of document you want to output. If you filled in the blanks in the window earlier, these will be filled out for you. The spacing of this section matters, so if you edit anything here and then get an error when you try to Knit your document, it may be worth returning to this section to make sure spacing is as it should be. Also on this page, you can see text sections. In this section, text should be written in Markdown. This means that the “## R Markdown” will appear as an H2 header when the document is rendered. and Knit will be bold, as discussed in a previous lesson. And finally, you will see code chunks. These are bounded by the triple backticks. These are pieces of R code, and are referred to as “code chunks”. These code in these chunks can run right from within your document - and the output of this code will be included in the document when you Knit it. The easiest way to see how each of these sections behave is to produce the HTML! To do so, you’ll learn how to knit the document below. 21.0.4 File paths in R Markdowns Remember how earlier we talked about relative vs absolute file paths. R Markdowns automatically use relative file paths starting from wherever the .Rmd file is located. In other words, things like setwd() will not work in an .Rmd file. This is for good reason! If you share this Rmd with someone else, you want the file paths in your Rmd to be relative. (Remember how we said absolute file paths do not work on other people’s computers?) Because of this, you can share an Rmd and its associated files and other should be able to run the Rmd and completely reproduce your results – reproducibility is very important, we’ll talk about more reproducibility techniques as we go through this course. 21.0.5 “Knitting” documents When you want to preview an R Markdown document and when you are finished with an R Markdown document, you’ll want to “knit” the plain text and code from your .Rmd into your final document. To do so with the R Markdown that opened with your R Markdown file, click on the “Knit” button along the top of the source panel. Knit When you do so, it will prompt you to save the document. For now, we’ll type “test_document” into the box. (However, when you’re generating these documents for projects, you’ll want to be sure that this document is saved in the appropriate directory, which is likely the raw_code directory. Click “Save.” Save .Rmd Upon saving the document, you should see a document like this appear in a new window: Knit HTML So here you can see that the content of the header was rendered into a title, followed by your name and the date. The text chunks produced a section header called “R Markdown” which is followed by two paragraphs of text. Following this, you can see the R code summary(cars), importantly, followed by the output of running that code. And further down you will see code that ran to produce a plot, and then that plot. This is one of the huge benefits of R Markdown - rendering the results to code inline. Go back to the R Markdown file that produced this HTML and see if you can see how you signify you want text bolded. (Hint: Look at the word “Knit” and see what it is surrounded by). Additionally, feel free to change the text in this document or add additional code. Then, click on “Knit” again and see how the changes alter the HTML that is produced. Upon Knitting to HTML, an additional file will now be saved in the same directory where you saved your .Rmd file. However, as expected, this file will have the extension .html. If you make changes and re-knit your file, this HTML file will be re-generated and all changes will be saved in this file. Saved HTML file One final note on knitting. In this example, we have Knit to HTML (a format that can be easily viewed in any web browser), but you can also knit to a PDF or Word document (among other options). To Knit to a different output format, click on the arrow to right of the Knit icon to expose a drop-down menu. Select the output document you’d like from this list. The new file type will be generated and saved in the same directory where the .Rmd file is, but with the appropriate extension (i.e. .pdf if you selected “Knit to PDF”). Feel free to play around with how these different file output options look when you Knit! Other file output options when Knitting 21.1 Introduction to Markdown Markdown is a basic markup language designed to be displayed on the web and is part of writing an R Markdown document. With a few basic commands, you can create polished documents that can be used to: Communicate your results to others Provide daily/weekly reports of your employer 21.1.1 How to use Markdown If you know how to type you know how to use Markdown! Writing with Markdown is the exact same task as writing in a text editor like Microsoft Word. The only difference is that all the fancy buttons and options are removed and instead replaced with a series of commands that you can type to format your text. 21.1.2 Markdown Example Here’s a small example of what Markdown can do! You can see everything in this document is written as plain text, with just letters and basic symbols. An example markdown file Now you can see how that text appears when it is rendered in Markdown. Even though it was created with plain text, it appears with italics, bolding, different sized text, and even an image! A rendered markdown file 21.1.3 Main commands Three major formatting basics of Markdown are headers, bold and italicized text, and lists. Headers help you separate sections of your document, bold or italicized format allows you to emphasize important points in your document, and lists help you orderly arrange your ideas. 21.1.3.1 Headers Headers are straightforward. To create a header, you simply add a # sign right before the text you want to make a header. Keep in mind the # must be on the beginning of a new line (no text on the line before it). The more #’s you add before the text, the smaller the header will be. For example here is a list of headers you can use ordered from largest to smallest. # Largest ## Slightly less large ### Even less large #### Even smaller ##### Smaller still ###### Smallest shown here (but you can go smaller!) The reason why this did not register as a header is because it is formatted it as a comment. By inserting three tick marks before and after the block of text you wish to comment, it will not execute any formatting. Without the tick marks here is what we get! A rendered markdown file The headers look as we wanted them too. Thus headers should not be included in code chunks. If pound signs (#) are within a code chunk, Markdown will consider them to be comments, rather than headers. 21.1.3.2 Bolded and italicized text Creating bolded and italicized text is also very straightforward. Use a double asterisk (**) before and after the text you want to be bold and a single asterisk (*) before and after text you want to italicize. So in this sentence **what you want to bold** is shown **bolded** and *what you want to italicize* is shown in *italics*. See the results here: So in this sentence **what you want to bold** will be shown what you want to bold and *what you want to italicize* will be shown what you want to italicize. 21.1.3.3 Lists Lists are a useful way to organize your ideas or tasks. In Markdown, you can make your lists numbered or non-numbered. To make a numbered list, just put the number and a period in front of the item. As with headers, you do need to make sure your first number is on a new line (no text on the line before it!) 1. First item 2. Second item 3. Third item This becomes: First item Second item Third item For non-numbered lists, you can use your choice of asterisks (*), pluses (+), or minuses (-) to indicate list items: * First item * Second item OR + First item + Second item OR - First item - Second item All become: First item Second item You can create sub-items for your list by indenting (using multiple spaces or Tab) before the number or the asterisk/plus/minus. Make the number or symbol of the sub-item line up with the text of the item above it! 1. First item 2. Second item 1. Sub-item 2. Sub-item 3. Third item * Sub-item * Sub-item This becomes: First item Second item Sub-item Sub-item Third item Sub-item Sub-item If you want to cross off items on your list (as you do them, perhaps) you can create a strike-through using double tildes (~~). 1. ~~First item~~ 2. Second item 1. ~~Sub-item~~ 2. Sub-item This becomes: First item Second item Sub-item Sub-item 21.1.4 More complicated editing There are many more text modifiers you can use for Markdown. A short instructional guide can be found here. You will see some more of these commands later in this module, but you might want to bookmark the above link now for future reference! 21.1.5 Links Links to content on the internet can be included in a Markdown document as well. The format for a link is to put what you want your link to say in square brackets followed immediately by the web address where the link should go inside of parentheses, like this: [What you want your link to say](Web address where the link should go) For example if you want the link to go to www.google.com when clicked, then you’d replace the text in the parentheses and write: [What you want your link to say](www.google.com) This is what it would look like in your rendered Markdown file: What you want your link to say If you want to change the text of the link itself, you just change what’s in the square brackets. For example, you might want the sentence: If you don’t know the answer, you should look it up on Google, where the word Google would be a link to www.google.com. You could do that by putting Google into square brackets, with the web address www.google.com following immediately in parentheses, like this: If you don&#39;t know the answer, you should look it up on [Google](www.google.com). This would be shown as: If you don’t know the answer, you should look it up on Google. You can insert images in a Markdown document as well. This is done in a similar manner to links. For images you can add ![Image Caption](/path_to_image/image_name.png). The link can be the web location of an image or the local address of an image. For instance, if you type ![Yosemite National Park](https://commons.wikimedia.org/wiki/Yosemite_National_Park#/media/File:Half_Dome_from_Glacier_Point,_Yosemite_NP_-_Diliff.jpg) will show this. Yosemite National Park 21.1.6 Additional Resources R Markdown Documentation, from RStudio R Markdown video, from RStudio Basic R Markdown cheatsheet “R Markdown cheatsheet” "],["dataset-file-types.html", "Chapter 22 Dataset File Types 22.1 Special R data files", " Chapter 22 Dataset File Types In this lesson, we’ll discuss a few of the most common file types used to store data for use with R. There are two general categories will discuss here: Tabular data files are data files like those stored in spreadsheets. Information from different variables are stored in columns and each observation is stored in a different row. The values for each observation is stored in its respective cell. Special R data files are made for use with R. They keep data formatted as you have it in R so after you import the data you won’t have to do as much manipulation. They aren’t generally able to be used outside of R though. 22.0.1 Tabular data files 22.0.2 CSV files Comma-separated values (CSV) files allow us to store tabular data in a simple format. CSVs are plain-text files, which means that all the important information in the file is represented by text (where text is numbers, letters, and symbols you can type on your keyboard). For example, consider a dataset that includes information about the heights and blood types of three individuals. You could make a table that has three columns (names, heights, and blood types) and three rows (one for each person) in Google Docs or Microsoft Word. However, there is a better way of storing this data in plain text without needing to put them in table format. CSVs are a perfect way to store these data. In the CSV format, the values of each column for each person in the data are separated by commas and each row (each person in our case) is separated by a new line. This means your data would be stored in the following format: sample CSV Notice that CSV files have a .csv extension at the end. You can see this above at the top of the file. One of the advantages of CSV files is their simplicity. Because of this, they are one of the most common file formats used to store tabular data. Additionally, because they are plain text, they are compatible with many different types of software. CSVs can be read by most programs. Specifically, for our purposes, these files can be easily read into R (or Google Sheets, or Excel), where they can be better understood by the human eye. Here you see the same CSV opened in Google Slides, where it’s more easily interpretable by the human eye: CSV opened in Google Slides As with any file type, CSVs do have their limitations. Specifically, CSV files are best used for data that have a consistent number of variables across observations. For example, in our example, there are three variables for each observation: “name”, “height”, and “blood_type”. If, however, you had eye color and weight for the second observation, but not for the other rows, you’d have a different number of variables for the second observation than the other two. This type of data is not best suited for CSVs. However, whenever you have information the same number of variables across all observations, CSVs are a good bet! 22.0.3 Downloading CSV files If you entered the same values used above into Google Slides first and wanted to download this file as a CSV to read into R, you would enter the values in Google slides, and then click on “File” and then “Download” as and choose “Comma-separated values (.csv, current sheet)”. The dataset that you created will be downloaded as a CSV file on your computer. Make sure you know the location of your file (if on a Chromebook, this will be in your “Downloads” folder). Download as CSV file 22.0.4 Reading files into RStudio Cloud Now that you have a CSV file, let’s discuss how to get it into RStudio Cloud! Log in to your RStudio Cloud account. Create a new project. On the RStudio workspace that you see, click on Upload under Files on the bottom right corner of the screen. On the window that pops up click on Choose File. Upload a file on RStudio Cloud Now, find where you saved the file (for instance “Downloads”) and click on OPEN. After this, the window closes automatically and you’ll be back in your workspace on RStudio Cloud. You will see that your CSV file now appears among other files. (A reminder: if you were working on a data science project, this would go in your “raw_data” directory. For this example, however, we’ll keep it in “cloud/project”) Find local file Now, while the file has now been uploaded to your RStudio Cloud project, it’s important to recognize the file is not yet imported to your R environment as an object. We’ll do that now! The best way to accomplish this is using the function read_csv() from the readr package. (Note, if you haven’t installed the readr package, you’ll have to do that first.) Inside the parenthesis of the function, write the name of the file in quotes, including the file extension (.csv). Make sure you type the exact file name. Save the imported data in a data frame called df_csv. Your data will now be imported into R environment. If you use the command head(df_csv) you will see the first several rows of your imported data frame: ## install.packages(&quot;readr&quot;) library(readr) ## read CSV into R df_csv &lt;- read_csv(&quot;sample_data - Sheet1.csv&quot;) ## look at the object head(df_csv) read_csv() Above, you see the simplest way to import a CSV file. However, as with many functions, there are other arguments that you can set to specify how to import your specific CSV file, a few of which are listed below. However, as usual, to see all the arguments for this function, use ?read_csv within R. col_names = FALSE to specify that the first row does NOT contain column names. skip = 2 will skip the first 2 rows. You can set the number to any number you want. Helpful if there is additional information in the first few rows of your data frame that are not actually part of the table. n_max = 100 will only read in the first 100 rows. You can set the number to any number you want. Helpful if you’re not sure how big a file is and just want to see part of it By default, read_csv() converts blank cells to missing data (NA). Note that the read_csv() function is a part of a family of functions that all have similar names. For example, if you need to read in a TSV file, you can use read_tsv() function. If you need to read in an RDS file, you can use the read_rds() function. Note that there is a function read.csv which is available by default in R. It’s not quite as nice to use, but you will likely see this function in others’ code, so we just want to make sure you’re aware of it. 22.0.5 Writing CSV files into RStudio Cloud The readr family of functions also includes writing functions. If you want to write a CSV, TSV, or RDS file you can use the write_csv(), write_tsv() or write_rds() functions! In this example, we can write the iris data to a CSV file by providing two arguments. The first is what data frame we would like to write to a CSV file. In this case, we want to write iris data frame to a file. The second argument is what the filename we should save to this to a file named \"iris_data.csv\". Note that the ## install.packages(&quot;readr&quot;) library(readr) write_csv(iris, &quot;iris_data.csv&quot;) 22.0.6 Excel files While CSV files hold plain text as a series of values separated by commas, an Excel (or .xls or .xlsx) file holds information in a workbook that comprises both values and formatting (colors, conditional formatting, font size, etc.). You can think of Excel files a fancier CSV file. While this may sound appealing, we’ll remind you that CSV files can be read by many different pieces of software, Excel files can only be viewed in specific pieces of software, and thus are generally less flexible. That said, many people save their data in Excel, so it’s important to know how to work with them in RStudio Cloud. Let’s go back to the Google Sheet that we created and instead of downloading the file locally as as CSV, download it as Microsoft Excel (.xlsx) file. Download as Excel file Save the file where you can find it. Similar to the CSV file, first, upload the file into your RStudio Cloud workspace. 22.0.7 Reading Excel files into RStudio Cloud To read this file into R, we’ll have to use a different function than above, as this file is not a CSV file. We’ll use the read_excel() function from the readxl package. Install the package first and then use the function read_excel() from the package read the Excel file into your R Environment. As above, by default, read_excel() converts blank cells to missing data (NA). ## install and load package install.packages(&quot;readxl&quot;) library(readxl) df_excel &lt;- read_excel(&quot;sample_data.xlsx&quot;) head(df_excel) Find local file 22.0.8 Text files Another common form of data is text files that usually come in the form of TXT or TSV file formats. Like CSVs, text files are simple, plain-text files; however, rather than columns being separated by commas, they are separated by tabs (represented by “ in plain-text). Like CSVs, they don’t allow text formatting (i.e. text colors in cells) and are able to be opened on many different software platforms. This makes them good candidates for storing data. 22.0.9 Reading TSV files into RStudio Cloud The process for reading these files into R is similar to what you’ve seen so far. We’ll again use the readr package, but we’ll instead use the read_tsv() function. ## read TSV into R df_tsv &lt;- read_tsv(&quot;sample_data - Sheet1.tsv&quot;) ## look at the object head(df_tsv) 22.0.10 Reading TXT files into RStudio Cloud Sometimes, tab-separated files are saved with the .txt file extension. TXT files can store tabular data, but they can also store simple text. Thus, while TSV is the more appropriate extension for tabular data that are tab-separated, you’ll often run into tabular data that individuals have saved as a TXT file. In these cases, you’ll want to use the more generic read_delim() function from readr. Google Sheets does not allow tab-separated files to be downloaded with the .txt file extension (since .tsv is more appropriate); however, if you were to have a file “sample_data.txt” uploaded into your RStudio Cloud project, you could use the following code to read it into your R Environment, where “ specifies that the file is tab-delimited. ## read TXT into R df_txt &lt;- read_delim(&quot;sample_data.txt&quot;, delim = &quot;\\t&quot;) ## look at the object head(df_txt) This function allows you to specify how the file you’re reading is in delimited. This means, rather than R knowing by default whether or not the data are comma- or tab- separated, you’ll have to specify it within the argument delim in the function. read_delim() is a more generic version of read_csv(). What this means is that you could use read_delim() to read in a CSV file. You would just need to specify that the file was comma-delimited if you were to use that function. 22.0.11 Exporting Data in R to CSV The last topic of this lesson is about how to export data from R. So far we learned about reading data into R. However, sometimes you would like to share your data with others and need to export your data from R to some format that your collaborators can see. As discussed above, CSV format is a good candidate because of its simplicity and compatibility. Let’s say you have a data frame in the R environment that you would like to export as a CSV. To do so, you could use write_csv() from the readr package. Since both methods are fairly similar, let’s look at the one from readr package. Since we’ve already created a data frame named df_csv, we can export it to a CSV file using the following code. After typing this command, a new CSV file called my_csv_file.csv will appear in the Files section. write_csv(df_csv, path = &quot;my_csv_file.csv&quot;) You could similar save your data as a TSV file using the function write_tsv() function. We’ll finally note that there are default R functions write.csv() and write.table() that accomplish similar goals. You may see these in others’ code; however, we recommend sticking to the intuitive and quick readr functions discussed in this lesson. 22.1 Special R data files R has some special file types for storing data that are only for use with R. The perks of using these data types is that they are generally better at saving the data the way that R has them formatted in the environment so you won’t have to do as much manipulation after importing data from a special R data file type. 22.1.0.1 Saved R objects (.rda, .RData) As you write R code (say, in an .R file), you will inevitably create objects. For example, in the code you see here, mpg is an object created by this code that contains the data from the mpg column of the mtcars data frame and cyl an object from the cyl column of the mtcars data frame. mpg &lt;- mtcars$mpg cyl &lt;- mtcars$cyl mpg and cyl are objects If I were to run these two lines of code, they would show up in the Console, and the objects mpg and cyl would now be visible in the Environment tab. mpg and cyl visible in Environment tab Often you’ll create objects that you won’t need to save. However, from time to time you’ll make changes to a data frame or generate an object that you’ll want to use later. To save an object and use it later, you can save it as an R data file. These files contain objects that can be saved directly from and read directly into R using the commands save() and load(). And, in particular, R data files can save multiple objects. To save the two objects mpg and cyl into an R Data file, you would do the following: save(mpg, cyl, file=&quot;mtcars_objects.rda&quot;) save The syntax for save requires you to specify the objects you want to save, separated by commas. Then, the filename you want this R data file to have is specified using the file= argument. When this line of code is run, the specified file (here, mtcars_objects.rda) appears in the Files tab within RStudio, demonstrating that this file has been created. If in the future, if you were to want to load the objects mpg and cyl back into your R session, you would do this using load(). To do that here, we’ll first have to remove these objects from RStudio. To accomplish this, click on the broom in the Environment tab of RStudio Cloud. broom icon removes all objects from R session After clicking “Yes” to confirm that you want to remove all objects, you’ll see that mpg and cyl are no longer in your RStudio environment. They have been removed from RStudio. Empty Environment To load them back in from mtcars_objects.rda, you would do the following: load(&quot;mtcars_objects.rda&quot;) Note that you need to put quotes around the filename to specify that this is a file and not an object in R. A second note is a reminder about file paths. Here, the file we want is in the directory we’re working in. If that were not the case, you would have to specify the path to the file within the quotation marks (i.e. “path/to/file/mtcars_objects.rda”) When you run this line of code, you’ll see that cyl and mpg are once again available objects in your RStudio Environment tab! load loads objects in .rda file back in To recap, when you want to save multiple objects to be read in at a later time, you’ll save them as an R data file (.rda). R Data files are generated using the save() function and read into R using the load() command. 22.1.0.2 Serialized R objects (.rds) Serialized R object files are similar to R data files with the exception that they can only save a single R object at a time. When you only want to save a single object (say, mpg), these files are the best option. To save a serialized R object file, you’ll use the function saveRDS(). To read a serialized object file into R, you’ll use the function readRDS(). For example, to save the mpg object you would use the following: saveRDS(mpg, file=&quot;mpg.rds&quot;) saveRDS This will generate the file mpg.rds. To read this file back in, we’ll first remove all objects from R Studio using the broom icon in the Environment tab and will then use the readRDS() function. broom icon to clean up Environment Unlike with .rda objects, when you read a .rds object back in, to make the object available, you have to assign the object a variable name. Otherwise, the contents of the object will just print to your Console. This is helpful because it doesn’t matter what the name of the object was when you saved it. As long as you know the filename, you can pick a new object name. Here, we assign the serialized R object file to mpg, but we could have chosen a different object name. mpg &lt;- readRDS(&quot;mpg.rds&quot;) readRDS 22.1.0.3 R Project files (.Rproj) R Projects are incredibly helpful files. In their simplest form, by working within an R Project and saving an .Rproj file, you are always able to pick back up on a project from where you left off. Fortunately, RStudio Cloud uses R projects automatically. You may have noticed that there is a “project.Rproj” file in your Files tab. This is an R Project file. RStudio Cloud will start a “project.Rproj” file within each project you start. This file will then track your project as you write code and create objects. R Project files Then, when you return to this project, your .Rproj file will: Start a new R session Load the .RData file in your project’s main directory file if there is one (this is the same directory where your .Rproj file is) Load the .Rhistory file into the RStudio History tab Set the current working directory to the project directory. Make previously edited documents available in the Files tab Restore RStudio settings to what they were the last time the project was closed. This allows you to pick back up from where you left off! Then, when you leave to work on a different project, your .RData will be saved and your .Rproj file updated so that you can pick up from where you left off the next time you sign in! 22.1.1 Datasets in R Now that we’ve covered the main types of R files you’ll be working with and generating in RStudio, we’ll move on to formally discussing the datasets available within R and RStudio/RStudio Cloud. You’ve previously seen and worked with datasets that are available to you in R. Specifically, you’ve seen examples that used the iris and mtcars datasets in previous lessons. However, we haven’t covered how to find all available datasets that are included in R automatically, so we’ll do that now. To access a list of all the available datasets within R, you’ll type data() into your Console. This will open up a file called “R data sets”. You can scroll through this list to get a sense of what data sets are available. The name of the data set is in the first column while a brief description of the dataset is in the second column. R data sets If one of these datasets looks interesting to you, say the “Orange” dataset, you can get more detailed information about the dataset by using help function in R (?). ?Orange This opens up an explanation in the Help tab of RStudio Cloud. Here, you can see a description of the dataset and can scroll to see what variables are included in this dataset. Finally, at the bottom of the Help window, you’ll see examples of how to work with the dataset! Orange dataset Help page 22.1.2 Datasets in Packages In addition to the datasets available by default to you in R, many packages (but not all!) also contain datasets that you can use. To see a list of what datasets are available from a specific package, you can again use the data() function, but you’ll want to specify the name of the package using the package= argument For example, to see a list of the datasets available from the ggplot2 package, you would want to use the following: data(package=&quot;ggplot2&quot;) datasets available from ggplot2 package As before, this function will open up a list of the available datasets within that package. In order to use any of these datasets, however, you’ll have to load the package using the library() function. After loading the package, you can then access the Help pages for a dataset as previously using ?. library(ggplot2) ?msleep msleep from ggplot2 package dataset Help page "],["finding-data.html", "Chapter 23 Finding Data 23.1 Where to get open source data 23.2 dslabs 23.3 FiveThirtyEight Data 23.4 Kaggle 23.5 More places to get data", " Chapter 23 Finding Data Now that we know what data are, how to work with them in RStudio Cloud, and how to get them into RStudio Cloud, if you have a question you want to answer with data, where do you find data to work with? In some cases you’ll have to create your own data set but in other cases you can find data that others have already generated and start from there! In this lesson, we’ll discuss the difference between public and private data and direct you to a number of resources where you can find helpful data sets for data science projects! 23.0.1 Public versus Private Data Before discussing where to find data, we need to know the difference between private and public data. Private data are datasets to which a limited number of people or groups have access. There are many reasons why a dataset may remain private. If the dataset has personally-identifiable information within it (addresses, phone numbers, etc.), then the dataset may remain private for privacy reasons. Or, if the dataset has been generated by a company, they may hang onto it so that they have an advantage over their competitors. Often, you will not not have access to private data (although sometimes you can request and gain access to the data or pay for the data to get access). But that’s OK because, in general, public data are freely-available. Unlike private data generated by companies, data generated by governments are often made public and are available to anyone for use. 23.0.2 Publicly-available data As a data scientist, there’s a good chance you may work with private company data as part of your job. However, before you have that job, it’s great practice to work with datasets that are publicly-available and waiting for you to use them! In this section, we’ll direct you to sources of different datasets where you can find a dataset of interest to you and get working with it! 23.0.2.1 Open Datasets There are a number of companies dedicated to compiling datasets into a central location and making these data easy to access. Two of the most popular are Kaggle and data.world. On each site, you’ll have to register for a free account. After registering you’ll have access to many different types of datasets! Explore what’s available there and then start playing around with a dataset that interests you! kaggle and data.world are great places to look for datasets Publicly-available datasets are also curated at Awesome Public Datasets, so feel free to look around there as well! 23.0.2.2 Government Data Government data can provide a wealth of information to a data science. Government data sets cover topics from education and student loan debt to climate and weather. They include business and finance datasets as well as law and agriculture data. Here we provide lists of governments’ open data to just give you and idea of how many datasets are out there. This will only include a tiny portion of what cities and federal governments’ data are available for you to use. So, if there’s a place whose data you want to work with, look on Google for “open data” from that place! 23.0.2.2.1 US Data If you’re interested in working with government data from the United States, data.gov is place to get datasets that have been released by the the United States government. Here you can find hundreds of thousands of datasets. These data cover many topics, so if you’re interested in working with government data, data.gov datasets is a great place to start! data.gov has hundreds of thousands of datasets 23.0.2.2.2 Census Data The US Census is responsible for collecting data about the people within the United States and United States’ economy every ten years. These data are also accessible online and they can be worked with in R using the very helpful tidycensus package! The US Census provides data about the US people and economy 23.0.2.2.3 Open City Data The US’s federal government is of course not the only place to obtain government data. More and more cities across the world are starting to release open data at the city level. A few of these cities and their respective open city data links are provided below: Baltimore, MD (USA) Cincinnati, OH (USA) Las Vegas, NV (USA) New York City, NY (USA) San Francisco, CA (USA) Toronto, Ontario (Canada) Additionally, to see a summary of what datasets are available from cities across the USA, check out the US Open City Data Census from the Sunlight Foundation. US City Open Data Census 23.0.2.2.4 Global Data In addition to the United States, there are many other countries providing access to open data with more and more providing access and updated datasets each year. These include (but are not limited to!) datasets from many countries within Africa and Latin America as well as Canada, Ireland, Japan, Taiwan, and the UK. Additionally, to see what datasets are available globally, the Global Open Data Index is a great place to start! Global Open Data Index 23.0.2.3 APIs We’ve mentioned APIs previously, but it’s important to include them here as well. APIs provide access to data you’re interested in obtaining from websites. There are APIs for so many of the websites you access regularly. Google, Twitter, Facebook, and GitHub (among many others) all have APIs that you can access to obtain the dataset you’re interested in working with! 23.0.2.4 Company Data Finally, we mentioned above that companies often keep their data private for a number of reasons, and that’s ok! When companies do release their data, they will often be found on websites like Kaggle and data.world. If there is a company whose data you’re interested in, you can search for the company’s data on either of these two data repositories or on on the company’s website directly to see if they provide the data there or if you can scrape their website to obtain the information you need! There may not always be a way to get the exact dataset you’re looking for, but you can often find something that will work! 23.0.3 Data You Already Have Sometimes, it’s not about finding data someone else has already collected on a bunch of individuals in a population. Rather, getting data sometimes just involves taking a look at things you already have but just haven’t yet realized are data you can analyze. For example, MP4 files you’ve bought and have on your computer are data! They can be analyzed using tuneR and seewave. You could use this type of data to categorize the music in your library or to build a model that takes data on what songs were already big hits to determine what qualities of a song predict that it may be a big hit. Alternatively, you could scrape the websites you frequently visit (using rvest!) to answer interesting questions. For example, if you were interested in writing a really great title for the newest video of your pet doing something super cute, you might scrape the web for titles of pet videos that have recently gone viral. You could then craft the perfect title to use when you upload your pet video. Granted, this may not be an example answering the most important type of data science question; however, writing up how you did this would make a really great blog post, which is something we’ll discuss in a lesson in a few courses! Finally, social networking websites like Facebook and Twitter, collect a lot of data about you as an individual. You have access to this information through the websites APIs, but can also download data directly. After news of the Facebook and Cambridge Analytica data breach, many articles were published about how to download your Facebook data. These data can be downloaded and then analyzed to look at trends in your data over time. How many pictures have you uploaded and been tagged in over time - has that changed? What topics do you most frequently discuss in Messenger? Or, maybe you’re interested in mapping the places you’ve been based on where you’ve checked in. All of these data can be analyzed from data that are already there, just waiting for you to work with them! In all, sometimes getting the data just means realizing the data you already have at your disposal, figuring how to get the data into a format you can use, and then working with the data using the tools you have! 23.1 Where to get open source data Often times for a particular project, you will have data given to you to work with. But sometimes you may want to go out and find data to work with for a particular question. In this section, we will give you some places you can go to get open source data that is freely available for you to use! Just make sure that you attribute where you got the data from originally 23.2 dslabs dslabs There’s 26 datasets already set up in R for you. To use the datasets in this package you need to do the following kind of steps: # Install the package install.packages(&quot;dslabs&quot;) library(dslabs) For the dataset admissions for example, you first need to prep it with data() function and then you can call it as an object. data(admissions) admissions 23.3 FiveThirtyEight Data There’s a variety of datasets available as CSVs. Go to the GitHub When you find the dataset you are interested in, click on that folder and then click on the data file you are interested in. Then click Raw. Copy and paste the URL that that brings you to. For example, for the airline-safety dataset, the URL would look like this: https://raw.githubusercontent.com/fivethirtyeight/data/master/airline-safety/airline-safety.csv Then you can use that URL in a read_csv function like this: airline_safety &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/airline-safety/airline-safety.csv&quot;) 23.4 Kaggle Kaggle has a variety of datasets that are generally available as CSV. You’ll have to make a login before you download, but you can use a Google login. To download a dataset, you can browse them and then when you find one you like, you can download the CSV. It will download it in a zip file. Upload this to your RStudio server using the Upload button. Then you will see a CSV included in that data file and you can use readr::read_csv()to read in the file. For example, for the Data Science Job Salaries dataset, you can click Download and then upload the resulting zip file. Then you can read in the file like: readr::read_csv(&quot;ds_salaries.csv&quot;) 23.5 More places to get data There are endless numbers of places on the internet to get data. Above are some great ones to get started with. But as you get more comfortable with data analysis, you may want to look at some of these other sources. Some of these source will require a bit more work to get these data, but are just as available for you to use. Open Case Studies Top 100 Open Source Datasets for Data Science 10 Open-Source Dataset Finders For Your Next ML Project UCI Machine Learning repository World Health Organization Datasets Mapping police violence Center for Disease Control Data Forensic science data portal Google Public Data Explorer "],["ethical-data-science.html", "Chapter 24 Ethical Data Science 24.1 Internet Safety 24.2 Data Privacy", " Chapter 24 Ethical Data Science From the emails automatically marked as spam in your Inbox and facial recognition on Facebook to the targeted advertisements you see whenever you’re online, data science has made its way into our everyday lives. Often, this is a positive because data scientists generally set out to improve the world (say, by analyzing crime data in hopes of reducing violent crime) or to make our lives easier (by preventing spams in our mailboxes, for example). However, data science projects are certainly not without negative consequences. We discussed some of the companies who have run up against issues properly securing user data in the last lesson; however, protecting the privacy of users’ data is not the only time data scientists should consider the ethics of their work. In fact, ethical considerations should be made from the start of a project and should never stop being made. We’ll discuss what we mean by that in this lesson. When we talk about the ethics of data science, we’re talking about how to use data to improve the lives of individuals in our society in the world without causing harm to any groups with our work. We’re talking about putting in the work ahead of time to avoid unintended negative consequences of our work. Rather than acting surprised when groups are inadvertently harmed or only dealing with unintended consequences after they occur and harm others, an ethical data scientist will consider whether they are doing any harm at each step along the way. 24.0.1 Data Science Team Data science projects can be done by an individual; however, more often, they are carried out by a team of individuals who are all interested in using data to answer an interesting question. Whether it’s one person setting out to answer the question or a team working to answer this question, it’s important at this outset of a project to really think about whether all the necessary individuals are on the team. For example, if a group of data scientists were setting out to reduce crime in a neighborhood, it would be in their best interest to either be very familiar with that neighborhood (say, live there themselves) or to talk to individuals who currently live in that neighborhood to be as informed as possible before they set out to analyze their data. By working with individuals who have more knowledge about the neighborhood than someone who has never been there, the team will be less likely to make incorrect assumptions or leave out critical pieces of information when analyzing the neighborhood’s data. 24.0.2 The Data After ensuring that the team you’ve assembled to answer the question is the right team to do so, there are many considerations to be made when collecting data. 24.0.2.1 Sampling Bias When data are collected, it’s difficult to ever get information about an entire population. Thus, when data are conducted, researchers will typically get data from a subset of individuals within the population (a process called sampling) and then infer something about the entire population using what they learned from the subset of individuals. They will try to ensure that their subset is a random subset of the population. However, whenever sampling of a population is done, there’s a chance for sampling bias. Sampling bias occurs whenever the sample of the population collected does not accurately reflect the population as a whole. For example, if the population you’re interested in studying is half female and half male, but the subset of people you’ve sampled is made up of 70% females and 30% males, then your sample is biased. Having a plan to avoid sampling bias is an important first step in the process of data collection. Checking the data after the data have been collected to ensure that the sample is not biased is an equally important step. When data are collected by survey for example, they may be sent out to an equal number of males and females; however, maybe more males respond than females. Even though your plan was solid, your responses are biased. This has to be accounted for during the analysis, as your responses do not accurately represent the population as a whole. And to be clear, gender is not the only consideration to make when sampling. Samples can be biased by many factors including (but not limited to) race and ethnicity, age, income, and education. Sampling bias should always be avoided both when planning to collect data and after the data are collected. 24.0.2.2 Informed Consent In addition to collecting data in an unbiased manner, data must be collected ethically. This means that individuals must consent to participating in data collection. Informed consent requires that the individual agreeing to share their information knows what data are being collected, has been informed of any possible consequences, and has been provided full knowledge of any risks or benefits of participation. If data are collected on individuals and later on a data scientist wants to use those data for a different purpose, they can only do so if the initial consent allowed for the data to be used in this manner. Alternatively, if the data are de-identified, they may be able to be used for future analysis; however, this must be done with care. 24.0.2.3 Privacy When collecting personally-identifiable information, these data must be protected. They should not be shared without the participants’ consent and they should be stored in a secure manner. 24.0.2.4 Withdrawal of consent If an individual consents to their data being collected and later changes their mind, they have every right to do so. It is your responsibility as the person collecting the data to have an effective plan for removing any data already collected from that individual. 24.0.3 The Analysis After avoiding sampling bias, collecting data ethically, avoiding privacy issues in data storage, there are still a number of ethical considerations to take into account during ones analysis. While we don’t discuss the details in these courses, machine learning is currently a popular way to analyze data. In general, it uses existing data to predict how individuals will behave in the future. This means that the any biases in the data used to build the model (the existing data) will likely be perpetuated in the predictions. This means that, for example, in predictive policing, where data scientists are trying to predict where crimes are more likely to happen, if the data used to build the model (often referred to as an algorithm) come from crimes that have occurred in primarily black neighborhoods, they are going to predict that crime happens more in primarily black neighborhoods. Then, if police officers are placed in these neighborhoods, they are going to find more crime, not because crime has increased, but just because there are more of them in the neighborhood now. Essentially, any biases in the data used initially are going to be perpetuated in the models they generate. This means that it is critically important that data scientists fully know and understand the data their working with, its biases, and its limitations. Further, even when something like race is predictive in the model, it’s important for a data scientist to consider whether or not that variable should be included. Are you absolutely positive that race is the important variable and it’s not just that you have a biased sample? Instead, if you’re trying to predict crimes, it’s likely best to use data about crimes that have occurred and where they have occurred in the model rather than race, since race is not a measure of what you’re interested in modeling. Considering the implications of ones work when carrying out an analysis is incredibly important, and making decisions that do not unnecessarily harm any group of people is critical It was previously common for analysts to say that “data do not lie” and “algorithms are impartial,” but recent history has shown that that is simply not the case. Data are collected by humans, and humans are not without their biases. The best way to avoid bias in your data is to be aware of this and constantly check to make sure you’re not disadvantaging groups of people with your analyses. Before considering any data analysis and publishing your results, make sure: You have gone through your data and have made sure there is not any major flaw in your data collection. Data can be biased. You can’t use the result of a study that is mostly based on data from men to say anything about women. You have checked for common mistakes in your analysis and know that your methodology is valid and defensible. Messing up the data for a single variable can drastically change the result of your analysis. The results of your work can not be used to harass people, especially minorities, in any way. Your analysis is independent of your opinion about the specific problem you’re trying to solve using data. When carrying out an analysis, you should be looking for the answer to a question, but be careful not to want a specific answer. By wanting the analysis to go a certain way, you can subconsciously analyze the data to get that answer. It’s best just to collect the data ethically and analyze it carefully. Then, the answer is whatever the answer from the analysis says it is. You can do awesome things with your data science skills: cure diseases, analyze health data, prevent climate change, improve your city, or fact check politicians. Don’t let your biases or mistakes get in the way. 24.0.4 After the Algorithm Finally, after someone answers an awesome data science question with their really great analysis and tell the world, they often think their job is done. However, if you’ve designed an algorithm that is going to predict something going forward or that is going to continue to be used, it is your job to maintain that algorithm. That means that it’s your job to check for biases after the fact. Is your analysis disadvantaging groups of people? If it is, should it be? Does something need to be changed. There has to be a way for you to update your algorithm going forward. If something is continuing to be used by others, your job isn’t done once you’ve built the algorithm. It’s your job to check for biases after the fact and to update your algorithm should there be a need. 24.0.4.1 Ethical Questions to Answer To summarize this lesson up to this point, be sure to answer all of these questions for every data science project you carry out: Does the team on this project include all the necessary individuals to avoid bias in our analysis? Does our data collection plan address ways to avoid sampling bias? Are the data we’ve collected / the data we’re using to answer the question of interest biased? In what ways? Has informed consent been obtained from all participants? Do we have a mechanism do remove an individual’s data from our dataset if they so choose? Are the variables we’ve chosen to use for our analysis appropriate? Do they discriminate against anyone? Is our analysis transparent? Do we understand how and why we’re getting the answer we’re getting? Have we considered possible negative or unintended consequences of our analysis and its results? Do we have a way to update our analysis/algorithm going forward should biases in the results be found? 24.0.5 Ethics in Data Science Now that we’ve discussed the ethical considerations to be made before and throughout every data science project, we’ll discuss a few data science projects that were recently covered the popular media due to the questionable ethics of the project. We present these examples not only to highlight the ethics of the particular project, but also to state the importance of considering the implications of your work. It is not enough to argue that you just “did the analysis” but “didn’t think of the implications” of the work. As a data scientist it is your responsibility to both do the analysis and consider the implications of your work. 24.0.5.1 Data Science in Sentencing In April of 2017, Wired reported in an opinion piece that courts were using artificial intelligence to sentence criminals and made the claim that such practices must be put to an end. The piece explained that courts and corrections departments across the United States collect data on defendants that are then used in an algorithm to determine the “risk” that defendant poses. This aim of this algorithm is to determine how likely it is that the individual will commit another crime. These likelihoods of reoffending from the algorithm are then used to make decisions about how to set bail, what the sentence should be, and the details of that individual’s parole. These tools are often built by private companies. This means that exactly how they work and what data are used to assess risk are not disclosed. The article in Wired highlights the use of Compas, one of these risk-assessment tools developed by a private company and used by the Department of Corrections in Wisconsin, in a judge’s decision to give a man a long sentence, in part because of his “high risk” score from the algorithm. In a courtroom, however, because the specifics of how this algorithm works are not disclosed, a judge and jury would not be able to determine whether or not the likelihood of reoffending was calculated accurately by the algorithm. Initially, arguments were made that removing human biases and judgment from bail, sentencing, and parole decisions would be for the benefit of society. This initial goal was a noble goal that would improve society. However, over time, biases in the data used to generate these algorithms that perpetuated into biases in the algorithms’ outcomes, the lack of transparency in how they work, and failure to check for inequities and biases in the algorithms after they were put in place have led to unfair decisions being made. While individuals may have been looking to improve society with their algorithm at the outset of this project (or at least that argument can be made), lack of considerations of the implications of their work and lack of transparency in the process of generating and using the algorithm have led to questionable ethics in the use of algorithms to make sentencing decisions. 24.0.6 Artificial Intelligence in the Military In March of 2018, Gizmodo reported that Google was helping the US Department of Defense use artificial intelligence to analyze drone footage, a project that started in April of 2017. The project between Google and the Defense department is called Project Maven, and the goal of this project was initially to take drone footage and accurately identify objects (such as vehicles) and to track individuals in the footage collected. A number of Google employees were upset that Google would provide resources to the military that would be used for surveillance in drone operations. Due to the fact that in other non-military situations the use of machine learning has led to biased outcomes, other employees felt there needed to be further discussion about the ethics in developing and using machine learning technology before it was deployed by the military. In response to these concerns, a Google spokesperson stated that Google is working “to develop policies and safeguards”, that the technology is being used for “non-offensive uses only.” In this and many large data science projects where machine learning and artificial intelligence are being used in situations where lives could be negatively impacted, the project would benefit from taking ethical considerations into account and discussing these in a transparent way before deploying the technology. Developing policies and safeguards after the fact is not enough. 24.0.6.1 Facial Recognition in Law Enforcement In May of 2018, the ACLU reported that Amazon had teamed up with law enforcement agencies to use Amazon’s face recognition technology, Rekognition. The ACLU called this new technology, which can quickly scan images to “identify, track, and analyze people in real time” both “powerful” and “dangerous.” This story was picked up by many news sources, including a piece in the New York Times Proponents of using facial recognition technology in law enforcement cite that such technology can help locate and identify criminals more quickly than we would have been able to in the past and that it will help “keep residents and visitors…safe”. Proponents also argue that those who use the technology do so within the limits of the law. Opponents, like the ACLU, however, cite the civil liberties and civil rights concerns that constant surveillance with using facial recognition technology pose. The ACLU argues that there is substantial “capacity for abuse” and due to this, that citizens should not be watched by the government any time they walk outside, that facial recognition systems threaten freedom, and that the deployment of these technologies post a greater threat to communities that are already unfairly targeted by the political climate. Concerns with the technology cite that anyone can be identified, not just criminals, a problem for civil liberties. Opponents further question the accuracy of the technology. Inaccuracies, which would misidentify individuals and suggest they were guilty of a crime when they were in fact not is clearly a problem. Further, there are concerns that accuracy varies based on gender and race, which if true, poses a clear bias in the use of this technology for the identification of criminals as certain groups of individuals would be misidentified more frequently than others. 24.0.7 Additional Resources Ethical Checklist for Data Science, by Alan Fritzler Weapons of Math Destruction, by Cathy O’Neil Automating Inequality, by Virginia Eubanks Courts are Using AI to Sentence Criminals. That must Stop Now, by Jason Tashea in Wired Machine Bias, by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner at ProPublica Amazon Teams Up With Law Enforcement to Deploy Dangerous New Face Recognition Technology, by Matt Cagle and Nicole A. Ozer with the ACLU Amazon Pushes Facial Recognition to Police. Critics See Surveillance Risk., by Nick Wingfield at the New York Times Google Is Helping the Pentagon Build AI for Drones, by Kate Conger and Dell Cameron at Gizmodo 24.1 Internet Safety Just like driving a car, there are inherent risks to using the Internet and doing data analysis on the cloud. These risks shouldn’t mean that you should always be fearful about your online safety, however, they should make you more aware that there are consequences if you are not cautious. Internet safety (or online safety) revolves around being knowledgeable about what these risks are, knowing how to deal with risks that arise when working online, and working hard to increase personal safety against security risks associated with using the Internet. The goal of Internet safety is to maximize personal safety and minimize security risks. We’ll discuss the basics of this below, but know that there are people whose entire careers are dedicated to Internet safety, so this will simply touch on the basics. 24.1.1 WiFi WiFi is the technology that enables you to wirelessly connect to the Internet. Computers, smartphones, video-game consoles, and TVs are among the many types of devices that are WiFi-compatible. As is often the case, increased access to the Internet comes with increased risk to network security. 24.1.1.1 Public WiFi Completely-open and free public WiFi, or Internet access that does not require a password or a screen to login from is the least secure type of network. When connected to public WiFi, be extra vigilant. Avoid working with sensitive data while on a completely open and public network, and do not make online purchases while on this network. 24.1.1.2 Semi-Open WiFi Semi-open WiFi networks are networks potentially open to everyone, but that may require a password (maybe one that is printed on your receipt at a coffee shop) or may require you to login by providing an e-mail address. When given the choice, opt to connect to these networks rather than completely-open public WiFi networks. 24.1.1.3 Password-protected WiFi The most secure option, however, is when you connect to a password-protected WiFi network, so use password-protected WiFi whenever you have the option. WiFi at your home should absolutely be password-protected. 24.1.1.4 Use HTTPS Whenever Possible The “S” in HTTPS stands for secure. So, what does HTTP stand for? HTTP refers to “Hyper Text Transfer Protocol.” You’re likely most familiar with these letters as being part of your website. When you type “www.gmail.com” into your Chrome browser, Chrome redirects you to “https://mail.google.com/mail/u/0/#inbox.” HTTP refers to how the data are sent from your browser (where you typed “www.gmail.com”) and the website to which you’re trying to connect (www.gmail.com). When the web address starts with https rather than http, this means that the data sent over your Internet connection are encrypted. Encryption is a process that generates a message that cannot be decoded by anyone without a key. Thus, if someone were to intercept your encrypted data as it were transferred over the Internet, they may be able to intercept your data, but they wouldn’t be able to decipher the information, as they wouldn’t have access to the key. When a website is using encryption (https://), there will be a padlock to the left of your web address bar to indicate that the HTTPS connection is in effect. When sharing sensitive information on the Internet (ie a credit card number or personal health information), it should only ever be done on a secure (encrypted) network. 24.1.2 Passwords In the age of apps for everything and technology being everywhere, you likely have a lot of requests daily for passwords. While they may seem like an annoyance, using passwords, and importantly, using good passwords is worth the effort. 24.1.2.1 Strong Passwords Strong passwords may be harder to remember, but that’s the point. You don’t want someone else to be able to easily guess your password. Characteristics of Strong Passwords: Use a combination of random letters Use both capital and lower case letters Include numbers, letters, and symbols Disperse the letters and symbols throughout the password (not just at the beginning and the end) Has at least 12 characters What to Avoid when Creating A Password: Avoid using names of you or anyone in your family Do not include the words “Passcode” or “Password” Avoid using sequential numbers (i.e. 123) Avoid using your telephone number Don’t make obvious substitutions to words (i.e. avoid simply replacing the letter “o” in a word with the number “0”) 24.1.2.2 Utilize Passwords When given the option, use a password. Do not opt out of using a password to log in to your phone or your computer. When asked to set a password, do so, and make it a good one. In addition to setting passwords, make sure you have multiple different passwords. Do not use the same password for all your most important accounts. If someone were to log onto your WiFi network, you wouldn’t also want that person to be able to gain access to your credit card, Gmail, bank account, and/or Facebook account. Use different passwords for different accounts. 24.1.2.3 When to Change Passwords If your password was set by a third-party company (say your Internet provider), you should change your password right away. Otherwise, it’s a good practice to change passwords to your most sensitive information (i.e. WiFi, credit cards/banking, etc.) at least every six months. 24.1.2.4 Never Share Passwords A final word on passwords: never share them. Reputable companies will never ask for them. Your bank will never require you to tell it to them. And, neither will the IRS. If someone is asking for your password, it is almost certainly a scam. 24.1.3 Good Internet behavior On the Internet there are few basic guidelines to follow that will help you be a good citizen and help keep you safe. This list is surely not exhaustive, but it’s a good start. When online: * Don’t be a jerk * Never share your passwords * Think before you click * Don’t click on links sent to you from people you don’t know * Don’t click on links from someone you do know if it doesn’t seem like something they would send (i.e. an email with a weird subject line that doesn’t sound like them or a link in an email that says “Vacation Pix” when they wouldn’t normally send those pictures, wouldn’t use the word “Pix”, or haven’t recently been on a trip) 24.1.4 Online Scams While we would love if everyone on the Internet behaved well (and most people do!), there are of course bad actors. To avoid getting caught up in an online scam, beware of: people/websites posing as people from a trustworthy company who attempt to obtain your private information - emails asking for credit card numbers or passwords are scams people who call and ask for passwords to accounts and who seem to know a lot about your family and claim to be from law enforcement. This is likely a scam. You can always hang up and call your local law enforcement directly to be sure. people/websites offering prices that are way too low for housing or big purchases on the Internet. These are likely a scam. 24.1.5 Malware &amp; Spyware Finally, malware and spyware are software designed to be malicious. The goal of malware and spyware is to collect your private information (usernames, passwords, credit card numbers) without you ever knowing. This software often acts through e-mail or other software. By avoiding clicking on suspicious links and not downloading software with which you’re not familiar, you can avoid the issues caused by malicious software. 24.1.6 Security on a Chromebook Because Chromebooks do not run a typically operating system, they are much more secure than other laptops. While explained in greater depth on Google’s support documentation, Chromebooks have certain advantages over other laptops when it comes to security: Automatic updates - ensure that your Chromebook is never behind on security updates Sandboxing - each web page runs in its own environment (sandbox), so if one page is infected, that page won’t affect other tabs or apps running at the time. Verified Boot - every time the Chromebook starts up, it does a self-check to detect any issues with security. Encryption - web apps on a Chromebook store all data safely in the cloud with encryption, making it resistant to tampering 24.2 Data Privacy In this lesson we’ll cover what data privacy is, why it’s important, and discuss encryption in slightly more detail. Again, remember that there are people who dedicate their entire careers to data privacy. This lesson simply touches on the basics. Feel free to search on the Internet to learn even more about Data Privacy! 24.2.1 What is Data Privacy? Data privacy is: {cite: “Wikipedia”, url: “https://en.wikipedia.org/wiki/Information_privacy”} &gt; the “relationship between the collection and dissemination of data, technology, the public expectation of privacy, and the legal and political issues surrounding them.” This complex definition correctly suggests that data privacy is not a simple matter. A simpler definition of data privacy would maybe be how to keep personal and private information safe. Concerns arise with data privacy whenever there is personally identifiable information (PII) or sensitive information that is being collected, stored, used, (and eventually destroyed and deleted). When handled appropriately, only people who should have access to the information do. When mishandled, this information could be accessed by others who should not have that information. Data are everywhere, so it’s incredibly important to think about data privacy early on in every single data science project. It’s much better to be too vigilant than to make a mistake, allow data to fall into the wrong hands, and have to try to clean up the mess, as you’ll see in a few examples below. So, think of data privacy concerns early and often to avoid making a mistake. 24.2.2 Personally Identifiable Information In the US, Personally Identifiable Information, often simply referred to by the terms initials, PII, is any “information that can be used on its own or with other information to identify, contact, or locate a single person, or to identify an individual. This could be a first and last name, a picture of someone’s face, a driver’s license number, a date of birth, an address, an individual’s genetic code, or a telephone number, among many other pieces of information. All of these pieces of information could help identify a person. It’s these type of data that must be handled extremely carefully. When you sign up for an app on your computer or phone, you likely provide that company with some PII. You often give them your name and your address (among other information). You likely assume that this company will handle your data appropriately, and, if everyone is following the rules, they will. This means they won’t share it with others, use it for purposes other than those for which you’ve given them permission, or allow others companies/people to steal your information. However, as reported regularly in the news, companies do not always handle your data as they should. We’ll discuss what concerns you should have as you’re working with other people’s data and how data should be handled. 24.2.3 What is encryption? To review from the last lesson, when data are encrypted, all the information is jumbled into a code that can only be decoded by someone who has the key. This means that if data are intercepted while being transferred, the person who has intercepted the data will not be able to understand the data, as they won’t have access to the key and will only have the completely jumbled information. It is a security measure to help keep data safe. 24.2.3.1 Working in RStudio Cloud In earlier lessons, you’ve been working in RStudio Cloud and will continue to do so as your hone your data science skills. Data on RStudio Cloud are encrypted at rest (when they’re sitting in your project) and in transit (when they’re being uploaded or downloaded). This means that, if intercepted, the person who has intercepted the data will not be able to decipher the information contained, as they don’t have the key. However, if you download data from RStudio Cloud and store them directly on your computer, they are no longer encrypted. If you have PII in a dataset that is stored on your laptop directly, that information is not protected. So, be careful working with datasets that contain PII that have been downloaded to your computer. 24.2.3.2 HTTPS An additional reminder: websites that have https:// (rather than just http://) also use encryption for data transfer, increasing security for data that are transferred. When working with PII and transferring data, be sure that the websites you’re working with are using HTTPS. 24.2.4 Human Security Human security is the concept that individual people (rather than just countries) should be kept safe and secure. With regards to data science, this means that when working on the Internet, regulations and laws should be made not just to protect the security of a nation, but rather to protect every individual and their data. Data Science projects should NOT: * increase risk or harm * pose threats to individuals * make individuals’ PII publicly-available or susceptible to theft * break any laws * share data with other groups/companies without the consent of the individuals * saved data insecurely 24.2.5 Computer Security Computer Security (or cybersecurity) is {cite: “Wikipedia”, url: “https://en.wikipedia.org/wiki/Computer_security”} &gt; “the protection of computer systems from the theft and damage to their hardware, software or information, as well as from disruption or misdirection of the services they provide.” This means that, in addition to keeping individuals’ data safe (maximizing human security), data science projects must also consider how to ensure that the computers and software they use and create are designed in a secure manner. For data science projects, this means that data being transferred over the Internet should be done so safely (using encryption) and that software you develop or use should not post PII or other sensitive to the Internet. 24.2.6 Open Science There is an important movement in science currently where individuals promote what is known as open science. Practitioners of open science support making their scientific studies as transparent as possible. Specifically, they provide all the data, programming code, and software necessary to repeat their analyses. Open science is great for the scientific community and public, as everyone has access to the tools and data used. However, it is not without its privacy risk. Open Science practitioners are still responsible for protecting PII. This means that individual-level information (names, dates of birth, addresses, etc.) must all be removed from one’s dataset before it is released to the public. Being a supporter and practitioner of open science is great, but you must always ensure that when you’re releasing and sharing data, you are not inadvertently putting others at risk. 24.2.6.1 Open Data Open data are data that are freely-available to anyone. Local and federal governments as well as many scientists make their data freely-available to the public. Open data are wonderful in that anyone can work with them. Sharing data is a great thing! But, sharing must be done with the caveats already mentioned. Individual-level information (PII) should be removed from datasets prior to release. Individuals should not be identifiable from data freely-available on the Internet. 24.2.6.2 Open-source Software Like open data, open-source software are software that are designed to be freely-available to anyone. This means that the software code is all available, so others can not only download and use the software for free, but also can download the code and modify it for their own purposes. The R programming language is itself an open-source project and all packages made for R are also open-source. This means that programming in R will always be free, and the code will always be publicly-available. 24.2.7 Data Breaches To understand data privacy, it’s good to know definitions and best practices, but it’s also important to learn from past mistakes. Companies have not always handled data securely. Here, we’ll discuss a few famous data breaches and touch on what went wrong so these errors can be avoided in the future. 24.2.7.1 Facebook In March of 2018, The New York Times and The Guardian both broke stories about how data from at least 50 million Facebook profiles were used by a company called Cambridge Analytica to build profiles of individual US voters, allowing individuals be to targeted with personalized political advertisements. While the details are provided in the links provided above, briefly here, the data were mined from individuals who had consented for their data to be used for academic purposes. However, Cambridge Analytica was collecting these data for non-academic purposes and collecting data from friends of these individuals who had consented to have their data collected but who had not consented to have their data used. This data breach at Facebook demonstrated that Facebook did not have sufficient protocols in place to protect user data. As a result, Mark Zuckerberg, Facebook’s CEO, has had to testify before Congress and the company has promised to make changes to improve data security. 24.2.7.2 Equifax In the Fall of 2017 Equifax, a credit-reporting agency, disclosed that partial driver’s license information was stolen from 2.4 million US consumers. This hack was traced back to a “preventable software flaw”. In other words, in March of 2017, Equifax identified a weakness in a piece of software they were using. However, they did not move to fix that weakness quickly. Partial drivers licenses were stolen from May until June of 2017. Thus, Equifax knew about the problem before any data were actually breached. The entire breach could have been avoided. Prioritizing security and moving to fix problems immediately upon realizing an issue is critically important. 24.2.7.3 Ashley Madison In July of 2015, Ashley Madison, a website that helps people cheat on their spouses, was hacked and personal information, including e-mail addresses, from 32 million site member’s were stolen and published. The windfall rippled through society, leading to resignations, divorces and suicides. The “Impact Team,” the group of hackers responsible for this hack, publicly stated that part of the reason they decided to release the data was because Ashley Madison had been charging users a $19 fee to completely erase their profile information, but that Ashley Madison had failed to actually scrub these data from their records. The security lesson here is that there should always be a mechanism by which users can remove themselves from a dataset. In the European Union, the General Data Protection Regulation (GDPR) has stated that, by law, companies must have this feature built-in. Users should always be able to remove themselves from a dataset. 24.2.7.4 OKCupid In May of 2016, researchers scraped profiles from 70,000 users on OKCupid, an online dating site, and made them completely open to the public. These researchers did not (1) have permission from OKCupid, (2) obtain informed consent from the users, nor did they (3) remove PII from the data prior to release. Since their original release on The Open Science Framework, given the issues with the dataset just mentioned and the fact that the collection of the data did not follow OKCupid’s Terms of Service, the dataset was taken down. However, hundreds of people had already downloaded the data at that point. In this case, a set of researchers did not comply with the Terms of Service they agreed to by signing up for OKCupid. While it was likely not illegal to obtain the data, as OKCupid made the data sort-of-public, it was certainly unethical, as the researchers did not take individual data security practices into consideration. So, even when you’re not necessarily doing something illegal, you can still do something unethical that does not follow good data privacy practices. 24.2.7.5 Strava A similar example of using data legally but with unintended negative consequences occurred when data from Strava, a popular fitness-tracking app, gave away the location of secret US army bases throughout the world in late 2017. Individual users would use the app to track their activity; however, in foreign countries, where Strava-users are almost exclusively foreign military personnel, bases can be identified easily from Strava heatmaps, which show all the activity tracked by its app users, as they are the only individuals contributing to the maps in those areas. The lesson to be learned here is that when releasing data, always consider possible unintended consequences. Think hard about the implications of your data and your analyses. Being vigilant and always considering data privacy and possible unintended consequences of your work are incredibly important. 24.2.8 Conclusions Hopefully we’ve explained a number of considerations to take into account when working with and sharing data. Data are powerful. It’s important to use them responsibly. Be careful with PII Fix security issues immediately Keep individuals’ data safe Don’t steal data Even if it’s legal, it may not be right or ethical Consider unintended consequences of your work "],["google-documents.html", "Chapter 25 Google Documents 25.1 Google Sheets", " Chapter 25 Google Documents There are two types of Google Documents we will discuss: - Google Sheets - for holding data - Google Docs - for writing up non-code documents 25.1 Google Sheets Google Sheets is a free, online spreadsheet program. If you’re familiar with Excel, it is similar to Excel. If you are unfamiliar with Excel, that’s ok! We’ll go through everything you need to know to get started on the project here. And, later in the program, we will go into more details to get you fully comfortable working with Google Sheets. As for right now, just know that when you have data that you want to input into a spreadsheet, Google Sheets is an ok place to start. Google Sheets is also great because you never have to worry about saving your work. If you are online, Google Sheets automatically saves your work. 25.1.1 What is a spreadsheet? A spreadsheet is a type of document where data are stored in rows and columns of a grid. Each square is referred to as a ‘cell’ in the spreadsheet. In Google Sheets (and many other spreadsheet programs like Excel), the rows are numbered (like 1,2,3,…) and the columns are labeled with capital letters (like A, B, C,…). spreadsheet If you want to talk about a specific spot on the grid you can use the number and letter corresponding to that point. For example, A2 specifies the data in cell in the first column (A) and second row (2) of the spreadsheet. spreadsheet position When you are working with data in a spreadsheet you can type directly into the spreadsheet. It is important to make sure you double check all the numbers you type since there isn’t a good way to “spellcheck” your work when you are editing a spreadsheet. We will talk a lot more in future courses about how to organize data that you have collected. Mostly we will want to collect “tidy data” which is data that has Each type of data in one column. Each data point in one row. One spreadsheet for each “kind” of data. If you have more than one spreadsheet, they should include a column in the table that allows them to be linked. 25.1.2 Setting up for our next data science project This DataTrail course is offered on Leanpub. Leanpub is a website where you can sell books and courses. In our next project, we are going to try to answer this question with data: “How does the price of a bestselling book relate to how much the author is charging for that book?” To answer our question, we need some information on books on the Leanpub website. If you go to https://leanpub.com/bookstore you will see a website that looks like this. Leanpub bookstore website This shows the bestseller books for the last week. If you click on one of the pictures of a book you can get some information on that book. If I click on the page for the first book “PowerShell 101” I see something like this. Powershell 101 landing page on Leanpub It will probably be a different book for you since it will be a different weekly bestseller. But you can look in the top left corner and see how many people read the book. This information is there for most books, but is sometimes missing if the author decides not to publish that number. In this case there are 1,036 total readers of this book. Number of readers for Powershell 101 Next we can find out the suggested price. This is on the right hand side and is the price the author thinks is the appropriate price for their book. In this case the suggested price is $15.99. Suggested price for Powershell 101 But one nice thing about Leanpub is that you can set up a “pay what you want” model where people can choose how much they pay for a book. When authors do this, there is also a minimum price they set for the book. If there is a minimum price it is also on the right hand side. In this case the minimum price is $7.99. Minimum price for Powershell 101 We could do this for each book and then we’d have a nice data set that would tell us something about the number of readers for a book and the price of that book. Then we could start to look at the numbers we collected and see if we see any patterns to the data that we have collected to try to answer our question. Here we are only collecting one “kind” of data - just data on books. The columns will be different types of information about the books. We will collect information on the name of the book, the number of sales of that book, the minimum price of the book, and the suggested price of the book. Each of those will be in a separate column. Then, for each book, we will make a new row with the data for that book 25.1.3 Setting up your spreadsheet Now let’s return to Googlesheets so we can start storing our data. When we collect the information we will use the Google Sheets software to store it for us. You will need to open up another web browser. You can do this by holding down the key ctrl and pressing t. This will open up a new tab. Leave this page open and type go to Google Sheets by navigating to the website https://docs.google.com/spreadsheets/ in the new tab. You will see something like this. Google sheets home Now click on the big plus sign and you will get a new spreadsheet that will look like this. Untitled sheet If you click on the words “Untitled Spreadsheet” you can rename the spreadsheet. Type in the words “leanpub_data” to change the name of your spreadsheet. You should now have a spreadsheet that looks like this. leanpub_data sheet We are almost done setting up the spreadsheet, now we just need to label the different kinds of data we are going to collect. Start by clicking on the upper left hand cell (A1) and type “title”. This will be the column where we are going to store information on the title of the book. leanpub_data sheet with title Then move one cell to the right, click and type “readers”. This will be where we will store how many readers a book has. Move one more cell to the right type “suggested” and then one more cell and type “minimum”. Make sure your column names are not capitalized. leanpub_data sheet with headers 25.1.4 Collecting data Now you are all set to start collecting data! To do this open another new tab by holding ctrl and pressing t, then go to the webpage: https://leanpub.com/bookstore. Click on the book and write the title, number of readers, suggested, and minimum prices on a row in the spreadsheet tab. When you are doing this make sure that: There are no commas in numbers. Just leave them out. So don’t write “1,036” write “1036” instead. You don’t put dollar signs for the price, just include the number like “7.99.” If a book’s minimum price is free, enter “0” in the cell. If the book has no readers, put “0” in the cell. If the book’s author opted not to include how many readers their book has, put “NA” in the “readers” column for that book. So for me, since the first book is “PowerShell 101” after getting the data for the first book my spreadsheet will look like this. First row of data for project Continue this process, entering each book into a new row. Collect information on ten or twenty books. One book for every row. At the end you should have a data set that looks something like this. But yours will have different numbers and names in it. First complete data set 25.1.5 Checking your data Now that you’ve entered your data into the Google Sheet, we want to check for a few possible issues before moving on to make sure the data are formatted correctly. Double to make sure the following are true for the data in your spreadsheet: You have at least 11 rows with reader and minimum price information (one header row and at least 10 books included - if you have NAs anywhere, you’ll want more than 11 books) Your dollar amounts do NOT have dollar signs next to them. Your number of readers does not include any commas. If a book’s minimum price is FREE, you have put the number 0 in the cell, rather than “FREE” Checking your data This is great! You now have a question you want to answer and you have collected some data to answer that question. You are on your way to becoming a data scientist! 25.1.6 Publishing to the web Our plan is to use the data in this spreadsheet to answer our question about how the price of a bestselling book relates to how much the author is charging for that book. To do so in the next lesson, you will first have to publish the data to the web. This gives the software we’ll use in the next lesson permission to access your data. to make your sheet public, you’ll want to click on File at the top of the Google Sheet. From the drop-down menu that appears, you’ll want to click on “Publish to the web.” Publish to the web… In the window that pops up, you’ll want to click on “Publish” Publish A box will appear to confirm that you would like to publish this Google Sheet. Click “OK.” OK 25.1.7 Making the sheet public After publishing your data to the web, the last step is to make these data accessible to others who have the link.This can be done easily on a Google Sheet by clicking on “Share” in the top right-hand corner of the Google Sheet. Share A “Share with others” box will pop up. Click on “Get shareable link.” Share with others Your screen will update so that this document can now be viewed by anyone, as long as they have the link to the spreadsheet. Shareable Congrats!! You have successfully made this spreadsheet shareable and the link has been copied. You’ll be asked to paste this link in the quiz for this lesson, and we’ll use this spreadsheet link in the next project, so don’t close your Google Sheets tab quite yet. "],["getting-data-project.html", "Chapter 26 Getting Data Project", " Chapter 26 Getting Data Project In this project, we are going to try to answer this question with data: “How does the price of a bestselling book relate to how much the author is charging for that book?” To answer our question, we need some information on books on the Leanpub website. Be sure you have followed the steps described in the previous chapter and have your Googlesheets with Leanpub data prepared and ready. 26.0.1 Starting up this project Go to the DataTrail workspace. Return to your own DataTrail_Projects project. Click on it to start this assignment. For this project, go to the 02_Getting_Data folder. Click on the file leanpub_project.Rmd to open this file. 26.0.2 Your objectives! To complete this project you’ll need to do a few things within this file. Go through this notebook, reading along. Fill in empty or incomplete code chunks when prompted to do so. Run each code chunk as you come across it by clicking the tiny green triangles at the right of each chunk. You should see the code chunks print out various output when you do this. At the very top of this file, put your own name in the author: place. Currently it says \"DataTrail Team\". Be sure to put your name in quotes. In the Conclusions section, write up responses to each of these questions posed here. When you are satisfied with what you’ve written and added to this document you’ll need to save it. In the menu, go to File &gt; Save. Now the nb.html output resulting file will have your new output saved to it. Open up the resulting leanpub_project.nb.html file and click View in Web Browser. Does it look good to you? Did all the changes appear here as you expected. Upload your Rmd and your nb.html to your assignment folder (this is something that will be dependent on what your instructors have told you – or if you are taking this on your own, just collect these projects in one spot, preferably a Google Drive)! Pat yourself on the back for finishing this project! "],["cleaning-the-data.html", "Chapter 27 Cleaning the Data 27.1 Learning Objectives", " Chapter 27 Cleaning the Data 27.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Understand the difference between tidy and untidy dataset Clean a dataset so that it is ready for plotting Reshape a dataset from long to wide or vice versa Manipulate string data in R Manipulate factor data in R Manipulate dates in R Organize your project Use good practices for managing your project files "],["what-is-tidy-data.html", "Chapter 28 What is Tidy Data 28.1 Untidy Data", " Chapter 28 What is Tidy Data The idea of tidy data was formalized in 2014 in a paper written by a leader in the data science field, Hadley Wickham. The principles of tidy data, which are discussed below, provide a standard way of formatting a data set. A tidy dataset follows a number of rules relating to how rows, columns, and spreadsheets are matched up with observations, variables, and types. 28.0.1 Why Tidy Data? Tidy data-sets, by design, are easier to manipulate, model, and visualize. By starting with data that are already in a tidy format or by spending the time at the beginning of a project to get data into a tidy format, the remaining steps of your data science project will be easier. 28.0.2 Data Terminology We’ve previously discussed what the rows and columns in a spreadsheet are. Here, we’ll discuss what is meant by observations, variables, and types, all of which are used to explain the principles of tidy data. 28.0.2.1 Variables Variables in a spreadsheet are the different categories of data that will be collected. They are the different pieces of information that can be collected or measured on each person. Here, we see there are 7 different variables: ID, LastName, FirstName, Sex, City, State, and Occupation. The names for variables are put in the first row of the spreadsheet. Variables 28.0.2.2 Observations The measurements taken from a person for each variable are called observations. Observations for each individual are stored in a single row, with each observation being put in the appropriate column for each variable. Observations 28.0.2.3 Types Often, data are collected for the same individuals from multiple sources. For example, when you go to the doctor’s office, you fill out a survey about yourself. That would count as one type of data. The measurements a doctor collects at your visit, however, would be a different type of data. Types 28.0.3 Principles of Tidy Data Each variable you measure should be in one column. Principle #1 of Tidy Data Each different observation of that variable should be in a different row. Principle #2 of Tidy Data There should be one spreadsheet for each “type” of data. Principle #3 of Tidy Data If you have multiple spreadsheets, they should include a column in each spreadsheet (with the same column label!) that allows them to be joined or merged. Principle #4 of Tidy Data 28.0.4 Rules for Tidy Spreadsheets In addition to these four principles, there are a number of rules to follow when entering data into a spreadsheet, or when re-organizing untidy data that you have already been given for a project into a tidy format. They are rules that will help make data analysis and visualization easier down the road. They were formalized in a paper called “Data organization in spreadsheets”, written by two prominent data scientists, Karl Broman and Kara Woo. In this paper, in addition to ensuring that the data are tidy, they suggest following these guidelines when entering data into spreadsheets: Be consistent Choose good names for things Write dates as YYYY-MM-DD No empty cells Put just one thing in a cell Don’t use font color or highlighting as data Save the data as plain text files We’ll go through each of these to make sure we’re all clear on what a great tidy spreadsheet looks like. 28.0.4.1 Be consistent Being consistent in data entry and throughout an analysis is key. It minimizes confusion and makes analysis simpler. For example, here we see sex is coded as “female” or “male.” Those are the only two ways in which sex was entered into the data. This is an example of consistent data entry. You want to avoid sometimes coding a female’s sex as “female” and then entering it as “F” in other cases. Simply, you want to pick a way to code sex and stick to it. With regard to entering a person’s sex, we were talking about how to code observations for a specific variable; however, consistency also matters when you’re choosing how to name a variable. If you use the variable name “ID” in one spreadsheet, use the same variable name (“ID”) in the next spreadsheet. Do not change it to “id” (capitalization matters!) or “identifier” or anything else in the next spreadsheet. Be consistent! Consistency matters across every step of the analysis. Name your files in a consistent format. Always code dates in a consistent format (discussed further below). Avoid extra spaces in cells. If you’re careful about and consistent in data entry, it will be incredibly helpful when you get to analysis. Be Consistent! 28.0.4.2 Choose good names for things Choosing good variable names is important. Generally, avoid spaces in variable names and file names. You’ll see why this is important as we learn more about programming, but for now, know that “Doctor Visit 1” is not a good file name. “doctor_visit_v1” is much better. Stick to using underscores instead of spaces or any other symbol when possible. The same thing goes for variable names. “FirstName” is a good variable name while “First Name” with a space in the middle of it is not. Additionally, make sure that file and variable names are as short as possible while still being meaningful. “F1” is short, but it doesn’t really tell you anything about what is in that file. “doctor_visit_v1” is a more meaningful file name. We know now that this spreadsheet contains information about a doctor’s visit. ‘v1’ specifies version 1 allowing for updates to this file later which would create a new file “doctor_visit_v2.” Choose good names 28.0.4.3 Write dates as YYYY-MM-DD When entering dates, there is a global ‘ISO 8601’ standard. Dates should be encoded YYYY-MM-DD. For example if you want to specify that a measurement was taken on February 27th, 2018, you would type 2018-02-27. YYYY refers to the year, 2018. MM refers to the month of February, 02. And DD refers to the day of the month, 27. This standard is used for dates for two main reason. First, it avoids confusion when sharing data across different countries, where date conventions can differ. By all using ISO 8601 standard conventions, there is less room for error in interpretation of dates. Secondly, spreadsheet software often mishandles dates and assumes that non-date information are actually dates and vice versa. By encoding dates as YYYY-MM-DD, this confusion is minimized. YYYY-MM-DD 28.0.4.4 No empty cells Simply, fill in every cell. If the data is unknown for that cell, put ‘NA.’ Without information in each cell, the analyst is often left guessing. In the spreadsheets below, on the left, is the analyst to assume that the empty cells should use the date from the cell above? Or are we to assume that the date for that measurement is unknown? Fill in the date if it is known or type ‘NA’ if it is not. That will clear up the need for any guessing on behalf of the analyst. On the spreadsheet to the right, the first two rows have a lot of empty cells. This is problematic for the analysis. This spreadsheet does not follow the rules for tidy data. There is not a single variable per column with a single entry per row. These data would have to be reformatted before they could be used in analysis. No empty cells 28.0.4.5 Put just one thing in a cell Sometimes people are tempted to include a number and a unit in a single cell. For weight, someone may want to put ‘165 lbs’ in that cell. Avoid this temptation! Keep numbers and units separate. In this case, put one piece of information in the cell (the person’s weight) and either put the unit in a separate column, or better yet, make the variable name weight_lbs. That clears everything up for the analyst and avoids a number and a unit from both being put in a single cell. As analysts, we prefer weight information to be in number form if we want to make calculations or figures. This is facilitated by the first column called “Weight_lbs” because it will be read into R as a numeric object. The second column called “Weight”, however, will be read into R as a character object because of the “lbs”, which makes our desired tasks more difficult. One thing per cell 28.0.4.6 Don’t use font color or highlighting as data Avoid the temptation to highlight particular cells with a color to specify something about the data. Instead, add another column to convey that information. In the example below, 1.1 looks like an incorrect value for an individual’s glucose measure. Instead of highlighting the value in red, create a new variable. Here, on the right, this column has been named ‘outlier.’ Including ‘TRUE’ for this individual suggests that this individual may be an outlier to the data analyst. Doing it in this way ensures that this information will not be lost. Using font color or highlighting however can easily be lost in data processing, as you will see in future lessons. No highlighting or font color 28.0.4.7 Save the data as plain text files The following lessons will go into detail about which file formats are ideal for saving data, such as text files (.txt) and comma-delimited files (.csv). These file formats can easily be opened and will never require special software, ensuring that they will be usable no matter what computer or analyst is looking at the data. 28.0.4.8 Tidy Data Summary The data entry guidelines discussed here and a few additional rules have been summarized below and are available online for reference. Naming Guidelines Most importantly, however, remember that tidy data are rectangular data. The data should be a rectangle with each variable in a separate column and each entry in a different row. All cells should contain some text, so that the spreadsheet looks like a rectangle with something in every cell. Tidy Data = rectangular data 28.1 Untidy Data We’ve just spent a while discussing the principles of tidy data as well as a number of guidelines on how to correctly enter data into spreadsheets. At this point, you may think “I got it. Make my data rectangular! I’ll do it.” But, the reality is that most data are untidy. If you are not the one entering the data but are instead handed the data from someone else to do a project, more often than not, those data will be untidy. Untidy data are often referred to simply as messy data. The following common problems seen in messy data sets again come from Hadley Wickham’s paper on tidy data. After briefly reviewing what each common problem is, we will then take a look at a few messy data sets. We’ll finally touch on the concepts of tidying untidy data, but we won’t actually do any practice yet. That’s coming soon! 28.1.1 Common problems with messy data sets Column headers are values but should be variable names. A single column has multiple variables. Variables have been entered in both rows and columns. Multiple “types” of data are in the same spreadsheet. A single observation is stored across multiple spreadsheets. 28.1.2 Examples of untidy data To see some of these messy datasets, let’s explore three different sources of messy data. 28.1.2.1 Examples from Data Organization in Spreadsheets In each of these examples, we see the principles of tidy data being broken. Each variable is not a unique column. There are empty cells all over the place. The data are not rectangular. Data formatted in these messy ways are likely to cause problems during analysis. Examples from Data Organization in Spreadsheets For a specific example, Miles McBain, a data scientist from Brisbane, Australia set out to analyze Australian survey data on Same Sex marriage. Before he could do the analysis, however, he had a lot of tidying to do. He annotated all the ways in which the data were untidy, including the use of commas in numerical data entry, blank cells, junk at the top of the spreadsheet, and merged cells. All of these would have stopped him from being able to analyze the data had he not taken the time to first tidy the data. Luckily, he wrote a Medium piece including all the steps he took to tidy the data. Miles McBain’s’ tidying of Australian Same Sex Marriage Postal Survey Data Inspired by Miles’ work, Sharla Gelfand decided to tackle a messy data set from Toronto’s open data. She similarly outlined all the ways in which the data were messy including, names and address across multiple cells in the spreadsheet, merged column headings, and lots of blank cells. She has also included the details of how she cleaned these data in a blog post. While the details of the code may not make sense yet, it will shortly as you get more comfortable with the programming language, R. Sharla Gelfand’s tidying of Toronto’s open data 28.1.3 Tidying untidy data There are a number of actions you can take on a dataset to tidy the data depending on the problem. These include: filtering, transforming, modifying variables, aggregating the data, and sorting the order of the observations. There are functions to accomplish each of these actions in R. While we’ll get to the details of the code in a few lessons, it’s important at this point to be able to identify untidy data and to determine what needs to be done in order to get those data into a tidy format. Specifically, we will focus in here on a single messy data set. This is dataset D from the ‘Data Organization in Spreadsheets’ example of messy data provided above. We note the blank cells and that the data are not rectangular. Messy data set To address this, these data can be split into two different spreadsheets, one for each type of data. Spreadsheet A included information about each sample. Spreadsheet B includes measurements for each sample over time. Note that both spreadsheets have an ‘id’ column so that the data can be merged if necessary during analysis. The ‘note’ column does have some missing data. Filling in these blank cells with ‘NA’ would fully tidy these data. We note that sometimes a single spreadsheet becomes two spreadsheets during the tidying process. This is OK as long as there is a consistent variable name that links the two spreadsheets! Tidy version of the messy data set 28.1.4 Additional resources Hadley Wickham’s paper on Tidy Data Data Organization in Spreadsheets How to Share Data for Collaboration Data Carpentry’s Data Organization in Spreadsheets Karl Broman’s Data Organization blog post "],["reshaping-data.html", "Chapter 29 Reshaping Data 29.1 Transposing data", " Chapter 29 Reshaping Data 29.0.1 Data Formats Tidy data generally exist in two forms: wide data and long data. Both types of data are used and needed in data analysis, and fortunately, there are tools that can take you from wide-to-long and from long-to-wide. This makes it easy to work with any tidy data set. We’ll discuss the basics of what wide and long data are and how to go back and forth between the two in R. Getting data into the right format will be crucial later when summarizing data and visualizing it. To help you visualize data as its being reshaped, we recommend you play around with the TidyDataTutor. 29.0.1.1 Wide Data Wide data has a column for each variable and a row for each observation. Data are often entered and stored in this manner. This is because wide data are often easy to understand at a glance. For example, this is a wide data set: Wide dataset This is a dataset we’ve looked at in a previous lesson. As discussed previously, it’s a rectangular and tidy dataset. Now, we can also state that it is a wide dataset. Here you can clearly see what measurements were taken for each individual and can get a sense of how many individuals are contained in the dataset. Specifically, each individual is in a different row with each variable in a different column. At a glance we can quickly see that we have information about four different people and that each person was measured in four different ways. 29.0.1.2 Long Data Long data, on the other hand, has one column indicating the type of variable contained in that row and then a separate row for the value for that variable. Each row contains a single observation for a single variable. Below is an example of a long data set: Long dataset This long dataset includes the exact same information as the previous wide dataset; it is just stored differently. It’s harder to see visually how many different measurements were taken and on how many different people, but the same information is there. While long data formats are less readable than wide data at a glance, they are a lot easier to work with during analysis. Most of the tools we’ll be working with use long data. Thus, to go from how data are often stored (wide) to working with the data during analysis (long), we’ll need to understand what tools are needed to do this and how to work with them. 29.0.2 R Packages Converting your data from wide-to-long or from long-to-wide data formats is referred to as reshaping your data and you can do this with the tidyr package. As with most helpful packages in R, there is more functionality than what is discussed here, so feel free to explore the additional resources at the bottom to learn even more. Reshaping data For these examples, we’ll work with the relig_income dataset available in tidyr. The data in this dataset includes the income ranges and religions from the Pew religion and income survey. We can see the first few lines of this dataset using the following code: library(tidyr) head(relig_income) ## # A tibble: 6 x 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 ## 2 Atheist 12 27 37 52 35 70 73 ## 3 Buddhist 27 21 30 34 33 58 62 ## 4 Catholic 418 617 732 670 638 1116 949 ## 5 Don’t k… 15 14 15 11 10 35 21 ## 6 Evangel… 575 869 1064 982 881 1486 949 ## # … with 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;, `Don&#39;t ## # know/refused` &lt;dbl&gt; Again, wide data are easy to decipher at a glance. We can see that we have different income levels but in reality, many of those columns contain the same variable (income) so these data are not tidy. 29.0.2.1 tidyr Within tidyr, there are two functions to help you reshape your data. pivot_longer(): go from wide data to long data pivot_wider(): go from long data to wide data To get started, you’ll need to be sure that the tidyr package is installed and loaded into your RStudio session. install.packages(&quot;tidyr&quot;) library(tidyr) 29.0.2.1.1 pivot_longer() As data are often stored in wide formats, you’ll likely use pivot_longer() a lot more frequently than you’ll use pivot_wider(). This will allow you to get the data into a long format that will be easy to use for analysis. In tidyr, gather() will take the relig_income dataset from wide to long, putting each column name into the first column and each corresponding value into the second column. Here, the first column will be called income. The second column will still be count. The first argument in pivot_longer() tells which columns you’d like to pivot. In this case, we want to pivot all columns except the religion column Let’s take a look at this dataset to start: head(relig_income) ## # A tibble: 6 x 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 ## 2 Atheist 12 27 37 52 35 70 73 ## 3 Buddhist 27 21 30 34 33 58 62 ## 4 Catholic 418 617 732 670 638 1116 949 ## 5 Don’t k… 15 14 15 11 10 35 21 ## 6 Evangel… 575 869 1064 982 881 1486 949 ## # … with 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;, `Don&#39;t ## # know/refused` &lt;dbl&gt; Now let’s pivot_longer and take a look again: ## use pivot_longer() to reshape from wide to long long_df &lt;- relig_income %&gt;% pivot_longer(!religion, names_to = &quot;income&quot;, values_to = &quot;count&quot;) ## take a look at first few rows of long data head(long_df) ## # A tibble: 6 x 3 ## religion income count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Agnostic &lt;$10k 27 ## 2 Agnostic $10-20k 34 ## 3 Agnostic $20-30k 60 ## 4 Agnostic $30-40k 81 ## 5 Agnostic $40-50k 76 ## 6 Agnostic $50-75k 137 gather dataset We can change what columns we want to include a different column select helpers: See here for all about select helpers. ## use pivot_longer() to reshape from wide to long relig_income %&gt;% pivot_longer(contains(&quot;$&quot;), names_to = &quot;income&quot;, values_to = &quot;count&quot;) ## # A tibble: 144 x 5 ## religion `&gt;150k` `Don&#39;t know/refused` income count ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Agnostic 84 96 &lt;$10k 27 ## 2 Agnostic 84 96 $10-20k 34 ## 3 Agnostic 84 96 $20-30k 60 ## 4 Agnostic 84 96 $30-40k 81 ## 5 Agnostic 84 96 $40-50k 76 ## 6 Agnostic 84 96 $50-75k 137 ## 7 Agnostic 84 96 $75-100k 122 ## 8 Agnostic 84 96 $100-150k 109 ## 9 Atheist 74 76 &lt;$10k 12 ## 10 Atheist 74 76 $10-20k 27 ## # … with 134 more rows We can change the names_to and values_to arguments if we want the resulting column names to be different. 29.0.2.1.2 pivot_wider() To return your long data back to its original form, you can use pivot_wider(). Here you specify two columns: the column that contains the names of what your wide data columns should be (names_from) and the column that contains the values that should go in these columns (values_from). The data frame resulting from pivot_wider() will have the original information back in the wide format (again, the columns will be in a different order). But, we’ll discuss how to rearrange data in the next lesson! ## use pivot_wider() to reshape from wide to long wider_data &lt;- pivot_wider(long_df, names_from = &quot;income&quot;, values_from = &quot;count&quot;) ## take a look at the wider data head(wider_data) 29.1 Transposing data One more transformation you may need to do, has a function in base R to do it. You may find you need your rows to be columns and your columns to be rows. In other words, you need to transpose your data table. To do this, you can use the t() function. But! Note that although the t() function can be given a data frame, it will return a matrix. This means if you want your data frame to stay a data frame you need to follow your t() function with a as.data.frame(). This might look like this: # We need to load this so we can use %&gt;% library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract iris_transposed &lt;- iris %&gt;% t() %&gt;% as.data.frame() Our original iris data frame has 150 rows and 4 columns. dim(iris) ## [1] 150 5 Whereas our transposed iris data frame has 4 rows and 150 columns. dim(iris_transposed) ## [1] 5 150 The information in both iris and iris_transposed is the same, it’s just in a different shape. 29.1.1 Additional Resources TidyDataTutor tidyr, part of the tidyverse and developed by Hadley Wickham and Lionel Henry tidyr tutorial by Hadley Wickham tidyr cheatsheet "],["how-to-tidy-data.html", "Chapter 30 How to Tidy Data", " Chapter 30 How to Tidy Data So far we’ve discussed what tidy and untidy data are. We’ve (hopefully) convinced you that tidy data are the right type of data to work with. And, more than that, hopefully we’ve explained that data are not always the tidiest when they come to you at the start of a project. An incredibly important skill of a data scientist is to be able to take data from an untidy format and get it into a tidy format. We’ve started to discuss how to do this in the last lesson where we learned to reshape data. In this lesson, we’ll discuss a number of other ways in which data can be tidied and the necessary tools to tidy data. These skills are often referred to as data wrangling. They are skills that allow you to wrangle data from the format they’re currently in into the format you actually want them in. As this is an incredibly important topic, this will be a long lesson covering a number of packages and topics. Take your time working through it and be sure to understand all of the examples! data wrangling example 30.0.1 dplyr Within R, there is a package specifically designed for helping you wrangle data. This package is called dplyr and will allow you to easily accomplish many of the data wrangling tasks necessary. In this lesson, we will cover a number of functions that will help you wrangle data using dplyr: %&gt;% - pipe operator for chaining a sequence of operations glimpse() - get an overview of what’s included in dataset filter() - filter rows select() - select, rename, and reorder columns rename() - rename columns arrange() - reorder rows mutate() - create a new column group_by() - group variables summarize() - summarize information within a dataset If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;dplyr&#39;) library(dplyr) 30.0.2 tidyr We will also return to the tidyr package. The same package that we used to reshape our data will be helpful when tidying data. The main functions we’ll cover from tidyr are: unite() - combine contents of two or more columns into a single column separate() - separate contents of a column into two or more columns If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;tidyr&#39;) library(tidyr) 30.0.3 janitor The third package we’ll include here is the janitor package. This package provides tools for cleaning messy data. The main functions we’ll cover from janitor are: clean_names() - clean names of a data frame tabyl() - get a helpful summary of a variable If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;janitor&#39;) library(janitor) 30.0.4 skimr The final package we’ll discuss here is the skimr package. This package provides a quick way to summarize a data frame. We’ll discuss its most useful function here: skim() - summarize a data frame If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;skimr&#39;) library(skimr) 30.0.5 The Pipe Operator Before we get into the important functions within dplyr, it will be very useful to discuss what is known as the pipe operator. The pipe operator looks like this in R: %&gt;%. Whenever you see the pipe %&gt;%, think of the word “then”, so if you saw the sentence “I went to the the store and %&gt;% I went back to my house,” you would read this as I went to the store and then I went back to my house. The pipe tells you to do one thing and then do another. Generally, the pipe operator allows you to string a number of different functions together in a particular order. If you wanted to take data frame A and carry out function B on it in R, you could depict this with an arrow pointing from A to B: A –&gt; B Here you are saying, “Take A and then feed it into function B.” In R syntax, from what you’ve seen so far, what is depicted by the arrow above would be carried out by calling the function B on the data frame object A: B(A) Alternatively, you could use the pipe operator (%&gt;%): A %&gt;% B However, often you are not performing just one action on a data frame, but rather you are looking to carry out multiple functions. We can again depict this with an arrow diagram. A –&gt; B –&gt; C –&gt; D Here you are saying that you want to take data frame A and carry out function B, then you want to take the output from that and then carry out function C. Subsequently you want to take the output of that and then carry out function D. In R syntax, we would first apply function B to data frame A, then apply function C to this output, then apply function D to this output. This results in the following syntax that is hard to read because multiple calls to functions are nested within each other: D(C(B(A))) Alternatively, you could use the pipe operator. Each time you want take the output of one function and carry out something new on that output, you will use the pipe operator: A %&gt;% B %&gt;% C %&gt;% D Below we’ll use this pipe operator a lot. Essentially, it takes output from the left hand side and feeds it into a function on the right hand side. You’ll get a better understanding of how it works as you run the code below. But, when in doubt remember that the pipe operator should be read as then. 30.0.6 Filtering Data When working with a large dataset, you’re often interested in only working with a portion of the data at any one time. For example, if you had data on people from ages 0 to 100 years old, but you wanted to ask a question that only pertained to children, you would likely want to only work with data from those individuals who were less than 18 years old. To do this, you would want to filter your dataset to only include data from these select individuals. Filtering can be done by row or by column. We’ll discuss the syntax in R for doing both. Please note that the examples in this lesson and the organization for this lesson were adapted from Suzan Baert’s wonderful dplyr tutorials. Links to the all four tutorials can be found in the “Additional Resources” section at the bottom of this lesson. For the examples below, we’ll be using a dataset from the ggplot2 package called msleep. (You’ll learn more about this package in a later chapter on data visualization.) This dataset includes sleep times and weights from a number of different mammals. It has 83 rows, with each row including information about a different type of animal, and 11 variables. As each row is a different animal and each column includes information about that animal, this is a wide dataset. To get an idea of what variables are included in this data frame, you can use glimpse(). This function summarizes how many rows there are (Observations) and how many columns there are (Variables). Additionally, it gives you a glimpse into the type of data contained in each column. Specifically, in this data set, we know that the first column is name and that it contains a character vector (chr) and that the first three entires are “Cheetah”, “Owl monkey”, and “Mountain beaver.” It works similarly to the summary() function covered in an earlier chapter. ## install packages if you haven&#39;t already install.packages(&#39;ggplot2&#39;) ## load package library(ggplot2) ## take a look at the data glimpse(msleep) Glimpse of msleep dataset 30.0.6.1 Filtering Rows If you were only interested in learning more about the sleep times of “Primates,” we could filter this dataset to include only data about those mammals that are also Primates. As we can see from glimpse(), this information is contained within the order variable. So to do this within R, we use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;) Note that we are using the equality == comparison operator that you learned about in the previous course. Also note that we have used the pipe operator to feed the msleep data frame into the filter() function. This is shorthand for: filter(msleep, order == &quot;Primates&quot;) Filtered to only include Primates Here, we have a smaller dataset of only 12 mammals (as opposed to the original 83) and we can see that the order variable column only includes “Primates.” But, what if we were only interested in Primates who sleep more than 10 hours total per night? This information is in the sleep_total column. Fortunately, filter() also works on numeric variables. To accomplish this, you would use the following syntax, separating the multiple filters you want to apply with a comma: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) Note that we have used the “greater than” comparison operator with sleep_total. Now, we have a dataset focused in on only 5 mammals, all of which are primates who sleep for more than 10 hours a night total. Numerically filtered dataset We can obtain the same result with the AND &amp; logical operator instead of separating filtering conditions with a comma: msleep %&gt;% filter(order == &quot;Primates&quot; &amp; sleep_total &gt; 10) Note that the number of columns hasn’t changed. All 11 variables are still shown in columns because the function filter() filters on rows, not columns. 30.0.6.2 Selecting Columns While filter() operates on rows, it is possible to filter your dataset to only include the columns you’re interested in. To select columns so that your dataset only includes variables you’re interested in, you will use select(). Let’s start with the code we just wrote to only include primates who sleep a lot. What if we only want to include the first column (the name of the mammal) and the sleep information (included in the columns sleep_total, sleep_rem, and sleep_cycle)? We would do this by starting with the code we just used, adding another pipe, and using the function select(). Within select, we specify which columns we want in our output. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_total, sleep_rem, sleep_cycle) Data with selected columns Now, using select() we see that we still have the five rows we filtered to before, but we only have the four columns specified using select(). Here you can hopefully see the power of the pipe operator to chain together several commands in a row. Without the pipe operator, the full command would look like this: select(filter(msleep, order == &quot;Primates&quot;, sleep_total &gt; 10), name, sleep_total, sleep_rem, sleep_cycle) 30.0.6.3 Renaming Columns select() can also be used to rename columns. To do so, you use the syntax: new_column_name = old_column_name within select. For example, to select the same columns and rename them total, rem and cycle, you would use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, total=sleep_total, rem=sleep_rem, cycle=sleep_cycle) Data with renamed columns names with select() It’s important to keep in mind that when using select() to rename columns, only the specified columns will be included and renamed in the output. If you, instead, want to change the names of a few columns but return all columns in your output, you’ll want to use rename(). For example, the following, returns a data frame with all 11 columns, where the column names for three columns specified within rename() function have been renamed. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% rename(total=sleep_total, rem=sleep_rem, cycle=sleep_cycle) Data with renamed columns names using rename() 30.0.7 Reordering In addition to filtering rows and columns, often, you’ll want the data arranged in a particular order. It may order the columns in a logical way, or it could be to sort the data so that the data are sorted by value, with those having the smallest value in the first row and the largest value in the last row. All of this can be achieved with a few simple functions. 30.0.7.1 Reordering Columns The select() function is powerful. Not only will it filter and rename columns, but it can also be used to reorder your columns. Using our example from above, if you wanted sleep_rem to be the first sleep column and sleep_total to be the last column, all you have to do is reorder them within select(). The output from select() would then be reordered to match the order specified within select(). msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) Here we see that sleep_rem name is displayed first followed by sleep_rem, sleep_cycle, and sleep_total, just as it was specified within select(). Data with reordered columns names 30.0.7.2 Reordering Rows Rows can also be reordered. To reorder a variable in ascending order (from smallest to largest), you’ll want to use arrange(). Continuing on from our example above, to now sort our rows by the amount of total sleep each mammal gets, we would use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(sleep_total) Data arranged by total sleep in ascending order While arrange sorts variables in ascending order, it’s also possible to sort in descending (largest to smallest) order. To do this you just use desc() with the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(desc(sleep_total)) By putting sleep_total within desc(), arrange() will now sort your data from the primates with the longest total sleep to the shortest. Data arranged by total sleep in descending order arrange() can also be used to order non-numeric variables. For example, arrange() will sort character vectors alphabetically. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name) Data arranged alphabetically by name If you would like to reorder rows based on information in multiple columns, you can specify them separated by commas. This is useful if you have repeated labels in one column and want to sort within a category based on information in another column. In the example here, if there were repeated primates, this would sort the repeats based on their total sleep. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name, sleep_total) 30.0.8 Creating new columns You will often find when working with data that you need an additional column. For example, if you had two datasets you wanted to combine, you may want to make a new column in each dataset called dataset. In one dataset you may put datasetA in each row. In the second dataset, you could put datasetB. This way, once you combined the data, you would be able to keep track of which dataset each row came from originally. More often, however, you’ll likely want to create a new column that calculates a new variable based on information in a column you already have. For example, in our mammal sleep dataset, sleep_total is in hours. What if you wanted to have that information in minutes? You could create a new column with this very information! The function mutate() was made for all of these new-column-creating situations. This function has a lot of capabilities. We’ll cover the basics here. Returning to our msleep dataset, after filtering and re-ordering, we can create a new column with mutate(). Within mutate(), we will calculate the number of minutes each mammal sleeps by multiplying the number of hours each animal sleeps by 60 minutes. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name) %&gt;% mutate(sleep_total_min = sleep_total * 60) Mutate to add new column to data 30.0.9 Separating Columns Sometimes multiple pieces of information are merged within a single column even though it would be more useful during analysis to have those pieces of information in separate columns. To demonstrate, we’ll now move from the msleep dataset to talking about another dataset that includes information about conservation abbreviations in a single column. To read this file into R, we’ll use the httr package, which will be discussed in detail in a future lesson. For now, however, know that we’re using this to read in a file from the Internet using the code below. ## if not already installed, you&#39;ll have to run the following line of code ## install.packages(&#39;httr&#39;) ## install.packages(&#39;readr&#39;) ## load the libraries library(httr) library(readr) ## download file GET(&quot;https://raw.githubusercontent.com/suzanbaert/Dplyr_Tutorials/master/conservation_explanation.csv&quot;, write_disk(tf &lt;- tempfile(fileext = &quot;.csv&quot;))) conservation &lt;- read_csv(tf) ## take a look at this file head(conservation) Conservation data set In this dataset, we see that there is a single column that includes both the abbreviation for the conservation term as well as what that abbreviation means. Recall that this violates one of the tidy data principles covered in the first lesson: Put just one thing in a cell. To work with these data, you could imagine that you may want these two pieces of information (the abbreviation and the description) in two different columns. To accomplish this in R, you’ll want to use separate() from tidyr. The separate() function requires the name of the existing column that you want to separate (conservation abbreviation), the desired column names of the resulting separated columns (into = c(\"abbreviation\", \"description\")), and the characters that currently separate the pieces of information (sep = \" = \"). We have to put conservation abbreviation in back ticks in the code below because the column name contains a space. Without the back ticks, R would think that conservation and abbreviation were two separate things. This is another violation of tidy data! Variable names should have underscores, not spaces! conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) The output of this code shows that we now have two separate columns with the information in the original column separated out into abbreviation and description. Output of separate() 30.0.10 Merging Columns The opposite of separate() is unite(). So, if you have information in two or more different columns but wish it were in one single column, you’ll want to use unite(). Using the code forming the two separate columns above, we can then add on an extra line of unite() code to re-join these separate columns, returning what we started with. conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) %&gt;% unite(united_col, abbreviation, description, sep = &quot; = &quot;) Output of unite() 30.0.11 Cleaning up column names While maybe not quite as important as some of the other functions mentioned in this lesson, a function that will likely prove very helpful as you start analyzing lots of different datasets is clean_names() from the janitor package. This function takes the existing column names of your dataset, converts them all to lowercase letters and numbers, and separates all words using the underscore character. For example, there is a space in the column name for conservation. clean_names() will convert conservation abbreviation to conservation_abbreviation. These cleaned up column names are a lot easier to work with when you have large datasets. conservation %&gt;% clean_names() clean_names() output 30.0.12 Combining data across data frames There is often information stored in two separate data frames that you’ll want in a single data frame. There are many different ways to join separate data frames. They are discussed in more detail in this tutorial from Jenny Bryan. Here, we’ll demonstrate how the left_join() function works, as this is used frequently. Let’s try to combine the information from the two different datasets we’ve used in this lesson. We have msleep and conservation. msleep contains a column called conservation. This column includes lowercase abbreviations that overlap with the uppercase abbreviations in the abbreviation column in the conservation dataset. To handle the fact that in one dataset the abbreviations are lowercase and the other they are uppercase, we’ll use mutate() to take all the lowercase abbreviations to uppercase abbreviations using the function toupper(). We’ll then use left_join() which takes all of the rows in the first dataset mentioned (msleep, below) and incorporates information from the second dataset mentioned (conserve, below), when information in the second dataset is available. The by = argument states what columns to join by in the first (“conservation”) and second (“abbreviation”) datasets. This join adds the description column from the conserve dataset onto the original dataset (msleep). Note that if there is no information in the second dataset that matches with the information in the first dataset, left_join() will add NA. Specifically, for rows where conservation is “DOMESTICATED” below, the description column will have NA because “DOMESTICATED”” is not an abbreviation in the conserve dataset. ## take conservation dataset and separate information ## into two columns ## call that new object `conserve` conserve &lt;- conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) ## now lets join the two datasets together msleep %&gt;% mutate(conservation = toupper(conservation)) %&gt;% left_join(conserve, by = c(&quot;conservation&quot; = &quot;abbreviation&quot;)) Data resulting from left_join It’s important to note that there are many other ways to join data, which are covered in more detail on this dplyr join cheatsheet from Jenny Bryan. For now, it’s important to know that joining datasets is done easily in R using tools in dplyr. As you join data frames in your own work, it’s a good idea to refer back to this cheatsheet for assistance. 30.0.13 Grouping Data Often, data scientists will want to summarize information in their dataset. You may want to know how many people are in a dataset. However, more often, you’ll want to know how many people there are within a group in your dataset. For example, you may want to know how many males and how many females there are. To do this, grouping your data is necessary. Rather than looking at the total number of individuals, to accomplish this, you first have to group the data by the gender of the individuals. Then, you count within those groups. Grouping by variables within dplyr is straightforward. 30.0.13.1 group_by() There is an incredibly helpful function within dplyr called group_by(). group_by() groups a dataset by one or more variables. On its own, it does not appear to change the dataset very much. The difference between the two outputs below is subtle: msleep msleep %&gt;% group_by(order) group_by() output In fact, the only aspect of the output that is different is that the number of different orders is now printed on your screen. However, in the next section, you’ll see that the output from any further functions you carry out at this point will differ between the two datasets. 30.0.14 Summarizing Data Throughout data cleaning and analysis it will be important to summarize information in your dataset. This may be for a formal report or for checking the results of a data tidying operation. 30.0.14.1 summarize() Continuing on from the previous examples, if you wanted to figure out how many samples are present in your dataset, you could use the summarize() function. msleep %&gt;% select(order) %&gt;% summarize(N=n()) This provides a summary of the data with the new column name we specified above (N) and the number of samples in the dataset. Note that we could also obtain the same information by directly obtaining the number of rows in the data frame with nrow(msleep). Summarize with n() However, if you wanted to count how many of each different order of mammal you had. You would first group_by(order) and then use summarize(). This will summarize within group. msleep %&gt;% group_by(order) %&gt;% select(order) %&gt;% summarize(N=n()) The output from this, like above, includes the column name we specified in summarize (N). However, it includes the number of samples in the group_by variable we specified (order). group_by() and summarize with n() There are other ways in which the data can be summarized using summarize(). In addition to using n() to count the number of samples within a group, you can also summarize using other helpful functions within R, such as mean(), median(), min(), and max(). For example, if we wanted to calculate the average (mean) total sleep each order of mammal got, we could use the following syntax: msleep %&gt;% group_by(order) %&gt;% select(order, sleep_total) %&gt;% summarize(N=n(), mean_sleep=mean(sleep_total)) summarize using mean() 30.0.14.2 tabyl() In addition to using summarize() from dplyr, the tabyl() function from the janitor package can be incredibly helpful for summarizing categorical variables quickly and discerning the output at a glance. Again returning to our msleep dataset, if we wanted to get a summary of how many samples are in each order category and what percent of the data fall into each category we could call tabyl on that variable. For example, if we use the following syntax, we easily get a quick snapshot of this variable. msleep %&gt;% tabyl(order) summarize using tabyl() from janitor Note, that tabyl assumes categorical variables. If you want to summarize numeric variables summary() works well. For example, this code will summarize the values in msleep$awake for you. summary(msleep$awake) summarize numeric variables 30.0.14.3 skim() When you would rather get a snapshot of the entire dataset, rather than just one variable, the skim() function from the skimr package can be very helpful. The output from skim() breaks the data up by variable type. For example, the msleep data set is broken up into character and numeric variable types. The data are then summarized in a meaningful way for each. This function provides a lot of information about the entire data set. So, when you want a summarize a dataset and quickly get a sense of your data, skim() is a great option! skim(msleep) summarize entire dataset using skim() from skimr 30.0.15 Conclusion We have gone through a number of ways to work with data in this lesson. Mastering the skills in this lesson will provide you with a number of critical data science skills. Thus, running the examples in this lesson and practicing on your own with other data sets will be essential to succeeding as a data scientist. 30.0.16 Additional Resources dplyr, part of the tidyverse janitor, from Sam Firke dplyr tutorials by Suzan Baert Part 1 Part 2 Part 3 Part 4 janitor tutorial by dplyr join cheatsheet by Jenny Bryan Note: a lot of the examples in this lesson were modified from the dplyr tutorials by Suzan Baert. To get an even deeper understanding of how to tidy data using dplyr, take a look at all of her dplyr tutorials. "],["joining-data.html", "Chapter 31 Joining Data 31.1 dplyr join family of functions", " Chapter 31 Joining Data Often times, you may find yourself with two datasets that contain overlapping sets of information. In many cases, it may make the most sense to combine multiple datasets into one datasets for easier handling and reading. In this chapter, we will discuss the different methods for combining multiple datasets. 31.1 dplyr join family of functions There are different kinds of joins - let’s discuss what they do. Let’s say you have to data frames. One is called df_a and the other, df_b. We can create these data frames using this code: df_a &lt;- data.frame(sample = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), number = c(2, 4, 4.4, 3.1)) df_b &lt;- data.frame(sample = c(&quot;A&quot;, &quot;B&quot;, &quot;E&quot;, &quot;F&quot;), color = c(&quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;green&quot;)) Let’s print these toy tables out to see what they look like. Here’s df_a. df_a ## sample number ## 1 A 2.0 ## 2 B 4.0 ## 3 C 4.4 ## 4 D 3.1 And here’s df_b df_b ## sample color ## 1 A blue ## 2 B blue ## 3 E red ## 4 F green Notice that both table contain information about samples, A and B but each also contain information that the other table doesn’t have. Join functions can be great for situation like these to make one bigger table that contains the information you want to use. join family of functions include these (which we will go over in this chapter): inner_join(): includes only rows that have matches in df_a and df_b. left_join(): includes all rows in df_a but not any of df_b that doesn’t have a match to df_a. right_join(): includes all rows in df_b but not any of df_a that doesn’t have a match to df_b. full_join(): includes all rows in of df_a and df_b, regardless if they have matches in the other data frame or not. 31.1.0.1 inner_join() First, let’s try an inner_join(). This will only keep the rowss common to both datasets. Note that we can try to have the function guess what column we would like to match the rows by, OR we should probably use the argument by to tell the function which column should be used as a key. In our example above, we want to use the sample column to match rows by. Aka the sample column is what has the IDs that indicate what rows are related to which in the two data data frames. # Need to load in these functions first before we can use them library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union inner_join(df_a, df_b, by = &quot;sample&quot;) ## sample number color ## 1 A 2 blue ## 2 B 4 blue Notice that now we only have the rows A and B because those were in both datasets. This is how inner_join() works. If we don’t like dropping all that other data, we might want to use a different join function. 31.1.0.2 left_join() Let’s say we mostly care about the data in df_a and want to keep all that data, but want to join data from df_b only if there are matches. This is a great case for a left_join. Let’s try it: left_join(df_a, df_b, by = &quot;sample&quot;) ## sample number color ## 1 A 2.0 blue ## 2 B 4.0 blue ## 3 C 4.4 &lt;NA&gt; ## 4 D 3.1 &lt;NA&gt; See that now we have all the rows of df_a but an added column, color, from df_b. You’ll see that the way the join deals with this is that it puts NAs for the rows it doesn’t have information for color – these will be the rows that are in df_b but not in df_a. 31.1.0.3 right_join() What if instead we mostly care about df_b’s data but are wanting to add on df_a’s data ONLY if it matches to df_b? This is where we can do a right_join() right_join(df_a, df_b, by = &quot;sample&quot;) ## sample number color ## 1 A 2 blue ## 2 B 4 blue ## 3 E NA red ## 4 F NA green Note that the above is also equivalent to a left_join() if we switch the order that we are providing the data frames. Aka if we list df_b first and df_a second. all.equal( right_join(df_a, df_b, by = &quot;sample&quot;), # A left join, where we have df_b listed first left_join(df_b, df_a, by = &quot;sample&quot;) ) ## [1] &quot;Names: 2 string mismatches&quot; ## [2] &quot;Component 2: Modes: numeric, character&quot; ## [3] &quot;Component 2: target is numeric, current is character&quot; ## [4] &quot;Component 3: Modes: character, numeric&quot; ## [5] &quot;Component 3: target is character, current is numeric&quot; In summary, the left or right in the function name just refers to which data frame is listed first (on the left) or second (on the right). 31.1.0.4 full_join() In the situation where we want to keep all data from both datasets, we’ll want to use a full_join. full_join(df_a, df_b, by = &quot;sample&quot;) ## sample number color ## 1 A 2.0 blue ## 2 B 4.0 blue ## 3 C 4.4 &lt;NA&gt; ## 4 D 3.1 &lt;NA&gt; ## 5 E NA red ## 6 F NA green This is the biggest data frame of the joins, and will always be that way, because we have not dropped any data. Instead we’ve just fill in NAs where we don’t have information after we’ve combined df_a and df_b. Having a bunch of NAs may be okay depending on what you are hoping to do with these data. But it goes back to the idea that a lot of data science decisions, including those about tidying data, will be largely dependent on the context and goals of what you are working on. But, to make these decisions, you will need to be aware of what your data look like so: Always be looking at your data! 31.1.1 Row binding In some cases it may be more appropriate to merely stack rows on top of each other instead of doing a join. Let’s say our datasets have most of the same columns but different rows. To do this, we could use a row bind. Look up ?dplyr::bind_rows to see this function’s help page. Let’s use this function. bind_rows(df_a, df_b) ## sample number color ## 1 A 2.0 &lt;NA&gt; ## 2 B 4.0 &lt;NA&gt; ## 3 C 4.4 &lt;NA&gt; ## 4 D 3.1 &lt;NA&gt; ## 5 A NA blue ## 6 B NA blue ## 7 E NA red ## 8 F NA green If we want to keep track of which row of data came from where, we should use the .id argument to specify a column name where this info will be kept. bind_rows(df_a, df_b, .id = &quot;dataset&quot;) ## dataset sample number color ## 1 1 A 2.0 &lt;NA&gt; ## 2 1 B 4.0 &lt;NA&gt; ## 3 1 C 4.4 &lt;NA&gt; ## 4 1 D 3.1 &lt;NA&gt; ## 5 2 A NA blue ## 6 2 B NA blue ## 7 2 E NA red ## 8 2 F NA green Note that the dataset column is not super easy to understand in that it has \"1\" and \"2\" when it really means df_a and df_b. So to fix this, we can specify these names of these dataframes. Look at the examples in the documentation and try to do this so that the id column says ffood and ffood_may instead of \"1\" and \"2\". bind_rows(&quot;df_a&quot; = df_a, &quot;df_b&quot; = df_b, .id = &quot;dataset&quot;) ## dataset sample number color ## 1 df_a A 2.0 &lt;NA&gt; ## 2 df_a B 4.0 &lt;NA&gt; ## 3 df_a C 4.4 &lt;NA&gt; ## 4 df_a D 3.1 &lt;NA&gt; ## 5 df_b A NA blue ## 6 df_b B NA blue ## 7 df_b E NA red ## 8 df_b F NA green 31.1.2 Column binding Let’s say we have information about the same rows, but different variables. This would be a good use of the dplyr::bind_cols() For this example, we will need a different dataset. We need two datasets that represent the same cases (or samples). Note that when we make df_c below, we know that the samples are the same in both datasets and that they are in the same order. df_a &lt;- data.frame(sample = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), number = c(2, 4, 4.4, 3.1)) df_c &lt;- data.frame(sample = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), animal = c(&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;snake&quot;)) Before we merge these two datasets, we would want to make absolutely sure that our samples are in the same order in each. Otherwise we will be combining data in a way that doesn’t make sense and could throw off the rest of our analysis! (Eeek!) We can use our friend, the function all.equal() to check that we are working with the same samples and in the same order. all.equal(df_a$sample, df_c$sample) ## [1] TRUE Now we know we can bind_cols() of df_a and df_c safely. bind_cols(df_a, df_c) ## New names: ## * sample -&gt; sample...1 ## * sample -&gt; sample...3 ## sample...1 number sample...3 animal ## 1 A 2.0 A dog ## 2 B 4.0 B cat ## 3 C 4.4 C bird ## 4 D 3.1 D snake Note that because our sample columns are named the same thing in both datasets, they’ve now been renamed to sample...1 and sample...3. But this isn’t super helpful or descriptive. We will probably want to use dplyr::rename() to make these names more informative OR since we’ve confirmed that they contain identical information, its probably best we drop one of them using dplyr::select(). bind_cols(df_a, df_c) %&gt;% rename(sample = sample...1) %&gt;% select(-sample...3) ## New names: ## * sample -&gt; sample...1 ## * sample -&gt; sample...3 ## sample number animal ## 1 A 2.0 dog ## 2 B 4.0 cat ## 3 C 4.4 bird ## 4 D 3.1 snake Now we have all these data in one dataset! Now you have all of these different tools in your tool belt to combine data frames that contain related information! "],["working-with-strings.html", "Chapter 32 Working with Strings", " Chapter 32 Working with Strings 32.0.1 Strings review Strings were introduced in an earlier lesson; however, to review briefly here: A string is a sequence of characters, letters, numbers or symbols. So within R, you could create a string using this syntax. Note that the string begins and ends with quotation marks: stringA &lt;- &quot;This sentence is a string.&quot; Multiple strings can be stored within vectors. So, if you have multiple vectors that you want to store in a single object, you could do so by using c() around the strings you want to store and commas to separate each individual string: objectA &lt;- c( &quot;This sentence is a string.&quot;, &quot;Short String&quot;, &quot;Third string&quot; ) 32.0.2 stringr stringr is a package within the tidyverse specifically designed to work well with strings. All functions within this package start with str_, as you’ll see below. There are many helpful functions within the stringr package. We’ll only review the basics here, but if you’re looking to accomplish something with a string and aren’t sure how to approach it, the stringr package is a good first place to look. To install and load the stringr package, you’ll use the following: # If not already installed, you&#39;ll need to install the package install.packages(&quot;stringr&quot;) # load package into R library(stringr) The best way to work through this lesson is to copy and paste every line of code into your RStudio window and see if the output makes sense to you. Working with strings and regular expressions is best learned by practice. 32.0.2.1 Available functions As we’ll only cover a few of the functions within stringr in this lesson, it’s important to remember that if you start typing “str_” within RStudio, a list of the many options will show up. str_ image 32.0.3 String basics When working with strings, some of the most frequent tasks you’ll need to complete are to: Determine the length of a string Combine strings together Subset strings 32.0.3.1 String length Returning to our object with three strings from earlier in the lesson, we can determine the length of each string in the vector. objectA &lt;- c( &quot;This sentence is a string.&quot;, &quot;Short String&quot;, &quot;Third string&quot; ) str_length(objectA) str_length output Here we see that the first string has a length of 26. If you were to go back and count the characters in the first string, you would see that this 26 includes each letter, space, and period in that string. The length of a string does not just could the letters in its length. The length includes every character. The second and third strings each have length 12. 32.0.3.2 Combining strings: str_c() If you were interested in combining strings, you’d want to use str_c. str_c( &quot;Good&quot;, &quot;Morning&quot;) str_c However, the output from this doesn’t look quite right. You may want a space between these two words when you combine the two strings. That can be controlled with the sep argument. str_c( &quot;Good&quot;, &quot;Morning&quot;, sep=&quot; &quot;) 32.0.3.3 Subsetting strings: str_sub() Often, it’s important to get part of a string out. To do this, you’ll want to subset the string using the str_sub() function. For example, if you wanted only the first three characters in the string below, you would specify that within str_sub(). object &lt;- c( &quot;Good&quot;, &quot;Morning&quot;) str_sub(object, 1, 3) str_sub output You can also use negative numbers to count from the end of the string. For example, below we see code that returns the last three positions in the string. object &lt;- c( &quot;Good&quot;, &quot;Morning&quot;) str_sub(object, -3, -1) str_sub output counting from end of string 32.0.3.4 String sorting: str_sort() Finally, if you wanted to sort a string alphabetically, str_sort() can help you accomplish that. names &lt;- c(&quot;Keisha&quot;, &quot;Mohammed&quot;, &quot;Jane&quot;) str_sort(names) str_sort() output sorts strings 32.0.4 Regular expressions Above we discuss the basics of working with strings within stringr. However, working with strings becomes infinitely easier with an understanding of regular expressions. Regular expressions (regexps) are used to describe patterns within strings. They can take a little while to get the hang of but become very helpful once you do. With regexps, instead of specifying that you want to extract the first three letters of a string (as we did above), you could more generally specify that you wanted to extract all strings that start with a specific letter or that contain a specific word somewhere in the string using regexps. We’ll explore the basics of regexps here. The use them in stringr, the general format is function(string , pattern = regexp), which you’ll see used in practice below. The set of functions from stringr we’ll cover are listed below We’ll cover a number of helpful stringr functions: str_view() - View the first occurrence in a string that matches the regex str_view_all() - View all occurrences in a string that match the regex str_count() - count the number of times a regex matches within a string str_detect() - determine if regex is found within string str_subset() - return subset of strings that match the regex str_extract() - return portion of each string that matches the regex str_replace() - replace portion of string that matches the regex with something else 32.0.4.1 Anchors If interested in finding a pattern at the beginning (^) or end ($) of a string, you can specify that using a regexp. For example, if you wanted to only look at names that started with the letter “M”, you would specify that using a regexp. The pattern you would include would be \"^M\" to identify all strings that start with the letter M. To specify those strings that end with a capital M, you would specify the pattern \"$M\". 32.0.4.2 Show matches: str_view() To get comfortable with using regexps with strings, str_view() can be very helpful. The output from str_view() highlights what portion of your string match the pattern specified in your regexp with a gray box. For example, to we’ll start using anchors and str_view() below: names &lt;- c(&quot;Keisha&quot;, &quot;Mohammed&quot;, &quot;Jane&quot;, &quot;Mathieu&quot;) ## identify strings that start with &quot;M&quot; str_view(names, &quot;^M&quot;) str_view() identifies names that start with M In this first example we see in the Viewer Panel that str_view has identified the names that start with the letter M. However, if you try to match strings that end with the letter “M”, no match is found. ## identify strings that end with &quot;M&quot; str_view(names, &quot;M$&quot;) str_view() does not identify any names that end with M To identify names by that end with the letter “a”, you would use the following. ## identify strings that end with &quot;a&quot; str_view(names, &quot;a$&quot;) str_view() identifies names that end with a Note, however, that regexps are case sensitive. To match patterns, you have to consider that “A” and “a” are different characters. ## identify strings that end with &quot;A&quot; str_view(names, &quot;A$&quot;) str_view() does not identify any names that end with A 32.0.4.3 Count matches: str_count() To count the number of matches within your strings, you would use str_count(). Below, using the names vector we’ve been using, we see that str_count() produces a 1 for those names that start with “M” and a 0 otherwise. ## identify strings that start with &quot;M&quot; ## return count of the number of times string matches pattern str_count(names, &quot;^M&quot;) str_count() strings that start with “M” However, if we instead wanted a count of the numbers of lowercase “m”s, we could still use str_count() to accomplish that. Notice below we’ve removed the specification to just look at the beginning of the string. Here, we’re looking for lowercase m’s anywhere in the string and counting them: ## identify strings that have a lowercase &quot;m&quot; ## return count of the number of times string matches pattern str_count(names, &quot;m&quot;) str_count() strings that have an m in them 32.0.4.4 Detect matches: str_detect() Instead of returning a count, at times you’re just interested in knowing which strings match the pattern you’re searching for. In these cases you’ll want to use str_detect(). This function simply returns a TRUE if the string matches the pattern specified and FALSE otherwise. ## identify strings that start with &quot;M&quot; ## return TRUE if they do; FALSE otherwise str_detect(names, &quot;^M&quot;) str_detect() returns TRUE for strings that match the specified pattern; FALSE otherwise 32.0.4.5 Subset matches: str_subset() To return the actual string that matches the specified pattern, rather than a TRUE/FALSE, you’ll look to str_subset(). This function pulls out those strings that match the specified pattern. For example, to obtain the subset of names whose values start with the capital letter “M”, you would use the following: ## identify strings that start with &quot;M&quot; ## return whole string str_subset(names, &quot;^M&quot;) str_subset() returns the strings that match the pattern specified 32.0.4.6 Extract matches: str_extract() To extract only the portions of the string that match the specified pattern, you would use str_extract(). This function returns the pattern specified for strings where it is found and NA otherwise. For example, by searching for names that start with M, below, we see that the second and fourth strings in our vector return the pattern specified (“M”) and that the first and third strings in the vector return NA, as they do not start with a capital “M”. ## return &quot;M&quot; from strings with &quot;M&quot; in it ## otherwise, return NA str_extract(names, &quot;^M&quot;) str_extract() returns the portions of the strings that match the pattern specified 32.0.4.7 Replace matches: str_replace() The final basic function from stringr that we’ll discuss is str_replace(). This function identifies a regex and replaces each occurrence with whatever replacement the user specifies. For example, below we search for strings that start with the capital letter “M” and replace each of them with a question mark. All strings that do not match the regex are returned unchanged. ## replace capital M with a question mark str_replace(names, &quot;^M&quot;, &quot;?&quot;) str_replace() replaces regex with specified characters 32.0.4.8 Common regular expressions Above we discuss two common patterns searched for using regular expressions: starts with (^) and ends with ($). However, there are a number of additional common ways to match patterns. They are listed here, and we’ll discuss each one in slightly more detail below. 32.0.4.8.1 Searching for characters To search for a set of characters, you place these characters within brackets. Below, this will identify anywhere in the strings where you have a lowercase vowel. Note, that we’re now using str_view_all() to identify all occurrences of these characters, rather than str_view(), which only identifies the first occurrence in each string. ## identify all lowercase vowels str_view_all(names, &quot;[aeiou]&quot;) brackets specify which characters to search for 32.0.4.8.2 Searching for anything other than a set of characters By adding a caret (^) before the vowels within the brackets, this regular expressions specifies that you are searching for any character that is not a lowercase vowel within your strings. ## identify anything that&#39;s NOT a lowercase vowel str_view_all(names, &quot;[^aeiou]&quot;) brackets with a caret first specify which characters NOT to search for 32.0.4.8.3 Search for digits To search for digits (numeric variable between 0 and 9) in a string you use “; however, backslashes are protected characters in R. This means that you have to escape this character first with an additional backslash (\\), to let R know that you want to search for the regular expression”. addresses &lt;- c(&quot;1234 Main Street&quot;, &quot;1600 Pennsylvania Ave&quot;, &quot;Brick Building&quot;) ## identify anything that&#39;s a digit str_view_all(addresses, &quot;\\\\d&quot;) earches for digits 32.0.4.8.4 Search for whitespace Identifying whitespace in R identifies any spaces, tabs or newlines. Note that again we have to escape the “” with a backslash for R to recognize the regular expression. ## identify any whitespace str_view_all(addresses, &quot;\\\\s&quot;) searches for whitespace 32.0.4.8.5 Identify any character (except newline) To identify any character except for a newline you’ll use \".\". Notice in our addresses example that there are no newlines, so this pattern will match with the entire string. ## identify any character str_view_all(addresses, &quot;.&quot;) . searches for any character 32.0.4.9 Repetition within regular expressions Searches for regular expressions allow you to specify how many times a pattern should be found within the string. To do so, you use the following: ? : 0 or 1 : 1 or more \\* : 0 or more {n} : exactly n times {n,} : n or more times {n,m} : between n and m times 32.0.4.9.1 Examples of repetition within regular expressions Using the definitions above, we can see that the following code will identify patterns within the addresses vector where n shows up one more more times in a string. ## identify any time n shows up one or more times str_view_all(addresses, &quot;n+&quot;) + specifies to match the pattern one or more times While the difference is slight in the output here, we’re identifying portions of the string where n shows up exactly once. So, instead of the ‘nn’ in Pennsylvania matching together, the code here splits these up, due to the fact that we’re specifying the pattern match ‘n’ exactly one time: ## identify any time n shows up str_view_all(addresses, &quot;n{1}&quot;) {#} looks to match the pattern exactly the number of times within the curly braces If you only wanted to match strings where n showed up twice in a row, you could specify that in this way: ## identify any time n shows up exactly two times in a row str_view_all(addresses, &quot;n{2}&quot;) {2} specifies that the pattern must be found exactly twice This could similarly be achieved by specifying to search for the pattern ‘nn’ one or more times (+): ## identify any time &#39;nn&#39; shows up one or more times str_view_all(addresses, &quot;nn+&quot;) nn+ searches for double n one or more times in a string You can also specify a range of the number of times to search for a pattern within your string. Below, we see that if we specify n be searched for at least two and at most 3 times, the pattern matches within our string. However, if we increase that to between three and four times, no pattern matching occurs, as there are never three or four n’s in a row in our strings. ## identify any time n shows up two or three times str_view_all(addresses, &quot;n{2,3}&quot;) ## identify any time n shows up three or four times str_view_all(addresses, &quot;n{3,4}&quot;) {n,m} looks to pattern match between n and m times 32.0.5 Conclusion This lesson set out to introduce you to how to work with strings within RStudio, using the stringr package and to introduce you to regular expressions. We’ve covered a number of functions and concepts in this lesson. Feel free to review the material and practice as much as you need before completing this quiz! 32.0.6 Additional Resources r4ds : Chapter 14 - Strings by Hadley Wickham stringr documentation, part of the tidyverse "],["working-with-factors.html", "Chapter 33 Working with Factors", " Chapter 33 Working with Factors In R, categorical data are handled as factors. By definition, categorical data are limited in that they have a set number of possible values they can take. For example, there are 12 months in a calendar year. In a month variable, each observation is limited to taking one of these twelve values. Thus, with a limited number of possible values, month is a categorical variable. Categorical data, which will be referred to as factors for the rest of this lesson, are regularly found in data. Learning how to work with this type of variable effectively will be incredibly helpful. To make working with factors simpler, we’ll utilize the forcats package. Similar to the stringr package, all functions within forcats begin with fct_. As before, to see available functions you can type ?fct_ in your RStudio console. A drop-down menu will appear with all the possible forcats functions. Before working through this lesson, you’ll want to be sure that forcats has been installed and loaded in: install.packages(&#39;forcats&#39;) library(forcats) fct_ output from RStudio 33.0.1 Factor basics In R, factors are comprised of two components: the actual values of the data and the possible levels within the factor. Thus, to create a factor, you need to supply both these pieces of information. For example, if we were to create a character vector of the twelve months, we could certainly do that: ## all 12 months all_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) ## our data some_months &lt;- c(&quot;Mar&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Apr&quot;, &quot;Jul&quot;) However, if we were to sort this vector, R would sort this vector alphabetically. sort(some_months) sort sorts variable alphabetically While you and I know that this is not how months should be ordered, we haven’t yet told R that. To do so, we need to let R know that it’s a factor variable and what the levels of that factor variable should be. mon &lt;- factor(some_months, levels = all_months) mon sort(mon) defining the factor levels sorts this variable sensibly Here, we specify all the possible values that the factor could take in the levels = all_months argument. So, even though not all twelve months are included in the some_months object, we’ve stated that all of the months are possible values. Further, when you sort this variable, it now sorts in the sensical way! 33.0.2 Manually change the labels of factor levels : fct_relevel() What if you wanted your months to start with July first? That can be accomplished using fct_relevel(). To use this function, you simply need to state what you’d like to relevel (mon) followed by the levels you want to relevel. If you want these to be placed in the beginning, the after argument should be after = 0. You can play around with this setting to see how changing after affects the levels in your output. mon_relevel &lt;- fct_relevel(mon, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;, after = 0) mon_relevel sort(mon_relevel) fct_relevel enables you to change the order of your factor levels After re-leveling, when we sort this factor, we see that Jul is placed first, as specified by the level re-ordering. 33.0.3 Keep the order of the factor levels : fct_inorder() Now, if you’re not interested in the months being in calendar year order, you can always state that you want the levels to stay in the same order as the data you started with, you simply specify fct_inorder. mon_inorder &lt;- fct_inorder(some_months) mon_inorder sort(mon_inorder) fct_inorder assigns levels in the same order the level is seen in the data We see now with fct_inorder that even when we sort the output, it does not sort the factor alphabetically, nor does it put it in calendar order. In fact, it stays in the same order as the input, just as we specified. 33.0.4 Advanced Factoring For the remainder of this lesson, we’re going to return to using a dataset that’s in R by default. We’ll use the chickwts dataset for exploring the remaining advanced functions. This data set includes data from an experiment that was looking to compare the “effectiveness of various feed supplements on the growth rate of chickens.” chickwts dataset 33.0.5 Re-ordering factor levels by frequency : fct_infreq() To re-order factor levels by frequency of the value in the dataset, you’ll want to use fct_infreq(). Below, we see from the output from tabyl() that ‘soybean’ is the most frequent feed in the data set while ‘horsebean’ is the least frequent. Thus, when we order by frequency, we can expect these two feeds to be at opposite ends for our levels. ## take a look at frequency of each level ## using tabyl() from `janitor` package library(janitor) tabyl(chickwts$feed) ## order levels by frequency fct_infreq(chickwts$feed) %&gt;% head() fct_infreq orders levels based on frequency in dataset As expected, soybean, the most frequent level, appears as the first level and horsebean, the least frequent level, appears last. The rest of the levels are sorted by frequency. 33.0.6 Reversing order levels : fct_rev() If we wanted to sort the levels from least frequent to most frequent, we could just put fct_rev() around the code we just used to reverse the factor level order. ## reverse factor level order fct_rev(fct_infreq(chickwts$feed)) %&gt;% head() fct_rev() reverses the factor level order 33.0.7 Re-ordering factor levels by another variable : fct_reorder() At times you may want to reorder levels of a factor by another variable in your dataset. This is often helpful when generating plots (which we’ll get to in a future lesson!). To do this you specify the variable you want to reorder, followed by the numeric variable by which you’d like the factor to be re-leveled. Here, we see that we’re re-leveling feed by the weight of the chickens. While we haven’t discussed plotting yet, the best way to demonstrate how this works is by plotting the feed against the weights. We can see that the order of the factor is such that those chickens with the lowest median weight (horsebean) are to the left, while those with the highest median weight (casein) are to the right. ## order levels by a second numeric variable chickwts %&gt;% mutate(newfeed = fct_reorder(feed, weight)) %&gt;% ggplot(., aes(newfeed,weight)) + geom_point() fct_reorder allows you to re-level a factor based on a secondary numeric variable 33.0.8 Combining several levels into one: fct_recode() To demonstrate how to combine several factor levels into a single level, we’ll continue to use our ‘chickwts’ dataset. Now, I don’t know much about chicken feed, and there’s a good chance you know a lot more. However, let’s assume (even if it doesn’t make good sense with regards to chicken feed) you wanted to combine all the feeds with the name “bean” in it to a single category and you wanted to combine “linseed” and “sunflower”” into the category “seed”. This can be simply accomplished with fct_recode. In fact, below, you see we can rename all the levels to a simpler term (the values on the left side of the equals sign) by re-naming the original level names (the right side of the equals sign). This code will create a new column, called feed_recode (accomplished with mutate()). This new column will combine “horsebean” and “soybean feeds”, grouping them both into the larger level “bean”. It will similarly group “sunflower” and “linseed” into the larger level “seed.” All other feed types will also be renamed. When we look at the summary of this new column by using tabyl(), we see that all of the feeds have been recoded, just as we specified! We now have four different feed types, rather than the original six. ## we can use mutate to create a new column ## and fct_recode() to: ## 1. group horsebean and soybean into a single level ## 2. rename all the other levels. chickwts %&gt;% mutate(feed_recode = fct_recode(feed, &quot;seed&quot; = &quot;linseed&quot;, &quot;bean&quot; = &quot;horsebean&quot;, &quot;bean&quot; = &quot;soybean&quot;, &quot;meal&quot; = &quot;meatmeal&quot;, &quot;seed&quot; = &quot;sunflower&quot;, &quot;casein&quot; = &quot;casein&quot; )) %&gt;% tabyl(feed_recode) fct_recode() can be used to group multiple levels into a single level and/or to rename levels 33.0.9 Converting numeric levels to factors: ifelse() + factor() Finally, when working with factors, there are times when you want to convert a numeric variable into a factor. For example, if you were talking about a dataset with BMI for a number of individuals, you may want to categorize people based on whether or not they are underweight (BMI &lt; 18.5), of a healthy weight (BMI between 18.5 and 29.9), or obese (BMI &gt;= 30). When you want to take a numeric variable and turn it into a categorical factor variable, you can accomplish this easily by using ifelse() statements. if{} statements and else{} statements were covered in an earlier lesson. Here we combine those two ideas. Within a single statement we provide R with a condition: weight &lt;= 200. With this, we are stating that the condition is if a chicken’s weight is less than or equal to 200 grams. Then, if that condition is true, meaning if a chicken’s weight is less than or equal to 200 grams, let’s assign that chicken to the category low. Otherwise, and this is the else{} part of the ifelse() function, assign that chicken to the category high. Finally, we have to let R know that weight_recode is a factor variable, so we call factor() on this new column. This way we take a numeric variable (weight), and turn it into a factor variable (weight_recode). ## convert numeric variable to factor chickwts %&gt;% mutate(weight_recode = ifelse(weight &lt;= 200, &quot;low&quot;, &quot;high&quot;), weight_recode = factor(weight_recode)) %&gt;% tabyl(weight_recode) converting a numeric type variable to a factor 33.0.10 Conclusions This lesson has covered how to work with factors (categorical) variables in R using the forcats package in R. In line with the previous few lessons, the best way to learn how to work with factors, is to actually work with factors. Play around with the examples here and continue to practice using this type of variable! You’ll come across factors regularly as you analyze data! 33.0.11 Additional Resources r4ds : Chapter 15 - Factors by Hadley Wickham forcats, part of the tidyverse forcats blog post by Hadley Wickham Wrangling Categorical Data in R by Amelia McNamara &amp; Nicholas J Horton Note: Wrangling Categorical Data in R is full of really great examples and goes into further depth than what is covered here. This is a great resource to get more practice working with categorical data (factors) in R. "],["working-with-dates.html", "Chapter 34 Working with Dates", " Chapter 34 Working with Dates In an earlier chapter, you were introduced to different types of objects in R, such as characters, numeric, and logicals. Then, in earlier lessons in this course, we covered how to work with strings and factors in detail. The remaining type of variable we haven’t yet covered is how to work with dates and time in R. As with strings and factors, there is a tidyverse package to help you work with dates more easily. The lubridate package will make working with dates and times easier. Before working through this lesson, you’ll want to be sure that lubridate has been installed and loaded in: install.packages(&#39;lubridate&#39;) library(lubridate) 34.0.1 Dates and time basics When working with dates and times in R, you can consider either dates, times, or date-times. Date-times refer to dates plus times, specifying an exact moment in time. It’s always best to work with the simplest possible object for your needs. So, if you don’t need to refer to date-times specifically, it’s best to work with dates. 34.0.2 Creating dates and date-time objects To get objects into dates and date-times that can be more easily worked with in R, you’ll want to get comfortable with a number of functions from the lubridate package. Below we’ll discuss how to create date and date-time objects from (1) strings and (2) individual parts. 34.0.2.1 From strings Date information is often provided as a string. The functions within the lubridate package can effectively handle this information. To use them to generate date objects, you can call a function using y, m, and d in the order in which the year (y), month (m), and date (d) appear in your data. The code below produces identical output for the date September 29th, 1988, despite the three distinct input formats. This uniform output makes working with dates much easer in R. ymd(&quot;1988-09-29&quot;) mdy(&quot;September 29th, 1988&quot;) dmy(&quot;29-Sep-1988&quot;) creating date and date-time objects However, this has only covered working with date objects. To work with date-time objects, you have to further include hour (h), minute(m), and second (s) into the function. For example, in the code below, you can see that the output contains time information in addition to the date information generated in the functions above: ymd_hms(&quot;1988-09-29 20:11:59&quot;) 34.0.2.2 From individual parts If you have a data set where month, date, year, and/or time information are included in separate columns, the functions within lubridate can take this separate information and create a date or date-time object. To work through examples using the functions make_date() and make_timedate(), we’ll use a dataset called nycflights13. As this dataset is not included with the R by default, you’ll have to install and load it in directly: install.packages(&#39;nycflights13&#39;) library(nycflights13) Loading this package makes a data frame called flights, which includes “on-time data for all flights that departed NYC in 2013,” available. We will work with this dataset to demonstrate how to create a date and date-time object from a dataset where the information is spread across multiple columns. First, to create a new column, as we’ve done throughout the lessons in this course, we will use mutate(). To create a date object, we’ll use the function make_date(). We just then need to supply the names of the columns containing the year, month, and day information to this function. ## make_date() creates a date object ## from information in separate columns flights %&gt;% select(year, month, day) %&gt;% mutate(departure = make_date(year, month, day)) mutate and make_date() create a new column – departure – with a date object A similar procedure is used to create a date-time object; however, this requires the function make_datetime() and requires columns with information about time be specified. Below, hour and minute are included to the function’s input. ## make_datetime() creates a date-time object ## from information in separate columns flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(departure = make_datetime(year, month, day, hour, minute)) mutate and make_datetime() create a new column – departure – with a date-time object 34.0.3 Working with dates The reason we’ve dedicated an entire lesson to working with dates and have shown you how to create date and date-time objects in this lesson is because you often want to plot data over time or calculate how long something has taken. Being able to accomplish these tasks is an important job for a data scientist. So, now that you know how to create date and date-time objects, we’ll work through a few examples of how to work with these objects 34.0.3.1 Getting components of dates Often you’re most interested in grouping your data by year, or just looking at monthly or weekly trends. To accomplish this, you have to be able to extract just a component of your date object. You can do this with the functions: year(), month(), mday(),wday(), hour(), minute() and second(). Each will extract the specified piece of information from the date or date-time object. mydate &lt;- ymd(&quot;1988-09-29&quot;) ## extract year information year(mydate) ## extract day of the month mday(mydate) ## extract weekday information wday(mydate) ## label with actual day of the week wday(mydate, label = TRUE) lubridate has specific functions to extract components from date and date-time objects 34.0.4 Time spans In addition to being able to look at trends by month or year, which requires being able to extract that component from a date or date-time object, it’s also important to be able to operate over dates. If I give you a date of birth and ask you how old that person is today, you’ll want to be able to calculate that. This is possible when working with date objects. By subtracting this birthdate from today’s date, you’ll learn now many days old this person is. By specifying this object using as.duration(), you’ll be able to extract how old this person is in years. ## how old is someone born on Sept 29, 1988 mydate &lt;- ymd(&quot;1988-09-29&quot;) ## subtract birthday from todays date age &lt;- today() - mydate age ## a duration object can get this information in years as.duration(age) dates and date-times can be operated upon Using addition, subtraction, multiplication, and division is possible with date objects, and accurately takes into account things like leap years and different number of days each month. This capability and the additional functions that exist within lubridate can be enormously helpful when working with dates and date-time objects. 34.0.5 What’s not covered in this lesson This lesson has not covered how to work with times, much detail about how to operate on date or date-time objects, nor how to deal with timezones in date-time objects. To learn more about these subjects, feel free to explore the additional resources below. 34.0.6 Additional Resources r4ds : Chapter 16 - Dates and times by Hadley Wickham lubridate, part of the tidyverse hms package, for working with time objects. This package is also part of the tidyverse "],["project-organization.html", "Chapter 35 Project Organization 35.1 Setting Up Data Science Project Folders", " Chapter 35 Project Organization As you are starting to see, data science projects involve lots of files. There are files of data, files for code, files for documentation, figures, documents to communicate with other people. A surprising amount of doing data science really well is just being good at managing and organizing all of these files so that they are: Easy to find Easy to share Easy to understand Easy to update The reason that you need to make it easy to find and work with the files in your analysis is because data science is really closely related to communication. We use data is to understand the world and communicate that understanding. So when you are building a data science project, you should be thinking about who will be on the receiving end of your data analysis. While you are actively working on a project it is often easy to find any file you need off the top of your head. But the audience of your data analysis is not you, right now. One audience is other people who you will share your work with. They don’t know about all the different places you have stored data, or which piece of code to run first. You need to make it easy on them to be able to understand what you did. Two of your most common audiences in a data science project. Another audience that may surprise you when you are just starting, but will be familiar to any experienced data analyst is you. A twist on a famous saying about data science is: Your closest co-worker is you six months ago, but you don’t reply to emails. This is because it is really common for you to work on a data science project, present the results, and then move on to another project. But someone else is studying your results and might have questions and want you to make a change. So a few months later, when you’ve forgotten where everything is, they come back and ask you to make a minor change and you have to dive back in and figure out which files you need to look at. The reason you 6 months from now is one of your audience members. While this all seems like something very simple and common sense, this is where the biggest problems in data science often happen. Let me give you two quick examples. The first happened a few years ago. A research group from Duke university analyzed data about human genomes to try to develop a way to predict what chemotherapy each person should get based on their genetic code. They came up with what they thought was a good predictor and published it, then proceeded to start testing it in clinical studies. The exciting result, predicting chemotherapy success from genomic data. Researchers at MD Anderson got so excited about the result that they tried to find the data and code and do the same analysis as a first step to trying the predictor on their patients. But there was a problem, they couldn’t find the data or code. They tried getting it from the researchers at Duke, but it took months of back and forth getting a data set here, a file there, none of it too organized. Once they actually managed to get all of the files together and organized, they realized there were some major problems with the predictors that would probably put patients at risk! Two statisticians helped organize all the data and code from the project Ultimately this discovery shut down the clinical studies and led to a major lawsuit. While there were a lot of problems with the original analysis, the reason there was so much trouble was that the files and code weren’t organized so it took a long time to figure out the problems that would put patients at risk. It took a long time to do this organization and the delay meant erroneous clinical studies were carried out. Another famous example where file organization caused problems was with the scientific paper “Growth in a time of debt”. This paper was written by two Harvard economists and suggested that countries with a high level of debt have slow economic growth. Growth in the time of debt scientific paper. Unlike most academic papers this paper had a big impact! Many countries used this research to justify austerity measures that impacted social and healthcare programs around the world. But it turns out there were some choices the authors made that were questionable or changed their results. This mistake was so important that it was covered by popular shows like The Colbert Show on Comedy Central. The error was actually discovered by a student, but not until much much later. One reason it took so long is the data and analysis files weren’t easily available to everyone and organized in a way that the error could be easily identified. The errors in the analysis were discussed on the Colbert Report It isn’t just because of errors that you’d want to have organized files and projects. Whether it’s helping your future self, communicating a data science idea, or simply reducing your cognitive load, organizing projects from the start can save you a huge amount of time and hassle. As Jenny Bryan, one of the most famous data scientists in the world, says File organization and naming are powerful weapons against chaos. Jenny Byran says “File organization and naming are powerful weapons against chaos.” Another famous data scientist, Karl Broman, suggests that the best way to end up with a good file organization system has three steps. Step 1 slow down and make lots of notes for yourself. Step 2 have sympathy for your future self. Step 3 have a standard system that you understand A key challenge is that its sometimes hard to feel like file organization is “real work”. It isn’t coding, or making plots, or writing final documents. Sometimes, when working under a deadline, you will feel pressure to skip file organization in favor of quickly producing results and handing them off. But this never pays off in the long run, the results of poorly organized work almost always fail at some point further down the line. As a general rule of thumb it makes sense to budget 10-20% of the time you will be working on a data science project just to organizing and documenting your work. Keeping your project files organized has the following benefits: Easier collaboration: You will see that as a data scientist, you will have to work with teams of other data scientists most of the time. Organizing makes collaboration a lot easier. Lower likelihood of making mistakes: When you organize, it is easier for you and others to see where you might have made a mistake before it’s too late. Easier recall: Going back to your analysis and understanding what you have done in 6 months or a year or even later will be a lot easier. If your project is not organized, it is very likely that you go back to your analysis and do not understand what you or your collaborator have previously done. More transparency and honesty on your part: This is because you make it easier for others to go through your files and replicate your analysis. 35.1 Setting Up Data Science Project Folders As you are working on your project, you will want to keep track of your files and iteratively return to how to organize and name the folders and files in that project. Naming and organizing files seems very boring, but it one of the most important parts of any data science project! Not having the files or the data available is one of the most common reasons that errors are missed in data science projects. 35.1.1 A project organization framework We will set up data science projects on RStudio Cloud. Open your web browser and navigate to the website https://rstudio.cloud/. Go to RStudio Cloud Then log in and click on the project my_first_project. Click on my_first_project You should now see a screen that looks like this where there are three windows. The window in the lower right hand corner of the screen is the part that shows all the files you have in your project. Right now there shouldn’t be any files since we just created this project in the last lesson but didn’t do anything in the project. The RStudio Cloud interface showing the empty my_first_project Each time you start a new project you will need to create a set of folders for that project. You can create a new folder by clicking on New Folder at the top of the file window. Click New Folder to create a new folder. Then you can type in the name of the folder you want to create. First let’s create a folder called data. Name the folder data You should now see a folder called data in the file window. We now have a data folder in the files in the project. Next we will create a few more folders. For each one click the New Folder button, enter the name and click ok. Different projects may call for different organization schemes and folder set ups. The rules we give you here may not fit every project you work on. But this organizational scheme is a good start that you can iteratively evaluate and change as needed as you continue to work on your project. These folders represent the four parts of any data science project. raw_data - is the folder where you will put all the raw versions of the data you have collected or been given to analyze. tidy_data - is the folder where you will put cleaned versions of the data. results - is where you will put plots, data pictures, and other results you have created from your data. util - code that is not in Rmds, but is more background code that doesn’t need to be referenced as much is kept here. Now that we have these folders in place the next thing you need to create is a README file. This is a file where you will describe all of the data and projects you will be doing. Every project should have a README file so that you can keep notes on what you have done during your project. You will add to this README as your project expands. To create the README file click File at the top left hand part of Rstudio. Click on File to create a new file. then over over New File to show the types of new files you can create. Move the cursor down and click on Text File. Hover over New File and move the cursor to Text File. You should see a new screen open with the title Untitled like this. An untitled text file. To save the file click on the disk icon in the top left hand corner of the screen. Click on the disk icon to save. Then you can title the file README.md and click Save to save it. Name the file README.md. You should now see the README.md file in your file list on the bottom right of the screen. You have now saved the file README.md. The next thing to do is fill in the README file with the initial description of your project. For now you can copy this text and paste it into your README file, then click the save button. # This is the README file for my_first_project The folders in this project are: * _raw_data_ - all the raw versions of the data are kept here. * _tidy_data_ - cleaned versions of the data are here. * _results_ - results, figures and plots from our data are kept here * _util_ - code that is not in Rmds, but is more background is kept here. The README file can be used to describe both the high level organization as well as any important special cases about your project. It may be helpful to create additional README files in each subfolder to provide information specific to the files in that subfolder. You would want to link to them from the global README file you have just created. 35.1.2 The next level of organization This is the top level of the organization of a new data science project, but we will usually need to create a little more organization within each folder. For example if you click on the data folder you will see that it is empty. Click on the data folder. You can see at the top of the file organization tab that you are inside the folder data which is inside of the folder project. Click on the data folder. We need to create two new folders inside of the data folder, one for our raw data and one for our tidy data. You will learn a lot more about them later, but for now use the New Folder button Click on the New Folder button.. to create one folder called raw_data and another called tidy_data. Create raw_data and tidy_data folders. One way to write the folders we have now created is like this. data/ raw_data/ tidy_data/ results/ figures/ tables/ util/ Using the same steps we did for creating the folders inside of the data folder, you can create the rest of the folders you will need to organize your data science project. Every time you start a new project you will need to do these steps to set up the folders you will need to store all the files you will be creating. In the next lesson we will talk about how to name the files that will go in these folders. 35.1.3 The README File Once you have a file structure in place, it’s best to write a README file. This file should explain how the code and data are organized as well as how they are related. In other words, explain what is what. For each filename, we recommend to have a short description of what the file is for. You can also have a description of how the data were obtained or collected including links or references to publication or other documentation. Mention people involved with the project, and provide contact information of at least one of the collaborators. A good idea is to update the README file as you work. As you create new files and folders, add their descriptions to the README file so you can keep track of what you are working on. You might also make a more detailed README file, which includes a list of variables, units of measurement of each variable, definition of code and symbols used to deal with missing data, Licenses or restrictions on the content of the project including the data, and information about how others should cite your analysis. 35.1.4 Using Comments Our next piece of advice is that use comments within your code. Commenting in R is done by adding the pound sign (#). If you add the pound sign at the beginning of each line, R will not read that line as code and will instead skip it. So you can add your explanations about each chunk of code using the pound sign. This is an example of commenting: # calculates the products of a vector and a matrix # checks if they can be multiplied first function (x, y){ if (dim(x)[2] != length(y)) { stop(&quot;Can&#39;t multiply matrix%*%vector because the dimensions are wrong&quot;) } product &lt;- matrix %*% vector return(product) } Note that the first line describes what the function does. Commenting is helpful especially when the code is more complicated and harder to understand. Keep comments short and informative and avoid inserting comments for code that have an obvious purpose, i.e. don’t add comment for a line of code that adds two variables). 35.1.5 Write in a modular way One mistake in writing code is to have everything in one file. This is bad practice since fixing bugs and replicating the analysis would be much harder. Therefore, it is recommended that you write code in a modular way so that each group of code that do similar things can be put in a single file and the master file calls these individual files. The result is a much cleaner and shorter master file. 35.1.6 Follow a style guide Just as it’s dIsTraCTING 2 reAD A SENtenceTHAT hasWEird spacing, puncTUATION, SPELLING. The same is true for code. Code also is easier for yourself and others to read if it is consistent and follows conventions. Style is not something you need to worry about as you are just starting to work on getting code working, but as you continue to polish a project it is something you will want to return to. What style you follow is not particularly as important as it is to just follow a style guide in general. Here’s some suggested style guides you may want to use: Google’s R Style guide The tidyverse style guide There’s also nifty packages that can automatically style your code for you like the styler package. "],["cleaning-data-project.html", "Chapter 36 Cleaning Data Project", " Chapter 36 Cleaning Data Project In this project, we are going to try to answer this question with data:       “Across the countries of the world, how is literacy related to life expectancy?” To answer our question, we’ll need a dataset that has literacy information by countries, as well as a dataset that has life expectancy statistics for different countries. We already downloaded the data for you from Kaggle. 36.0.1 Starting up this project Go to the DataTrail workspace. Return to your own DataTrail_Projects project in RStudio. For this project, go to the 03_Cleaning_Data folder. Click on the file countries_project.Rmd to open this file. 36.0.2 Your objectives! To complete this project you’ll need to do a few things within the countries_project.Rmd file. Go through this notebook, reading along. Fill in empty or incomplete code chunks when prompted to do so. Run each code chunk as you come across it by clicking the tiny green triangles at the right of each chunk. You should see the code chunks print out various output when you do this. At the very top of this file, put your own name in the author: place. Currently it says \"DataTrail Team\". Be sure to put your name in quotes. In the Conclusions section, write up responses to each of these questions posed here. When you are satisfied with what you’ve written and added to this document you’ll need to save it. In the menu, go to File &gt; Save. Now the nb.html output resulting file will have your new output saved to it. Open up the resulting countries_project.nb.html file and click View in Web Browser. Does it look good to you? Did all the changes appear here as you expected? Upload your Rmd and your nb.html to your assignment folder (this is something that will be dependent on what your instructors have told you – or if you are taking this on your own, just collect these projects in one spot, preferably a Google Drive)! Pat yourself on the back for finishing this project! "],["plotting-the-data.html", "Chapter 37 Plotting the Data 37.1 Learning Objectives", " Chapter 37 Plotting the Data 37.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Understand what makes a good data visualization Use ggplot2 to make a data visualization Save a plot to a file Identify the difference between an exploratory and explanatory plot Create a data table Create a multipanel plot Use data visualizations to illustrate a facet about your data 37.1.1 Data Visualization The adage “A pictures worth a thousand words” is especially true in data science. When you are first handed a data set visualizing your data can help you get a handle on what’s going on in the data set. Similarly, once you’ve done your analysis and are ready to communicate your findings to your teammates, data visualizations are an effective way to communicate your results to others. In this lesson, we’ll walk through what data visualization is and define some of the basic types of data visualizations. At its core, the term ‘data visualization’ refers to any visual display of data that helps us understand the underlying data better. This can be a plot or figure of some sort or a table that summarizes the data. Generally, there are a few characteristics of all good plots. 37.1.1.1 General Features of Plots Overall, a good data visualization is clear and communicates an idea. The best visualizations do not need long-winded explanations because when people look at them, they can understand what is happening. A number of features can contribute to good data visualizations. While not exhaustive, good plots have: 1. Clearly-labeled axes. 2. Text that are large enough to see. 3. Axes that are not misleading. 4. Data that are displayed appropriately considering the type of data you have. More specifically, however, there are two general approaches to data visualization: exploratory plots and explanatory plots. 37.1.1.2 Exploratory Plots These are data displays to help you better understand and discover hidden patterns in the data you’re working with. These won’t be the prettiest plots, but they will be incredibly helpful. Exploratory visualizations have a number of general characteristics: They are made quickly. You’ll make a large number of them. The axes and legends are cleaned up. Below we have a graph where the axes are labeled and general pattern can be determined. This is a great example of an exploratory plot. It lets you the analyst know what’s going on in your data, but it isn’t yet ready for a big presentation. Exploratory Plot As you’re trying to understand the data you have on hand, you’ll likely make a lot of plots and tables just to figure out to explore and understand the data. Because there are a lot of them and they’re for your use (rather than for communicating with others), you don’t have to spend all your time making them perfect. But, you do have to spend enough time to make sure that you’re drawing the right conclusions from this. Thus, you don’t have to spend a long time considering what colors are perfect on these, but you do want to make sure your axis are not cut off. Other Exploratory Plotting Examples 37.1.2 Explanatory Plots These are data displays that aim to communicate insights to others. These are plots that you spend a lot of time making sure they’re easily interpretable by an audience. General characteristics of explanatory plots: They take a while to make. There are only a few of these for each project. You’ve spent a lot of time making sure the colors, labels, and sizes are all perfect for your needs. Here we see an improvement upon the exploratory plot we looked at previously. Here, the axis labels are more descriptive. All of the text is larger. The legend has been moved onto the plot. The points on the plot are larger. And, there is a title. All of these changes help to improve the plot, making it an explanatory plot that would be presentation-ready. Explanatory Plots Explanatory plots are made after you’ve done an analysis and once you really understand the data you have. The goal of these plots is to communicate your findings clearly to others. To do so, you want to make sure these plots are made carefully - the axis labels should all be clear, the labels should all be large enough to read, the colors should all be carefully chosen, etc.. As this takes times and because you do not want to overwhelm your audience, you only want to have a few of these for each project. We often refer to these as “publication ready” plots. These are the plots that would make it into an article at the New York Times or in your presentation to your bosses. Other Explanatory Plotting Examples: How the Recession Shaped the Economy (NYT) 2018 Flue Season (FiveThirtyEight) 37.1.3 Types of Plots Above we saw data displayed as both an exploratory plot and an explanatory plot. That plot was an example of a scatterplot. However, there are many types of plots that are helpful. We’ll discuss a few basic ones below and will include links to a few galleries where you can get a sense of the many different types of plots out there. To do this, we’ll use the “Davis” dataset which includes, height and weight information for 200 people. Data Set 37.1.3.1 Histogram Histograms are helpful when you want to better understand what values you have in your data set for a single set of numbers. For example, if you had a dataset with information about many people, you may want to know how tall the people in your data set are. To quickly visualize this, you could use a histogram. Histograms let you know what range of values you have in your data set. For example, below you can see that in this data set, the height values range from around 50 to around 200 cm. The shape of the histogram also gives you information about the individuals in your dataset. The number of people at each height are also counted. So, the tallest bars show that there are about 40 people in the data set whose height is between 165 and 170 cm. Finally, you can quickly tell, at a glance that most people in this data set are at least 150 cm tall, but that there is at least one individually whose reported height is much lower. Histogram 37.1.3.2 Scatterplot Scatterplots are helpful when you have numerical values for two different pieces of information and you want to understand the relationship between those pieces of information. Here, each dot represents a different person in the data set. The dot’s position on the graph represents that individual’s height and weight. Overall, in this data set, we can see that, in general, the more someone weighs, the taller they are. Scatterplots, therefore help us at a glance better understand the relationship between two sets of numbers. Scatter Plot 37.1.3.3 Barplot When you only have one numerical piece of information and a second piece of information that can be broken down into a few categories, a barplot will help you make numerical comparisons across categories. For example if you wanted to look at how many females and how many males you have in your data set, you could use a barplot. The comparison in heights between bars clearly demonstrates that there are more females in this dataset than males. Barplot 37.1.3.4 Boxplot The final basic plot we’ll talk about here is the boxplot. Boxplots also summarize numerical values across a category; however, instead of just comparing the heights of the bar, they give us an idea of the range of values that each category can take. For example, if we wanted to compare the heights of men to the heights of women, we could do that with a boxplot. Boxplot To interpret a boxplot, there are a few places where we’ll want to focus our attention. For each category, the horizontal line through the middle of the box corresponds to the median value for that group. So, here, we can say that the median, or most typical height for females is about 165 cm. For males, this value is higher, just under 180 cm. Outside of the colored boxes, there are dashed lines. The ends of these lines correspond to the typical range of values. Here, we can see that females tend to have heights between 150 and 180cm. Lastly, when individuals have values outside the typical range, a boxplot will show these individuals as circles. These circles are referred to as outliers. 37.1.3.5 Resources to look at these and other types of plots: R Graph Gallery Ferdio Data Visualization Catalog 37.1.4 Tables While we have focused on figures here so far, tables can be incredibly informative at a glance too. If you are looking to display summary numbers, a table can also visually display information. Using this same data set, we can use a table to get a quick breakdown of how many males and females there are in the data set and what percentage of each gender there is. A few things to keep in mind when making tables is that it’s best to: Limit the number of digits in the table Include a caption When possible, keep it simple. Table "],["good-plots.html", "Chapter 38 Good Plots", " Chapter 38 Good Plots The goal of data visualization in data analysis is to improve understanding of the data. As mentioned in the last lesson, this could mean improving our own understanding of the data or using visualization to improve someone else’s understanding of the data. We discussed some general characteristics and basic types of plots in the last lesson, but here we will step through a number of general tips for making good plots. 38.0.1 Tips for Making Good Plots 38.0.1.1 Choose the right type of plot If your goal is to allow the viewer to compare values across groups, pie charts should largely be avoided. This is because it’s easier for the human eye to differentiate between bar heights than it is between similarly-sized slices of a pie. Thinking about the best way to visualize your data before making the plot is an important step in the process of data visualization. Choose an appropriate plot for the data you’re visualizing. 38.0.1.2 Be mindful when choosing colors Choosing colors that work for the story you’re trying to convey with your visualization is important. Avoiding colors that are hard to see on a screen or when projected, such as pastels, is a good idea. Additionally, red-green color blindness is common and leads to difficulty in distinguishing reds from greens. Simply avoiding making comparisons between these two colors is a good first step when visualizing data. Choosing appropriate colors for visualizations is important Beyond red-green color blindness, there is an entire group of experts out there in color theory.To learn more about available color palettes in R or to read more from a pro named Lisa Charlotte Rost talking about color choices in data visualization, feel free to read more. 38.0.1.3 Label your axes Whether you’re making an exploratory or explanatory visualization, labeled axes are a must. They help tell the story of the figure. Making sure the axes are clearly labeled is also important. Rather than labeling the graph below with “h” and “g,” we chose the labels “height” and “gender,” making it clear to the viewer exactly what is being plotted. Having descriptive labels on your axes is critical 38.0.1.4 Make sure text is readable Often text on plots is too small for viewers to read. By being mindful of the size of the text on your axes, in your legend, and used for your labels, your visualizations will be greatly improved. On the right, we see that the text is easily readable 38.0.1.5 Make sure your numbers add up When you’re making a plot that should sum to 100, make sure that it in fact does. Taking a look at visualizations after you make them to ensure that they make sense is an important part of the data visualization process. At left, the pieces of the pie only add up to 95%. On the right, this error has been fixed and the pieces add up to 100% 38.0.1.6 Make sure the numbers and plots make sense together Another common error is having labels that don’t reflect the underlying graphic. For example, here, we can see on the left that the turquoise piece is more than half the graph, and thus the label 45% must be incorrect. At right, we see that the labels match what we see in the figure. Checking to make sure the numbers and plot make sense together is important 38.0.1.7 Make comparisons easy on viewers There are many ways in which you can make comparisons easier on the viewer. For example, avoiding unnecessary whitespace between the bars on your graph can help viewers make comparisons between the bars on the barplot. At left, there is extra white space between the bars of the plot that should be removed. On the right, we see an improved plot 38.0.1.8 Use y-axes that start at zero Often, in an attempt to make differences between groups look larger than they are, y-axis will be started at a value other than zero. This is misleading. Y-axis for numerical information should start at zero. At left, the differences between the vars appears larger than on the right; however, this is just because the y-axis starts at 200. The proper way to start this graph is to start the y-axis at 0. 38.0.1.9 Keep it simple The goal of data visualization is to improve understanding of data. Sometimes complicated visualizations cannot be avoided; however, when possible, keep it simple. Here, the graphic does not immediately convey a main point. It’s hard to interpret what each circle means or what the story of this graphic is supposed to me. Make sure in your graphics that your main point comes through Similarly, the intention of your graphic should never be to mislead and confuse. Unlike what was done here, be sure that your data visualizations improve viewers’ understanding. Do not aim to confuse 38.0.2 What To Consider When Making Plots Having discussed some general guidelines, there are a number of questions you should ask yourself before making a plot. These have been nicely laid out in a blog post from the wonderful team at Chartable, Datawrapper’s blog. We will summarize them here and include a number of the images from their post. The post argues that there are three main questions you should ask any time you create a visual display of your data. We will discuss these three questions below and then step through the process of creating data displays in R. Three Questions for Creating a Chart What’s your point? How can you emphasize your point in your chart? What does your final chart show exactly? 38.0.2.1 The Data We Want to Plot When discussing data visualization, it’s always good to have an example to look at. For the example here, we’ll use the same example Lisa Charlotte Rost used in her blog post. If you were interested in analyzing data that looked to assess the success of the iPhone, you would want to look at data to see how sales of iPhones have changed over time. You might, for example, start with a super basic plot like this: iPhone Sales over time 38.0.2.2 What’s your point? Whenever you have data you’re trying to plot, think about what you’re actually trying to show. Once you’ve taken a look at your data, a good title for the plot can be helpful. Your title should tell viewers what they’ll see when they look at the plot. For the iPhone example, a reasonable headline would be **“iPhone more successful than all other Apple products.” This tells us what others would expect to conclude from looking at the data in the figure. iPhone Sales over time with title 38.0.2.3 How can you emphasize your point? We talked about it in the last lesson, but an incredibly important decision is that, choosing an appropriate chart for the type of data you have is very important. In the next section of this lesson, we’ll discuss what type of data are appropriate for each type of plot in R; however, for now, we’ll just focus on the iPhone data example. With this example, we’ll discuss that you can emphasize your point by: adding data highlighting data with color annotating your plot 38.0.2.3.1 Adding data With our example data set, our title suggests that the iPhone has been Apple’s most successful product. To make that claim, it would be really helpful for the plot to compare iPhone sales with other Apple products, say, the iPad or the iPod. By adding data about other Apple products over time, we can visualize just how successful the iPhone has been. iPhone Sales over time vs other Apple Products 38.0.2.3.2 Highlighting data with color Colors help direct viewers’ eyes to the most important parts of the figure. Colors tell your readers where to focus their attention. Grays help to tell viewers where to focus less of their attention, while other colors help to highlight the point your trying to make. For example, in the iPhone example, we can de-emphasize the iPod and iPad data using gray lines, while really highlighting the huge amount of growth of the iPhone, by making its line red. iPhone Sales, in red, over time vs other Apple Products, in gray 38.0.2.3.3 Annotate your plot By highlighting parts of your plot with arrows or text on your plot, you can further draw viewers’ attention to certain part of the plot. These are often details that are unnecessary in exploratory plots, where the goal is just to better understand the data, but are very helpful in explanatory plots, when you’re trying to draw conclusions from the plot. In the iPhone example, by highlighting when Apple announced the iPhone 4 in 2010 and adding text to explain that this was the first time that more iPhones were sold than iPods, viewers get a better understanding of the data. iPhone sales over time annotated 38.0.2.4 What does your final chart show? The first step of the process told viewers what they would see in the plot. The second step showed them. The third step makes it extra clear to viewers what they should be seeing. You explain to viewers what they should be seeing in the plot. This is where you are sure to add descriptions, legends, and the source of your data. Again, these are important pieces of creating a complete explanatory plot, but are not all necessary when making exploratory plots. 38.0.2.4.1 Write precise descriptions Whether it’s a figure legend at the bottom of your plot, a subtitle explaining what data are plotted, or clear axes labels, text describing clearly what’s going on in your plot is important. Here, the author of these plots decided to include a subtitle, “Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014.” She could have similarly included this information on the y-axis “Worldwide sales of Apple products (in millions).” While there are different approaches, including this information is critical. iPhone sales over time annotated with description 38.0.2.4.2 Add legends When making a plot, be sure that viewers are able to easily determine what each line or point on a plot represents. Here, by adding text to label which line is iPhone, which is iPad, and which is iPod, viewers are quickly able to understand the plot iPhone sales over time annotated with description and text labels 38.0.2.4.3 Add a source When finalizing an explanatory plot, be sure to source your data. It’s always best for readers to know where you obtained your data and what data are being used to create your plot. Transparency is important. iPhone sales over time with source information The finalized plot is clear, the conclusion the viewer is to make is obvious, the data are well-labeled, and the plot is annotated. Final blog post plot 38.0.3 Making The iPhone Plot in R Here, we’ll introduce the code required to generate the plot used as an example in this lesson in R, but we won’t walk through this code step-by-step until a later lesson. We include this now to make two points: Once you have access to the data, you can manipulate plots to look the way you want. It often takes a lot of somewhat complicated code to reproduce someone else’s beautiful plot. As to the second point above, while the code here is rather complicated, you’ll see that the code required to make basic plots in R is quite simple. Once you master the basics, you’ll be able to start generating more and more complex plots, building on the basic building blocs that we’ll go over in the next lesson! library(ggplot2) library(reshape2) library(zoo) library(directlabels) ## after downloading data from https://blog.datawrapper.de/better-charts/ ## read data into R df &lt;- read.csv(&quot;data-orzoM.csv&quot;,stringsAsFactors=FALSE) ## get quarter year variable into a form R knows how to work with df$Year &lt;- grep(&quot;Q&quot;,unlist(strsplit(as.character(df$Quarter),&quot;\\\\s&quot;)), value=TRUE, invert=TRUE) df$Q &lt;- grep(&quot;Q&quot;,unlist(strsplit(as.character(df$Quarter),&quot;\\\\s&quot;)), value=TRUE, invert=FALSE) df$yrq &lt;- paste(df$Year,df$Q, sep=&quot; &quot;) df$yrq &lt;- as.yearqtr(df$yrq) ## reshape data into long, tidy format df2 &lt;- melt(df, id=c(&quot;Quarter&quot;,&quot;yrq&quot;,&quot;Year&quot;, &quot;Q&quot;)) df2$value &lt;- as.numeric(df2$value) ## plot data as in blog post p &lt;- ggplot(data = df2, aes(x=yrq,y=value, group=variable, color=variable)) + geom_line(size = 1.5) + scale_x_yearqtr(limits = c(2004, 2015), format = &quot;%Y&quot;, breaks=c(2004:2015), expand = c(0.1, 0.01)) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), legend.position =&quot;none&quot;, plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), panel.border = element_blank() ) + geom_dl(aes(label = variable), method = list(c(&quot;last.points&quot;), aes(colour = &quot;black&quot;), cex = 1.3) ) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + annotate(&quot;rect&quot;, xmin = 2010, xmax = 2011, ymin = 0, ymax = Inf, fill = &quot;grey&quot;, alpha = 0.2) + annotate(&quot;text&quot;, x = 2010.5, y = 40, label = &quot;After Apple announced \\n the iPhone 4 in 2010, \\n more iPhones were sold \\n than iPods for the first time.&quot;, hjust = 1, size=6) ## save plot as PNG ggsave(&quot;iPhone_sales.png&quot;,p, width=12 , units=c(&quot;in&quot;)) Reproduced blog post plot While the plots are not identical, they are very similar and tell the same story. Making good plots in R is an important skill and can be learned using the ggplot2 package in R. We’ll discuss how to generate ggplot2 plots in the next lesson! 38.0.4 Additional Resources To learn more about bad plotting techniques to avoid, you can check out Flowing Data’s Ugly Charts or WTF Visualizations. To see examples of beautiful data visualizations, check out Nathan Yau’s Flowing Data blog, the Chartable blog, or many analytical pieces published by FiveThirtyEight. To read more about red-green color blindness, read more information here. To read more about Data Visualization and the goals of Data Visualization read Elijah Meek’s Data Visualization Fast and Slow - links to the rest of the posts in this series are included there. "],["introduction-to-ggplot2.html", "Chapter 39 Introduction to ggplot2", " Chapter 39 Introduction to ggplot2 R was initially developed for statisticians, who often are interested in generating plots or figures to visualize their data. As such, a few basic plotting features were built in when R was first developed. These are all still available; however, over time, a new approach to graphing in R was developed. This new approach implemented what is known as the grammar of graphics, which allows you to develop elegant graphs flexibly in R. Making plots with this set of rules requires the R package ggplot2. To get started using ggplot2 requires you to install and load the package into R. install.packages(&quot;ggplot2&quot;) library(ggplot2) 39.0.1 The basics The grammar of graphics implemented in ggplot2 is based on the idea that you can build any plot as long as you have a few pieces of information. To start building plots in ggplot2, we’ll need some data and to know the type of plot we want to make. The type of plot you want to make in ggplot2 is referred to as a geom. This will get us started, but the idea behind ggplot2 is that every new concept we introduce will be layered on top of the information you’ve already learned. In this way, ggplot2 is layered - layers of information add on top of each other as you build your graph. In code written to generate a ggplot2 figure, you will see each line is separated by a plus sign (+). Think of each line as a different layer of the graph. We’re simply adding one layer on top of the previous layers to generate the graph. You’ll see exactly what we mean by this throughout each section in this lesson. To get started, we’ll start with the two basics (data and a geom) and build additional layers from there. As we get started plotting in ggplot2, plots will take the following general form: ggplot(data = DATASET) + geom_PLOT_TYPE(mapping = aes(VARIABLE(S))) After installation and loading ggplot2 in, you will always begin by calling the ggplot() function. You’ll then specify your dataset. Within the ggplot() function. Then, before making your plot you will also have to specify what geom type you’re interested in plotting. We’ll focus on a few basic geoms in the next section and give examples of each plot type (geom), but for now we’ll just work with a single geom: geom_point. geom_point is most helpful for creating scatterplots. As a reminder from an earlier lesson, scatterplots are useful when you’re looking at the relationship between two numeric variables. Within geom you will specify the arguments needed to tell ggplot2 how you want your plot to look. You will map your variables using the aesthetic argument aes. We’ll walk through examples below to make all of this clear. However, get comfortable with the overall look of the code now. 39.0.2 Example dataset: diamonds To build your first plot in ggplot2 we’ll make use of the fact that there are some datasets already available in R. One frequently-used data set is known as diamonds. This data set contains prices and other attributes of 53,940 diamonds, with each row containing information about a different diamond. If you look at the first few rows of data, you can get an idea of what data are included in this dataset. First 12 rows of diamonds dataset Here you see a lot of numbers and can get an idea of what data are available in this data set. For example, in looking at the column names across the top, you can see that we have information about how many carats each diamond is (carat), some information on the quality of the diamond cut (cut), the color of the diamond from J (worst) to D (best) (color), along with a number of other pieces of information about each diamond. We will use this data set to better understand how to generate plots in R, using ggplot2. 39.0.3 Scatterplots: geom_point In ggplot2 we specify these by defining x and y in the aes() argument. The x argument defines which variable will be along the bottom of the plot. y refers to which variable will be along the left side of the plot. If we wanted to understand the relationship between the number of carats in a diamond and that diamond’s price, we may do the following: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) diamonds scatterplot In this plot, we see that, in general, the larger the diamond is (or the more carats it has), the more expensive the diamond is (price), which is probably what we would have expected. However, now, we have a plot that definitively supports this conclusion. 39.0.4 Aesthetics What if we wanted to alter the size, color or shape of the points? Probably unsurprisingly, these can all be changed within the aesthetics argument. After all, something’s aesthetic refers to how something looks. Thus, if you want to change the look of your graph, you’ll want to play around with the plot’s aesthetics. In fact, in the plots above you’ll notice that we specified what should be on the x and y axis within the aes() call. These are aesthetic mappings too! We were telling ggplot2 what to put on each axis, which will clearly affect how the plot looks, so it makes sense that these calls have to occur within aes(). Additionally now, we’ll focus on arguments within aes() that change how the points on the plot look. 39.0.4.1 Point color In the scatterplot we just generated, we saw that there was a relationship between carat and price, such that the more carats a diamond has, generally, the higher the price. But, it’s not a perfectly linear trend. What we mean by that is that not all diamonds that were 2 carats were exactly the same price. And, not all 3 carat diamonds were exactly the same price. What if we were interested in finding out a little bit more about why this is the case? Well, we could look at the clarity of the diamonds to see whether or not that affects the price of the diamonds? To add clarity to our plot, we could change the color of our points to differ based on clarity: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price, color = clarity) changing point colors helps us better understand the data Here, we see that not only are the points now colored by clarity, ggplot2 has also automatically added a legend for us with the various classes and their corresponding point color. The Help pages of the diamonds dataset (accessed using ?diamonds) state that clarity is “a measurement of how clear the diamond is.” The documentation also tells us that I1 is the worst clarity and IF is the best (Full scale: I1, SI1, SI2, VS1, VS2, VVS1, VVS2, IF). This makes sense with what we see in the plot. Small (&lt;1 carat) diamonds that have the best clarity level (IF) are some of the most expensive diamonds. While, relatively large diamonds (diamonds between 2 and 3 carats) of the lowest clarity (I1) tend to cost less. By coloring our points by a different variable in the dataset, we now understand our dataset better. This is one of the goals of data visualization! And, specifically, what we’re doing here in ggplot2 is known as mapping a variable to an aesthetic. We took another variable in the dataset, mapped it to a color, and then put those colors on the points in the plot. Well, we only told ggplot2 what variable to map. It took care of the rest! Of course, we can also manually specify the colors of the points on our graph; however, manually specifying the colors of points happens outside of the aes() call. This is because ggplot2 does not have to go through the mapping the variable to an aesthetic process. In the code here, ggplot2 doesn’t have to go through the trouble of figuring out which level of the variable is going to be which color on the plot (the mapping to the aesthetic part of the process). Instead, it just colors every point red. Thus, manually specifying the color of your points happens outside of aes(): ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), color = &quot;red&quot;) manually specifying point color occurs outside of aes() 39.0.4.2 Point size As above, we can change the point size by mapping another variable to the size argument within aes: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price, size = clarity)) mapping to size changes point size on plot As above, ggplot2 handles actually doing the mapping. All you have to do is specify what variable you want mapped (clarity) and how you want ggplot2 to handle the mapping (change the point size). With this code, you do get a warning when you run it in R that using a “discrete variable is not advised.” This is because mapping to size is usually done for numeric variables, rather than categorical variables like clarity. This makes sense here too. The relationship between clarity, carat and price was easier to visualize when clarity was mapped to color than here where it is mapped to size. As above, the size of every point can be changed by calling size outside of aes: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), size = 4.5) manually specifying point size of all points occurs outside of aes() Here, we have manually increased the size of all the points on the plot. 39.0.4.3 Point Shape You can also change the shape of the points (shape). We’ve used solid, filled circles thus far (the default in geom_point), but we could specify a different shape for each clarity. ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price, shape = clarity)) mapping clarity to shape Here, while the mapping occurs correctly within ggplot2, we do get a warning message that discriminating more than six different shapes is difficult for the human eye. Thus, ggplot2 won’t allow more than six different shapes on a plot. This suggests that while you can do something, it’s not always the best to do that thing. Here, with more than six levels of clarity, it’s best to stick to mapping this variable to color as we did initially. To manually specify a shape for all the points on your plot, you would specify it outside of aes using one of the twenty-five different shape options available: options for points in ggplot2’s shape For example, to plot all of the points on the plot as filled diamonds (it is a dataset about diamonds after all…), you would specify shape ‘18’: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), shape = 18) specifying filled diamonds as shape for all points manually 39.0.5 Facets In addition to mapping variables to different aesthetics, you can also opt to use facets to help make sense of your data visually. Rather than plotting all the data on a single plot and visually altering the point size or color of a third variable in a scatterplot, you could break each level of that third variable out into a separate subplot. To do this, you would use faceting. Faceting is particularly helpful for looking at categorical variables. To use faceting, you would add an additional layer (+) to your code and use the facet_wrap() function. Within facet wrap, you specify the variable by which you want your subplots to be made: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) + facet_wrap(~clarity, nrow = 2) Here, read the tilde as the word “by”. Specifically here, we want a scatterplot of the relationship between carat and price and we want it faceted (broken down) by (~) clarity. facet_wrap breaks plots down into subplots Now, we have eight different plots, one for each level of clarity, where we can see the relationship between diamond carats and price. You’ll note here we’ve opted to specify that we want 2 rows of subplots (nrow = 2). You can play around with the number of rows you want in your output to customize how your output plot appears. 39.0.6 Geoms Thus far in this lesson we’ve only looked at scatterplots, which means we’ve only called geom_point. However, there are many additional geoms that we could call to generate different plots. Simply, a geom is just a shape we use to represent the data. In the case of scatterplots, they don’t really use a geom since each actual point is plotted individually. Other plots, such as the boxplots, barplots, and histograms we described in previous lessons help to summarize or represent the data in a meaningful way, without plotting each individual point. The shapes used in these different types of plots to represent what’s going on in the data is that plot’s geom. To see exactly what we mean by geoms being “shapes that represent the data”, let’s keep using the diamonds dataset, but instead of looking at the relationship between two numeric variables in a scatterplot, let’s take a step back and take a look at a single numeric variable using a histogram. 39.0.6.1 Histograms: geom_histogram To review, histograms allow you to quickly visualize the range of values your variable takes and the shape of your data. (Are all the numbers clustered around center? Or, are they all at the extremes of the range? Somewhere in between? The answers to these questions describe the “shape” of the values of your variable.) For example, if we wanted to see what the distribution of carats was for these data, we could to the following. ggplot(data = diamonds) + geom_histogram(mapping = aes(carat)) histogram of carat shows range and shape The code follows what we’ve seen so far in this lesson; however, we’ve now called geom_histogram to specify that we want to plot a histogram rather than a scatterplot. Here, the rectangular boxes on the plot are geoms (shapes) that represent the number of diamonds that fall into each bin on the plot. Rather than plotting each individual point, histograms use rectangular boxes to summarize the data. This helps us quickly understand what’s going on in our dataset. Specifically here, we can quickly see that most of the diamonds in the dataset are less than 1 carat. This is not necessarily something we could be sure of from the scatterplots generated previously in this lesson (since some points could have been plotted directly on top of one another). Thus, it’s often helpful to visualize your data in a number of ways when you first get a dataset to ensure that you understand the variables and relationships between variables in your dataset! 39.0.6.2 Barplots: geom_bar Barplots show the relationship between a set of numbers and a categorical variable. In the diamonds data set, we may be interested in knowing how many diamonds there are of each cut of diamonds. There are five categories for cut of diamond. If we make a barplot for this variable, we can see the number of diamonds in each category. ggplot(data = diamonds) + geom_bar(mapping = aes(cut)) Again, the changes to the code are minimal. We are now interested in plotting the categorical variable cut and state that we want a bar plot, by including geom_bar(). diamonds barplot Here, we again use rectangular shapes to represent the data, but we’re not showing the distribution of a single variable (as we were with geom_histogram). Rather, we’re using rectangles to show the count (number) of diamonds within each category within cut. Thus, we need a different geom: geom_bar! 39.0.6.3 Boxplots: geom_boxplot Boxplots provide a summary of a numerical variable across categories. For example, if you were interested to see how the price of a diamond (a numerical variable) changed across different diamond color categories (categorical variable), you may want to use a boxplot. To do so, you would specify that using geom_boxplot: ggplot(data = diamonds) + geom_boxplot(aes(x = color, y = price)) In the code, we see that again, we only have to change what variables we want to be included in the plot the type of plot (or geom) we want (here, geom_boxplot()) to get a basic boxplot. diamonds boxplot In the figure itself we see that the median price (the black horizontal bar in the middle of each box represents the median for each category) increases as the diamond color increases from the worst category (J) to the best (D). Now, if you wanted to change the color of this boxplot, it would just take a small addition to the code for the plot you just generated. ggplot(data = diamonds) + geom_boxplot(aes(x = color, y = price), fill = &quot;red&quot;) diamonds boxplot with red fill Here, by specifying the color “red” in the fill argument, you’re able to change the plot’s appearance. In the next lesson, we’ll go deeper into the many ways in which a plot can be customized within ggplot2! 39.0.6.4 Other plots While we’ve reviewed basic code to make a few common types of plots, there are a number of other plot types that can be made in ggplot2. These are listed in the online reference material for ggplot2 or can be accessed through RStudio directly. To do so, you would type ?geom_ into the Console in RStudio. A list of geoms will appear. You can hover your cursor over any one of these to get a short description. ?geom in Console Or, you can select a geom from this list and click enter. After selecting a geom, such as geom_abline and hitting ‘Enter,’ the help page for that geom will pop up in the ‘Help’ tab at bottom right. Here, you can find more detailed information about the selected geom. geom_abline help page 39.0.7 Summary In this lesson, we’ve walked through the basics of generating plots in ggplot2. You should be comfortable generating basic scatterplots, histograms, barplots, and boxplots after this lesson. You should also know how to interpret each of these plots. Finally, you should also know how to map variables within ggplot2 to specify what variables you want to plot, what colors the points should be mapped to, and how to specify the size/shape of those points. In the next lesson we’ll get into details about how to customize your plots further by adjusting font size, customizing labels and annotation, and incorporating different themes into your plots. 39.0.8 Additional Resources: R For Data Science by Hadley Wickham ggplot2 ggplot2 reference "],["customization-in-ggplot2.html", "Chapter 40 Customization in ggplot2", " Chapter 40 Customization in ggplot2 In the previous lesson, we walked through the steps of generating a number of different graphs (using different geoms) in ggplot2. We discussed the basics of mapping variables to your graph to customize its appearance or aesthetic (using size, shape, and color within aes(). In this lesson, we’ll build on what we’ve previously learned to really get down to how to customize your plots so that they’re as clear as possible for communicating your results to others. The skills learned in this lesson will help take you from generating exploratory plots that help you better understand your data to explanatory plots – plots that help you communicate your results to others. We’ll cover how to customize the colors, labels, legends, and text used on your graph. Since we’re already familiar with it, we’ll again use the diamonds dataset that we’ve been using to learn about ggplot2. 40.0.1 Colors To get started, we’ll learn how to control color across plots in ggplot2. in the last lesson we discussed using color within aes() on a scatterplot to automatically color points by the clarity of the diamond when looking at the relationship between price and carat. color within aes() to color points However, what if we wanted to carry this concept over to a bar plot and look at how many diamonds we have of each clarity group? ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) diamonds broken down by clarity Well that’s a start since we see the breakdown, but all the bars are the same color. What if we adjusted color within aes()? ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, color = clarity)) color does add color but around the bars As expected, color added a legend for each level of clarity; however, it colored the lines around the bars on the plot, rather than the bars themselves. In order to color the bars themselves, you want to specify the more helpful argument fill: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = clarity)) fill automatically colors the bars Great! We now have a plot with bars of different colors, which was our first goal! However, adding colors here, while maybe making the plot prettier doesn’t actually give us any more information. We can see the same pattern of which clarity is most frequent among the diamonds in our dataset that we could in the first plot we made. Color is particularly helpful here, however, if we wanted to map a different variable onto each bar. For example, what if we wanted to see the breakdown of diamond “cut” within each “clarity” bar? ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut)) mapping a different variable to fill provides new information Now we’re getting some new information! We can see that each level in clarity appears to have diamonds of all levels of cut. Color here has really helped us understand more about the data. But what if we were going to present these data? While there is no comparison between red and green (which is good!), there is a fair amount of yellow in this plot. Some projectors don’t handle projecting yellow well, and it will show up too light on the screen. To avoid this, let’s manually change the colors in this bar chart! To do so we’ll add an additional layer to the plot using scale_fill_manual. ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) manually setting colors using scale_fill_manual Here, we’ve specified five different colors within the values argument of scale_fill_manual(), one for each cut of diamond. The names of these colors can be specified using the names explained on the third page of the cheatsheet here. (Note: There are other ways to specify colors within R. Explore the details in that cheatsheet to better understand the various ways!) Additionally, it’s important to note that here we’ve used scale_fill_manual to adjust the color of what was mapped using fill = cut. If we had colored our chart using color within aes(), there is a different function called scale_color_manual. This makes good sense! You use scale_fill_manual() with fill and scale_color_manual() with color. Keep that in mind as you adjust colors in the future! Now that we have some sense of which clarity is most common in our diamonds dataset and were able to successfully specified the colors we wanted manually in order to make this plot useful for presentation, what if we wanted to compare the proportion of each cut across the different clarities? Currently that’s difficult because there is a different number within each clarity. In order to compare the proportion of each cut we have to use position adjustment. What we’ve just generated is a stacked bar chart. It’s a pretty good name for this type of chart as the bars fur cut are all stacked on top of one another. If you don’t want a stacked bar chart you could use one of the other position options: identity, fill, or dodge. Returning to our question about proportion of each cut within each clarity group, we’ll want to use position = \"fill\". Building off of what we’ve already done: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) position = \"fill\" allows for comparison of proportion across groups Here, we’ve specified how we want to adjust the position of the bars in the plot. Each bar is now of equal height and we can compare each colored bar across the different clarities. As expected, we see that among the best clarity group (IF), we see more diamonds of the best cut (“Ideal”)! Briefly, we’ll take a quick detour to look at position = \"dodge\". This position adjustment places each object next to one another. This will not allow for easy comparison across groups, as we just saw with the last group but will allow values within each clarity group to be visualized. ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) position = \"dodge\" helps compare values within each group Unlike in the first plot where we specified fill = cut, we can actually see the relationship between each cut within the lowest clarity group (I1). Before, when the values were stacked on top of one another, we were not able to visually see that there were more “Fair” and “Premium” cut diamonds in this group than the other cuts. Now, with position = \"dodge\", this information is visually apparent. Note: position = \"identity\" is not very useful for bars, as it places each object exactly where it falls within the graph. For bar charts, this will lead to overlapping bars, which is not visually helpful. However, for scatterplots (and other 2-Dimensional charts), this is the default and is exactly what you want. 40.0.2 Labels Text on plots is incredibly helpful. A good title tells viewers what they should be getting out of the plot. Axis labels are incredibly important to inform viewers of what’s being plotted. Annotations on plot help guide viewers to important points in the plot. We’ll discuss how to control all of these now! 40.0.2.1 Titles Now that we have an understanding of how to manually adjust color, let’s improve the clarity of our plots by including helpful labels on our plot by adding an additional labs() layer. We’ll return to the plot where we were comparing proportions of diamond cut across diamond clarity groups. You can include a title, subtitle, and/or caption within the labs() function. Each argument, as per usual, will be specified by a comma. ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) labs() adds helpful tittles, subtitles, and captions 40.0.2.2 Axis labels You may have noticed that our y-axis label says “count”, but it’s not actually a count anymore. In reality, it’s a proportion. Having appropriately labeled axes is so important. Otherwise, viewers won’t know what’s being plotted. So, we should really fix that now using the ylab() function. Note: we won’t be changing the x-axis label, but if you were interested in doing so, you would use xlab(\"label\"). ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) Accurate axis labels are incredibly important 40.0.3 Themes To change the overall aesthetic of your graph, there are 8 themes built into ggplot2 that can be added as an additional layer in your graph: themes For example, if we wanted remove the gridlines and grey background from the chart, we would use theme_classic(). Building on what we’ve already generated: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) + theme_classic() theme_classic changes aesthetic of our plot We now have a pretty good looking plot! However, a few additional changes would make this plot even better for communication. Note: Additional themes are available from the ggthemes package. Users can also generate their own themes. 40.0.4 Theme In addition to using available themes, we can also adjust parts of the theme of our graph using an additional theme() layer. There are a lot of options within theme. To see them all, look at the help documentation within RStudio Cloud using: ?theme. We’ll simply go over the syntax for using a few of them here to get you comfortable with adjusting your theme. Later on, you can play around with all the options on your own to become an expert! 40.0.4.1 Altering text size For example, if we want to increase text size to make it more easily viewable when presenting this graph, we would do that within theme. Notice here that we’re increasing the text size of the title, axis.text, axis.title, and legend.text all within theme()! The syntax here is important. Within each of the elements of the theme you want to alter, you have to specify what it is you want to change. Here, for all three, we want to later text, so we specify element_text(). Within that, we specify that it’s size that we want to adjust. ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size =14), axis.title = element_text(size = 16), legend.text = element_text(size = 14) ) theme() allows us to adjust font size 40.0.4.2 Additional text alterations Changing the size of text on your plot is not the only thing you can control within theme(). You can make text bold* and change its color within theme(). Note here that multiple changes can be made to a single element. We can change size and make the text bold**. All we do is separate each argument with a comma, per usual. ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size = 14), axis.title = element_text(size = 16, face = &quot;bold&quot;), legend.text = element_text(size = 14), plot.subtitle = element_text(color = &quot;gray30&quot;) ) theme() allows us to tweak many parts of our plot Any alterations to plot spacing/background, title, axis, and legend will all be made within theme() 40.0.5 Legends At this point, all the text on the plot is pretty visible! However, there’s one thing that’s still not quite clear to viewers. In daily life, people refer to the “cut” of a diamond by terms like “round cut” or “princess cut” to describe the shape of the diamond. That’s not what we’re talking about here when we’re discussing “cut”. In these data, “cut” refers to the quality of the diamond, not the shape. Let’s be sure that’s clear as well! We can change that using an additional layer and guides()! ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;purple4&quot;)) + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size = 14), axis.title = element_text(size = 16, face = &quot;bold&quot;), legend.text = element_text(size = 14), plot.subtitle = element_text(color = &quot;gray30&quot;) ) + guides(fill = guide_legend(&quot;cut quality&quot;)) guide() allows us to change the legend title At this point, we have an informative title, clear colors, a well-labeled legend, and text that is large enough throughout the graph. This is certainly a graph that could be used in a presentation. We’ve taken it from a graph that is useful to just ourselves (exploratory) and made it into a plot that can communicate our findings well to others (explanatory)! We have touched on a number of alterations you can make by adding additional layers to a ggplot. In the rest of this lesson we’ll touch on a few more changes you can make within ggplot2 using a slightly different graph. 40.0.6 Scales There may be times when you want a different number of values to be displayed on an axis. The scale of your plot for continuous variables (i.e. numeric variables) can be controlled using scale_x_continuous or scale_y_continuous. Here, we want to increase the number of labels displayed on the y-axis, so we’ll use scale_y_continuous: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) + scale_y_continuous(breaks = seq(0, 17000, by = 1000)) Continuous cales can be altered However, for discrete variables (aka factors or categorical variables), where there is a limited number of levels, you would use scale_x_discrete or scale_y_discrete: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) + scale_x_discrete(limit = c(&quot;SI2&quot;, &quot;SI1&quot;, &quot;I1&quot;)) + scale_y_continuous(breaks = seq(0, 17000, by = 1000)) Discrete scales can be altered 40.0.7 Coordinate Adjustment There are times when you’ll want to flip your axis. This can be accomplished using coord_flip(). Adding an additional layer to the plot we just generated switches our x- and y-axes, allowing for horizontal bar charts, rather than the default vertical bar charts: ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) + scale_y_continuous(breaks = seq(0, 17000, by = 1000)) + scale_x_discrete(limit = c(&quot;SI2&quot;, &quot;SI1&quot;, &quot;I1&quot;)) + coord_flip() + labs( title = &quot;Clearer diamonds tend to be of higher quality cut&quot;, subtitle = &quot;The majority of IF diamonds are an \\&quot;Ideal\\&quot; cut&quot;) + ylab(&quot;proportion&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size = 14), axis.title = element_text(size = 16, face = &quot;bold&quot;), legend.text = element_text(size = 14), plot.subtitle = element_text(color = &quot;gray30&quot;) ) + guides(fill = guide_legend(&quot;cut quality&quot;)) Axes can be flipped using coord_flip It’s important to remember that all the additional alterations we already discussed can still be applied to this graph! ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) + scale_y_continuous(breaks = seq(0, 17000, by = 1000)) + scale_x_discrete(limit = c(&quot;SI2&quot;, &quot;SI1&quot;, &quot;I1&quot;)) + coord_flip() + labs( title = &quot;Number of diamonds by diamond clarity&quot;, subtitle = &quot;Subset of all diamonds, looking three levels of clarity&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size = 14), axis.title = element_text(size = 16, face = &quot;bold&quot;), legend.text = element_text(size = 14), plot.subtitle = element_text(color = &quot;gray30&quot;) ) Additional layers will help customize this plot 40.0.8 Annotation Finally, there will be times when you’ll want to add text to a plot or to annotate points on your plot. We’ll discuss briefly how to accomplish that here! To add text to your plot, we can use the function annotate. This requires us to specify that we want to annotate here with a “text” geom (rather than say a shape, like a rectangle - “rect”). Additionally, we have to specify what we’d like that text to say (label), where on the plot we’d like that text to show up (using x and y for coordinates), how we’d like the text aligned (using hjust for horizontal alignment where the options are “left”, “center”, or “right” and vjust for vertical alignment where the arguments are “top”, “center” or “bottom”), and how big we’d like that text to be (size): ggplot(data = diamonds) + geom_bar(mapping = aes(x = clarity)) + scale_y_continuous(breaks = seq(0, 17000, by = 1000)) + scale_x_discrete(limit = c(&quot;SI2&quot;, &quot;SI1&quot;, &quot;I1&quot;)) + coord_flip() + labs( title = &quot;Number of diamonds by diamond clarity&quot;, subtitle = &quot;Subset of all diamonds, looking three levels of clarity&quot;) + theme_classic() + theme( title = element_text(size = 18), axis.text = element_text(size = 14), axis.title = element_text(size = 16, face = &quot;bold&quot;), legend.text = element_text(size = 14), plot.subtitle = element_text(color = &quot;gray30&quot;) ) + annotate(&quot;text&quot;, label = &quot;SI1 diamonds are among \\n the most frequent clarity diamond&quot;, y = 12800, x = 2.9, vjust = &quot;top&quot;, hjust = &quot;right&quot;, size = 6) annotate helps add text to our plot Note: we could have accomplished this by adding an additional geom: geom_text. However, this requires creating a new data frame, as explained here. This can also be used to label the points on your plot. Keep this reference in mind in case you have to do that in the future. 40.0.9 Summary In this lesson we’ve covered how to manually change the colors of your plot in R, how to use built-in ggplot2 themes, how to tailor your plot to look exactly how you want it to look within theme(), how to customize legends, how to scale axes, how to flip coordinates, and how to annotate your plots. What’s important to remember is that ggplot2 is flexible and benefits from its layered nature. Additionally, it’s important to remember that the last lesson and this lesson have just touched the surface of what’s capable in ggplot2. You’ll certainly run into things when graphing that were not covered in this lesson, and that’s great! That means you’re beginning to master ggplot2. Use what you’ve learned here and additional online available resources to generate any plot you can think of! 40.0.10 Additional References Color Cheatsheet ggthemes package - GitHub repo R4DS: Chapter 3 R4DS: Chapter 18 "],["saving-plots.html", "Chapter 41 Saving Plots", " Chapter 41 Saving Plots While you’ll frequently be generating R Markdown documents where figures are generated directly as the file is knit, it is nonetheless important to know how to save an image directly. This is helpful when you have to add plots into slide presentations (using Google Slides, for example) or to send a quick update to your team. In this lesson, we’ll discuss how to save plots you’ve generated in R within RStudio Cloud as images that you can share with others as you need. 41.0.1 Image Types There are a number of different image file formats that can be generated from R. We will only discuss four of them here, but we’ll list the other possible file formats in each of the following sections, so that you know what types of files can be generated from R. In R, frequently, individuals tend to generate one of the following four file formats. Brief characteristics of each file format are summarized here. These are worth considering when deciding what type of image you want to generate: JPEG - a popular file format that will take up less space on your computer due to how its compressed. Figures that are saved multiple times, however, will lose quality each time they’re saved. These files have a white background by default. PNG - a high-quality bitmap image file format that preserves its integrity over time. Due to this, these images will take up more storage space. The background of these files is transparent. There will be no white border around these images. If magnified, pixels may be visible. TIFF - a bitmap file format. TIFF files are not meant to compress but are meant to preserve quality over time. TIFF images can be edited using photo editors, such as Photoshop. Use this file format if you intend to edit this image later. If magnified, pixels may be visible. PDF - a vector file format. This is most helpful if you want to plot multiple images to a single file or when you want to print your images, as they can be scaled to any size without suffering from pixelation. file format summary 41.0.2 Saving Plots: ggsave() We’ve discussed how to generate plots using ggplot2, and here we will discuss how to save them using the convenient function ggsave(). By specifying the plot you want to save, the filename you want the file output to have and the path, or destination where you want this plot to be saved, you can quickly and easily save any plot generated in ggplot2. To save a different file format, you simply change the file extension in the filename argument. ggsave() will output plots in any of the following file formats: “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (on Windows only). For example, here we see how to save a plot in the “figures/exploratory_figures” directory. The code demonstrates how to save it both as a PNG and JPEG image. ## generate plot myplot &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() ## save plot as PNG ggsave(plot = myplot, filename = &quot;myplot.png&quot;, path = &quot;figures/exploratory_figures&quot;) ## save plot as JPEG ggsave(plot = myplot, filename = &quot;myplot.jpeg&quot;, path = &quot;figures/exploratory_figures&quot;) There are additional arguments within ggsave. In particular, you will often want to adjust the size of your images. You can control the size of your image using the height and width arguments. Additionally, ggsave() allows you to specify the units of those height and width arguments as either inches (“in”), centimeters (“cm”), or millimeters (“mm”). For example, to increase the size of and overwrite the PNG image created above, you could use the following: ## save plot as larger PNG ggsave(plot = myplot, filename = &quot;myplot.png&quot;, path = &quot;figures/exploratory_figures&quot;, height = 9, width = 9, unit = c(&quot;in&quot;)) 41.0.3 Alternative Approach As ggsave() is an incredibly simple function,, we recommend that you use that to generate images. However, there may be times where you run into trouble saving an image or see someone else has saved an image in a different way. In addition to ggsave(), there are a number of graphics devices that you can use to save plots as images. We recommend you use ggsave(); however we want you to additionally be aware of an alternative way to generate image files. To utilize the graphics devices outside of ggsave(), you first call the graphics device with the appropriate function: png(), jpeg(), pdf(), or tiff(). Within this function call, you’ll specify the path to and name of the file you’d like to save. You then print the image object. Once done, you always have to type dev.off() to finalize the image file generation. For example, to generate a png, you would use the following: png(&quot;figures/exploratory_figures/myplot.png&quot;) print(myplot) dev.off() To generate a JPEG, you would use this very similar syntax, but ensure that you change the initial function call to jpeg and the file extension to “.jpeg”: jpeg(&quot;figures/exploratory_figures/myplot.jpeg&quot;) print(myplot) dev.off() Similar to above, the height, width can again be specified, as above. For jpeg, bmp, and tiff files, units can also be specified (as either pixels (“px”), inches (“in”), centimeters (“cm”), or millimeters (“mm”). The default is pixels. When generating PDFs using pdf() there are additional arguments that can be viewed using ?pdf; however, there is no units argument to be specified. 41.0.4 Additional Resources R Cookbook - Further reading on file formats in R "],["from-exploratory-to-explanatory.html", "Chapter 42 From Exploratory to Explanatory", " Chapter 42 From Exploratory to Explanatory Up to this point, we have covered what to consider when making a plot, the basics of using ggplot2 to generate plots in R, how to customize ggplot2 plots, and how to save figures. In this lesson we’re going to put all of those skills together and walk through an example of how to take a plot from exploratory (its roughest form) to explanatory (polished and ready to be presented). 42.0.1 Apple Product Sales Data To discuss going from exploratory plots, such as the four plots made last lesson reviewing common geoms used in ggplot2, to explanatory plots, such as the iPhone plot we recreated from Lisa Charlotte Rost’s blog post, we’ll leverage the fact that ggplot2 is incredibly flexible and allows for layering. This means that if you can think of a change you want to make to your plot, there is almost always a way to make it happen in ggplot2. To discuss the types of changes you can make to plots, we’ll return to the iPhone sales plot we looked at in an earlier lesson in this course where we discussed considerations that should be made when making good plots and use that as an example. We’ll walk through the code used to make that plot and highlight the types of changes you can make to the the overall appearance of a plot step-by-step. Final reproduction of blog post plot Before we do that, let’s take a glance at the data used to make this plot. data for iPhone plot The variables used to reproduce the blog post graph are: yrq - a variable for the year and quarter from which the sales of the Apple product was recorded variable - which Apple Product we’re talking about (iPhone, iPad, or iPod) value - the sales for that Apple product in that year and quarter in millions of US dollars. NA means no data were available for that product in that year-quarter. 42.0.2 Exploratory Plot While there is a lot of code that led to this final product of this plot, you’ll note that it still starts with the same basic framework we already discussed. There is a ggplot() call and then the geom is defined. Here, as we wanted a line for each Apple product, so we specify the geom geom_line(). This isn’t a geom we have discussed explicitly yet, but it connects the points on a plot with a single line. It’s helpful for plotting data over time, which is exactly what we’re doing here! In addition to specifying the x and y arguments within aes(), we are also specifying the group and color arguments, both of which we have discussed previously. Here, group lets ggplot2 to map the variable variable to three different lines (one for each Apple product) and color specifies that we want to map the variable variable to three different colors. plot code highlighting first two lines In fact, if we were to just run these two lines of code, we would have a reasonable exploratory plot. ggplot(data = df2, aes(x = yrq, y = value, group = variable, color = variable)) + geom_line() Exploratory plot While this plot is not nearly as clear nor as pretty as the final plot, you can better understand the data at this point, which is precisely the goal of an exploratory plot. You may notice that in this code aes is including within the ggplot() function rather than within geom_line(). We could have put aes within geom_line() as we’ve done previously; however, when aes() is included in ggplot(), its arguments are used throughout each subsequent function that is called. As we aren’t going to be changing the x, y, group, or color variable in subsequent function calls, there is no harm in including these arguments within ggplot() rather than geom_line. Feel free to play around with aes to see if anything changes if you move it to geom_line()! 42.0.3 Increasing Line Thickness The lines on this plot are a little too thin to be seen easily and certainly not as thick as the lines on the original plot. So, let’s use the size argument within geom_line to make these lines thicker and more visible. ggplot(data = df2, aes(x = yrq, y = value, group = variable, color = variable)) + geom_line(size = 1.5) size increases line thickness 42.0.4 Adding a Title Before we go any further, we’ll want to consider including a great title. As discussed at the beginning of this lesson, it’s important to add a title that tells viewers what conclusions that should draw from the plot. ggtitle() allows the addition of both a title and a subtitle to any plot made with ggplot2. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) ggtitle adds title to plot Here, we’re using the same title and subtitle as was used in the plot originally. 42.0.5 Changing Line Colors Here, since we’ve specified a variable to color in our aes() originally, we’re able to manually control what the colors of this color variable are. We do that within the function scale_colour_manual() by specifying the three colors for our three lines using the values argument. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) scale_colour_manual allows for manual control of the line colors 42.0.6 Specifying a Theme Before we go any further, let’s just remove that gray background. To do so, we’ll use the very helpful black and white theme: theme_bw() ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() theme_bw() gets us closer to the aesthetic we’re looking for 42.0.7 Customizing the Theme After getting closer to the look we want, we can modify the theme specifically using theme(). To get us started we’ll first remove the vertical gridlines. Note that within theme, when you want to remove a theme element you specify that by setting the theme element to element_blank(). ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() ) removing vertical grid lines We can then remove the axis labels yrq and value from the plot. The necessary information about what is plotted on each axis is included in the subtitle already on this plot ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank() ) removing axis labels Now, let’s increase the size of those axis labels and the plot title, so that they’re clear to anyone looking at this plot! ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15) ) increase font size across plot Now, let’s just make a few more tweaks. Let’s first remove the legend because we’re going to directly label the lines in a few steps and let’s remove the unnecessary border around the plot. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank() ) Removing the legend and border around the plot Now that the border has been removed, we’re kind of squished up right against the edge of the plotting area. We can increase the area around the plot by specifying plot.margin. The default for this argument is plot.margin = unit(c(1, 1, 0.5, 0.5), \"lines\") where the four numbers correspond to the top, right, bottom, and left of the plot. Here, by increasing the value from 0.5 to 1 we’re increasing the space in the margin area beneath and to the left of the plot a bit. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank(), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), ) Adjusting plot margin 42.0.8 Customizing Axis Labels Now that we’ve got the theme pretty close to what we want and have increased the size of the font on the axes, it’s clear that they’re not quite displaying enough information. We can specify how and what values are displayed on the x and y axis so that each year is displayed along the x-axis and multiples of ten along the y-axis. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank(), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), ) + scale_x_yearqtr(limits = c(2004, 2015), format = &quot;%Y&quot;, breaks=c(2004:2015), expand = c(0.1, 0.01)) + scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80)) controlling axis labels 42.0.9 Adding direct labels There isn’t direct functionality to directly label lines in ggplot2, but someone has written a package called directlabels to do just that for plots generated in ggplot2. Here we add an additional geom, geom_dl to label our lines directly. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank(), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), ) + scale_x_yearqtr(limits = c(2004, 2015), format = &quot;%Y&quot;, breaks=c(2004:2015), expand = c(0.1, 0.01)) + scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80)) + geom_dl(aes(label = variable), method = list(c(&quot;last.points&quot;), aes(colour = &quot;black&quot;), cex = 1.3) ) geom_dl directly labels our three lines 42.0.9.1 Adding Annotations The plot is really close to complete at this point. But, to really make it as explanatory as possible, we want to annotate the plot to draw viewers attention to a particular point on the plot. To do so in ggplot2, we’ll use annotate(). Here, we’ll use annotate() to both to add the grey rectangle and to add the text to the plot explaining that in 2010 was the first year when more iPhones were sold than iPods. First, we specify we want to generate a “rect” using annotate and include the limits of that rectangle. ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank(), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), ) + scale_x_yearqtr(limits = c(2004, 2015), format = &quot;%Y&quot;, breaks=c(2004:2015), expand = c(0.1, 0.01)) + scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80)) + geom_dl(aes(label = variable), method = list(c(&quot;last.points&quot;), aes(colour = &quot;black&quot;), cex = 1.3) ) + annotate(&quot;rect&quot;, xmin = 2010, xmax = 2011, ymin = 0, ymax = Inf, fill = &quot;grey&quot;, alpha = 0.2) adding the rectangle to draw attention Within annotate, we to specify the geom you want to add to the plot (here we add “rect” and “text”) and the x- and y-coordinates on the graph where you want the annotation to appear. Additionally, you also have the ability to control a number of parameters, including: alpha - the transparency of shapes on the plot size - the size of the text hjust - the horizontal alignment of text (vjust controls vertical alignment) Finally, we can again use annotate() to add text directly to the plot to explain what that gray box is highlighting. Here, we specify that this annotation is “text” (rather than “rect”) and include information about where we want that annotation to appear on the plot (x and y), what we want it to say (label), how we want it to be aligned (hjust), and how big it should be (size). ggplot(data = df2, aes(x = yrq,y = value, group = variable, color = variable)) + geom_line(size = 1.5) + ggtitle(&quot;iPhone more successful than all other Apple products&quot;, subtitle=&quot;Worldwide sales of selected Apple products in million, by fiscal quarter, 2000 to 2014&quot;) + scale_colour_manual(values = c(&quot;red3&quot;,&quot;grey&quot;,&quot;grey&quot;)) + theme_bw() + theme( panel.grid.major.x = element_blank() , panel.grid.minor.x = element_blank() , axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text = element_text(size=15), plot.title = element_text(size = 22), plot.subtitle = element_text(size = 15), legend.position =&quot;none&quot;, panel.border = element_blank(), plot.margin = unit(c(1,1,1,1), &quot;lines&quot;), ) + scale_x_yearqtr(limits = c(2004, 2015), format = &quot;%Y&quot;, breaks=c(2004:2015), expand = c(0.1, 0.01)) + scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80)) + geom_dl(aes(label = variable), method = list(c(&quot;last.points&quot;), aes(colour = &quot;black&quot;), cex = 1.3) ) + annotate(&quot;rect&quot;, xmin = 2010, xmax = 2011, ymin = 0, ymax = Inf, fill = &quot;grey&quot;, alpha = 0.2) + annotate(&quot;text&quot;, x = 2010.5, y = 40, label = &quot;After Apple announced \\n the iPhone 4 in 2010, \\n more iPhones were sold \\n than iPods for the first time.&quot;, hjust = 1, size=6) adding text to plot 42.0.10 Explanatory Plot With all of these changes, you now have a beautiful plot that accomplishes everything an explanatory plot should. It conveys information clearly to the viewer, is well-labeled, and draws the attention of the viewer to the important parts of the graph. Explanatory Plot We point these out now not so you master each of these alterations, but rather so that you have some idea of what it takes to take an exploratory plot that you generally make very quickly for your own purposes and turn it into an explanatory plot that clearly communicates your results to someone else. As we walked through this example, you saw that it can take a lot of code to generate a polished graph. As you practice making plots in R, know that it’s typical to struggle to figure out how to make the plot look exactly the way you want. Google will help here and so will people on stack overflow. Googling your specific question and looking for solutions in R that other people have already come up with is a great place to start whenever you’re stuck trying to figure out something on a plot. Also, really great explanatory plots take time. It’s ok to spend a lot of time making a single important plot that effectively communicates your point. It can take a lot of code "],["data-tables.html", "Chapter 43 Data Tables", " Chapter 43 Data Tables So far, we’ve spent a fair amount of time discussing how to make great exploratory and explanatory plots. However, plots are not the only ways to display data. Often, a table is a perfect approach to summarizing and displaying your data. In this lesson, we’ll discuss when to consider making a table instead of a plot, what to consider when making your table, and how to make tables in R and RMarkdown. 43.0.1 What are Data Tables? Data tables are used to display data in a tabular format. They display the information in rows and columns. Good data tables do this in a meaningful and concise way. Similar to plots, tables can be exploratory (help you to better understand the data) or explanatory (beautifully and clearly display information in a table to others). Exploratory vs. explanatory tables In this lesson we will focus on characteristics of good plots and discuss how to take tables from exploratory to explanatory. 43.0.2 When to Make a Table Just as for deciding what type of plot to make given the data you want to display, it’s important to consider what you’re trying to convey when making a table. Tables are effective when you want to display summary information about a dataset or when you want to display top results from an analysis. 43.0.2.1 Summarize dataset information When you have an entire dataset, it’s often nice to have a table that summarizes important pieces of information. For example if you had a dataset with information on 32 different cars, you may be interested in the difference between cars in that dataset that have automatic vs. manual transmissions. Rather than go through and try to count them in this dataset, a table can do a nice job quickly summarizing and displaying this information for you. For example, you could display summarized information across a number of variables, such as how many of each type of car there are in your dataset (N), how fuel efficient the cars in each group are on average (mpg), the average weight of the cars in each group (weight), their average horsepower in each group (horsepower). This information can be easily displayed in a table. A table is better than a plot here because you’re trying to summarize information across a few different variables. A single plot that attempted to include all of this information would be less clear than this table. Similarly, four separate plots – one for each variable – would take up much more space. Thus, a table is the right approach here to display these data. Sample summary tables 43.0.2.2 Summarize top results Tables are also helpful when you want to display information about the top results from an analysis (i.e. the top 10 best-selling books, the business that had the highest sales last year, etc.). For example, if you had information about 162,049 different flights that departed the Pacific Northwest in 2014 and wanted to know which destinations had the longest average delay for flights out of the Pacific Northwest, you could use a data table to display these results. At a glance from this table we can clearly see which airports had the longest delay in arrival time without . Top results table 43.0.3 What to Consider When Making A Table When making a table, it’s important to keep the ordering and spacing of your table in mind. It’s important to make sure everything is labeled appropriately and values within the table are displayed sensibly. More specifically, we’ll discuss a number of considerations to make when designing tables. 43.0.3.1 What to put in columns and what to put in rows The human eye is much better at comparing values up and down rather than from left to right. Consider what comparisons you want viewers to be able to make easily when looking at the table. Then, put those in a column, rather than in a row. Results vertically are better than comparing horizontally 43.0.3.2 The order of rows Consider the data you’re putting in the table. If you’re displaying data over time, make sure the rows are in chronological order. If you’re displaying data about individuals, it’s likely best to order alphabetically by last name. Make sure that the order of rows helps viewers understand the table most quickly. For example, if you are displaying a table about longest delay in flight time arrival, it’s likely best to order the table from longest delay in the first row to shorter delays at the bottom of the table. Order rows logically 43.0.3.3 The order of columns Put the most informative and important columns at the left and the less important columns at the right to assist viewers who read from left to right. (In languages where reading occurs from right to left, the ordering of columns should be reversed.) Here, if we want viewers to understand which airports had the longest delays, it makes the most logical sense to start with the airport name at the left, rather than the mean arrival delay time, giving viewers some context about what they’re looking at before getting to the actual numbers. Order columns with most important information at left 43.0.3.4 The number of rows and columns Viewers should be able to figure out what the table is saying at a quick glance. If there are too many rows or too many columns, your data should most likely be a plot, not a table. For example, if you are displaying information about arrests in each of the 50 US states, it may be better to consider plots, as it’s difficult to draw any conclusions about these data across 50 different rows. However, when we are only comparing across 5 rows, as in the case of our top results from the flights analysis, we can easily make comparisons across these five airports whose data are included in the table. Limit the number of rows and columns 43.0.3.5 Labels Labels on your columns and rows should be informative and clear. If talking about the price of diamonds, a bad column label would be “P” and a better column label would be “price.” An even better column label would be “price (USD).” This label is preferred because it provides viewers with the unit for the values in the table, conveying that these prices are all in US dollars. Viewers should be able to easily determine what information the table is displaying. Below, the labels on the table at right are informative, whereas a viewer may not easily be able to determine what the column labels on the left (AN, AC, and MAD) mean. Labels should be informative and concise 43.0.3.6 Significant digits Including a lot of decimal places in a table is not helpful to viewers. Often, two significant digits is enough. You’ll want to always double check the values in your table to make sure they are displayed appropriately before finalizing a table. Consider whether the appropriate number of digits for values in your table has been used 43.0.3.7 A good title or caption Just like with good plots, good tables should have a title or caption that is clear and concise. It should tell viewers what they should determine from the data in the table. Including a good title or caption is critical 43.0.3.8 The source of the data Like with explanatory graphs, it’s important to include the source of the data used in your table at the bottom of the table when finalizing a table. Always include the source of the data in your table With an idea of how to make a great explanatory table, you’re ready to start practicing making your own in R. 43.0.4 Additional resources Intro to table design Data Tables in R slideshow "],["tables-in-r.html", "Chapter 44 Tables in R", " Chapter 44 Tables in R Now that we have a good understanding of what to consider when making tables, we can to practice making good tables in R. To do this, we’ll return to the diamonds data set. As a reminder, this dataset contains prices and other information about ~54,000 different diamonds. If we want to provide viewers with a summary of these data, we may want to provide information about diamonds broken down by the quality of the diamond’s cut. To get the data we’re interested in, we’ll use the diamonds dataset and the dplyr R package, which we discussed in a lesson in an earlier chapter. 44.0.1 Getting the data in order To start figuring out how the quality of the cut of the diamond affects the price of that diamond, we first have to first get the data in order. To do that, we’ll use the dplyr package that you learned about in an earlier chapter in this series. This allows us to group the data by the quality of the cut (cut) before summarizing the data to determine the number of diamonds in each category (N), the minimum price of the diamonds in this category (min), the average price (avg), and the highest price in the category (max). To get these data in order, you could use the following code. This code groups the data by cut (quality of the diamond) and then calculates the number of diamonds in each group (N), the minimum price across each group (min), the average price of diamonds across each group (avg), and the maximum price within each group (max): library(dplyr) df &lt;- diamonds %&gt;% group_by(cut) %&gt;% dplyr::summarize( N = n(), min = min(price), avg = mean(price), max = max(price) ) 44.0.2 An exploratory table df By getting the data summarized into a single object in R (df), we’re on our way to making an informative table. However, this is clearly just an exploratory table. The output in R from this code follows some of the good table rules above, but not all of them. At a glance, it will help you to understand the data, but it’s not the finished table you would want to send to your boss. Exploratory diamonds table From this output, you, the creator of the table, would be able to see that there are a number of good qualities: there is a reasonable number of rows and columns - There are 5 rows and 5 columns. A viewer can quickly look at this table and determine what’s going on. the first column cut is organized logically - The lowest quality diamond category is first and then they are ordered vertically until the highest quality cut (`ideal)) Comparisons are made top to bottom - To compare between the groups, your eye only has to travel up and down, rather than from left to right. There are also things that need to be improved on this table: column headers could be even more clear there’s no caption/title It could be more aesthetically pleasing 44.0.3 Improving the table output By-default, R outputs tables in the Console using a monospaced font. However, this limits our ability to format the appearance of the table. To fix the remaining few problems with the table’s format, we’ll use the kable() function from the R package knitr and the additional formatting capabilities of the R packages kableExtra. The first step to a prettier table just involves using the kable() function from the knitr package, which improves the readability of this table kable(df) kable basic output However, there are still a few issues we want to improve upon: column names could be more informative too many digits in the avg column Caption/title is missing Source of data not included. To begin addressing these issues, we can use the add_header_above function from kableExtra() to specify that the min, avg, and max columns refer to price in US dollars (USD). Additionally, kable() takes a digits argument to specify how many significant digits to display. This takes care of the display of too many digits in the avg column. Finally, we can also style the table so that every other row is shaded, helping our eye to keep each row’s information separate from the other rows using kable_styling() from kableExtra. These few changes really improve the readability of the table. If you copy this code into your R console, the formatted table will show up in the Viewer tab at the bottom right-hand side of your RStudio console and the HTML code used to generate that table will appear in your console. library(knitr) library(kableExtra) kable(df, digits=0, &quot;html&quot;) %&gt;% kable_styling(&quot;striped&quot;, &quot;bordered&quot;) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;price (USD)&quot; = 3)) Viewer tab with formatted table 44.0.4 Annotating your table We mentioned earlier that captions and sourcing your data are incredibly important. The kable package allows for a caption argument. Below, an informative caption has been included. Additionally, kableExtra has a footnote() function. This allows you to include the source of your data at the bottom of the table. With these final additions, you have a table that clearly displays the data and could be confidently shared with your boss. kable(df, digits=0, &quot;html&quot;, caption=&quot;Table 1: Diamonds Price by Quality of Cut. Most Diamonds are of the highest quality cut and the most expensive diamonds are of the highest quality&quot;) %&gt;% kable_styling(&quot;striped&quot;, &quot;bordered&quot;) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;price (USD)&quot; = 3)) %&gt;% footnote(general=&quot;Diamonds dataset from ggplot2&quot;, general_title=&quot;Source:&quot;,footnote_as_chunk = T) Viewer tab with annotated and formatted table 44.0.5 Tables in RMarkdown So far, this has all been done within RStudio. However, in an earlier chapter, we discussed the importance of making reports in RMarkdown. The previous lesson focused on how to format RMarkdown documents generally. Here we want to point out that tables made using kable() are properly formatted for RMarkdown reports. Code and table rendered from RMarkdown document Similar to how we approached learning about making figures in R, we’ve demonstrated how to make good tables with a single example. The best way to really learn how to make tables is to practice using the tips here and playing around within R. So, feel free to use this as a start and go practice making a few beautiful tables in R! 44.0.6 Additional resources Karl Broman’s approach to tables in R knitr documentation kableExtra package documentation "],["multiple-plots-in-r.html", "Chapter 45 Multiple Plots in R", " Chapter 45 Multiple Plots in R When generating reports for a data science project, there is often more than one plot that you want to display at a time. To accomplish this in R, we will use the R package patchwork from Thomas Lin Pedersen, which simplifies this process. 45.0.1 Installing patchwork As with other R packages you’ve used, you will first need to install this package. As this package is relatively new, however, it is not on CRAN so install.packages('patchwork') will not work. Instead, this package has to be installed using devtools. If you don’t have devtools installed yet, do that first. If you already have that library installed, you can simply load devtools and then install the patchwork package from GitHub. # install.packages(&quot;devtools&quot;) library(devtools) devtools::install_github(&quot;thomasp85/patchwork&quot;) 45.0.2 Basic plotting using patchwork Patchwork allows any plots generated from ggplot2 to be combined simply. For example, if you were interested in plotting data from the mtcars dataset in R but wanted to combine two different plots, you could use patchwork. Below, you’ll see code for combining two plots. The first plot examines the relationship between how many miles per gallon (mpg) a car a gets and the weight of that car in 1000lb (wt). This is assigned to object p1. The second plot looks at the relationship between how many mpgs a car gets and the number of forward gears that car has (p2). While you could plot each individually, you may want to see them side by side. With patchwork, you can do this my simply using a plus sign (p1+p2). Using this very simple code, you’ll obtain side by side plots, which is exactly what you were looking to achieve! library(ggplot2) library(patchwork) p1 &lt;- ggplot(mtcars) + geom_point(aes(wt, mpg)) p2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, mpg, group = gear)) p1 + p2 Two plots side-by-side 45.0.3 Altering the layout If you don’t want these plots side-by-side, but rather one on top of the other, this can be controlled with plot_layout(), where you can define the number of columns (ncol) or number of rows (nrow) you would like to display. p1 + p2 + plot_layout(ncol = 1) two plots one on top of the other Similarly, if you want these on top of one another but would rather the first plot be larger, this can be controlled within plot_layout() using the heights argument. The ‘3’ in the code below specifies that you would like the top plot to be 3 times larger than the bottom plot (whose relative height is defined as ‘1’). p1 + p2 + plot_layout(ncol = 1, heights = c(3, 1)) two plots one on top of the other of unequal heights 45.0.4 Nesting plots Finally, sometimes you want a more complicated layout. Patchwork can handle this too. To nest a plot within another plot, the formatting is slightly more complicated, you can use parentheses ‘()’ or brackets ‘{}’ around the plots you want to nest. For example, in the code below, plot p4 is plotted first and the whole plot is defined to have one column. Then, plot p1 is added. Finally,p2 and p3 are combined into a single column plot before this combined plot (p2 + p3) is nested into the overall plot. The brackets {} are placed carefully around the plots to determine the achieve the desired plotting format. p3 &lt;- ggplot(mtcars) + geom_smooth(aes(disp, qsec)) p4 &lt;- ggplot(mtcars) + geom_bar(aes(carb)) p4 + ( p1 + ( p2 + p3 + plot_layout(ncol = 1) ) ) + plot_layout(ncol = 1) Nested Plots 45.0.5 Additional operators So far we have only added plots to one another using ‘+’. While this operator can accomplish most multi-plotting goals, we will briefly highlight a few additional operators within patchwork. 45.0.5.1 | and / If you are simply adding plots beside one another or on top of one another, this can be accomplished using | and /. | tells patchwork to put the plots next to one another from left to right. / tells patchwork you want those plots on top of each other. For example, below we see that p1, p2, and p3 should all be next to one another. These are grouped together because of the parentheses. Then these three plots should all be atop p4. (p1 | p2 | p3) / p4 Three plots together horizontally over another plot vertically 45.0.5.2 &amp; and * ggpplot2 allows for flexibility in theme in many different ways, as demonstrated in a previous lesson. To apply the same theme to all plots in the multi-plot image, you will want to use &amp;. Below you’ll see that with the use of &amp;, theme_bw has been applied to all the plots. p1 + (p2 + p3) + p4 + plot_layout(ncol = 1) &amp; theme_bw() All four plots changed to theme_bw() If you only wanted this theme to be applied to the plots in the current nesting level, you would use *. (p1 + (p2 + p3) + p4 + plot_layout(ncol = 1)) * theme_bw() Plots in main nesting level have theme_bw, but p2 and p3 remain unchanged 45.0.5.3 - Thus far, we have only focused on adding plots together using +. There is also an - operator. Think of this as a hyphen, rather than as a subtraction sign, because we won’t be removing any plots. This operator puts what is on the left and right side of the - on the same nesting level. In the code below, the overall plot layout defines that there should be one column. That means that p2 and p3 are on the same nesting level (because of the -). This forces p1 and p2 to be side by side. This operator is slightly less intuitive, and thus is included here for completeness but included at the end because you can likely compose all the multiplot images you need without fully understanding the - operator. p1 + p2 - p3 + plot_layout(ncol = 1) Using the - operator 45.0.6 Additional Resources Patchwork GitHub "],["advanced-data-visualization.html", "Chapter 46 Advanced Data Visualization", " Chapter 46 Advanced Data Visualization So far in this course we’ve discussed the characteristics of good plots and tables as well as how to generate these using the statistical programming language R. The remainder of this course will build on what you have already learned, exposing you to a few additional tools that are to you for data visualization. The rest of this course is meant to expose you to the possibilities in R, not help you become an expert. However, after having an idea of the possibilities, we will provide links to other places where you can obtain more information, allow you to practice your data visualization skills on your own, and eventually become an expert at advanced data visualization. 46.0.1 Interactive Graphics Up to this point, all the graphics we’ve generated have been static graphics. Static graphics can certainly convey a lot of important information about data. However, they don’t react to user input. Nothing changes when you scroll over them. And, there is no way to adjust what is seen in the graphic after the static graphic is generate. On the other hand, interactive graphics allow users to interact with the information displayed. These graphics add an additional layer of information to the graphics we’ve previously generated in R. For example, an interactive graphic may allow the user to scroll over points on the plot and display information about that point. Or, it may allow the user to highlight a portion of the graph and zoom in on that part of the graph. Generally, if a graph changes in response to user input, it is an interactive graphic. 46.0.2 Animated Graphics Animated graphics are similar to interactive graphics in that they are not static. However, unlike interactive graphics – which respond to user input – animated graphics can generally be thought of as a number of static plots displayed one after another in a specific order in an animation. These are helpful when trying to display changes over time. 46.0.3 Advanced Graphics In R Both interactive and animated graphics can be generated in R. We’ll cover the basics below and will include additional resources that can be used to learn even more about generating these types of graphics in R. 46.0.3.1 plotly plotly can be used to “easily translate ggplot2 graphs to an interactive web-based version and/or create custom web-based visualizations directly from R.” plotly is its own framework that has been implemented to be usable in R through the plotly package 46.0.3.1.1 plotly interactive graphics Below, you see that p is a ggplot object. Here, we’re using a dataset with data from 150 different iris flowers. We’re looking at the petal length and petal width of these flowers, broken down by the species of the flower in the plot below. To take this static plot from ggplot (which we learned to generate earlier in this course), to an interactive plot, it only requires calling ggplotly(). ## Load libraries library(ggplot2) library(plotly) ## generate ggplot2 object p &lt;- ggplot(iris, aes(x = factor(Species), y = Petal.Length, color = Petal.Width)) + geom_boxplot() + geom_point() ## generate intreactive graphic p &lt;- ggplotly(p) p Within RStudio, ggplotly() graphs will display within the ‘Viewer’ tab. iris interactive plot in RStudio While this picture here appears to be static, within RStudio, you are able to interact with this graph. For example, if you hover over a point on the graph, information about that particular point will display. information about a specific point Similarly, if you hover your mouse over the boxplot, summary information about that category will display. information about a specific group of points If you hover over the graph in general, options will display at the top right of the interactive graph. hover over plot menu displays We won’t walk through each of these now, but if you hover your mouse over any of the icons in the menu, it will describe what that button does. The best way to get a feel for what these do is to play around with these options in RStudio and to read the plotly documentation hover over button in menu displays 46.0.3.1.2 plotly animated graphics In addition to general interactive graphics, plotly can also be used to generate animated plots - plots that show changes over time. The code for this is similar in form to what is seen above; however, it requires the frame argument be specified. This states what variable ggplotly should use for each frame in the animation. To demonstrate how this can be done, we’ll use a very common example data set, frequently used by R users. This dataset is from Jenny Bryan’s gapminder R package. It contains data about the life expectancy, population, and per capita GDP from a number of countries from 1952 to 2007. We will use these data to generate an animated graphic that shows the relationship between per capita GDP and life expectancy by country. Each point on the graph represents a different country. The points are colored by the continent that each country is from. As mentioned above, to generate an animated plot within plotly, frame must be specified. Below, frame is specified to be year, which specifies that the animation should cycle through the years within this dataset. ## install the package install.packages(&quot;gapminder&quot;) ## get the data data(gapminder, package = &quot;gapminder&quot;) ## generate ggplot object gg &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(aes(frame = year, ids = country)) + scale_x_log10() ## plot animated graphic ggplotly(gg) plotly animated graphic An important point is that this animated graphic is still an interactive graphic. For example, if we were only interested in looking at this relationship for countries in Africa and Europe, we could click on “Americas”, “Asia”, and “Oceania” in the legend to remove those points from the plot in the current view. plotly animated graphic is still interactive 46.0.3.1.3 plotly in RMarkdown reports plotly objects can be printed directly into RMarkdown reports (.Rmd files) as long as that .Rmd file is knit into an HTML document. If, however, a plotly object is knit in a non-HTML format, it will print simply as a .png screenshot of the graph. 46.0.3.2 gganimate With many things in R, there is more than one way to generate animated objects in RStudio. There is an additional package that allows for the generation of animated graphics from ggplot2 graphs: gganimate. Below we will generate the same graph as above but will use gganimate() to do so. The code to generate the ggplot object created (gg) will not change. The benefit of gganimate() is that if you are interesting in saving the image as a GIF, rather than embedding the plot into your .Rmd report, gganimate() has this capability. (gganimate() can save animated graphics as .gif, .mp4, .swf, and .html objects) ## download and load gganimate devtools::install_github(&quot;dgrtwo/gganimate&quot;) library(gganimate) ## plot animated graphic using gganimate gganimate(gg) ## to save this plot gganimate(gg, &quot;output.gif&quot;) {format: gif} 46.0.4 Additional resources plotly documentation gganimate documentation "],["plotting-data-project.html", "Chapter 47 Plotting Data Project", " Chapter 47 Plotting Data Project In this project, we are going to try to answer this question with data: “What variables relate to the cost of an Airbnb?” To answer our question, we’ll need a dataset that has information about many Airbnbs. We have gotten this data downloaded for you from Kaggle. 47.0.1 Starting up this project Go to the DataTrail workspace. Return to your own DataTrail_Projects project in RStudio. For this project, go to the 04_Plot_the_Data folder. Click on the file airbnb_project.Rmd to open this file. 47.0.2 Your objectives! To complete this project you’ll need to do a few things within this file. Go through this notebook, reading along. Fill in empty or incomplete code chunks when prompted to do so. Run each code chunk as you come across it by clicking the tiny green triangles at the right of each chunk. You should see the code chunks print out various output when you do this. At the very top of this file, put your own name in the author: place. Currently it says \"DataTrail Team\". Be sure to put your name in quotes. In the Conclusions section, write up responses to each of these questions posed here. When you are satisfied with what you’ve written and added to this document you’ll need to save it. In the menu, go to File &gt; Save. Now the nb.html output resulting file will have your new output saved to it. Open up the resulting airbnb_project.nb.html file and click View in Web Browser. Does it look good to you? Did all the changes appear here as you expected. Upload your Rmd and your nb.html files to your assignment folder (this is something that will be dependent on what your instructors have told you – or if you are taking this on your own, just collect these projects in one spot, preferably a Google Drive)! Pat yourself on the back for finishing this project! "],["getting-statistics.html", "Chapter 48 Getting Statistics 48.1 Learning Objectives", " Chapter 48 Getting Statistics 48.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Translate your data science questions into statistical tests Interpret statistical output toward the conclusion of your question Understand the strengths and limitations of the most common statistical tests "],["translating-questions-to-data-science-questions.html", "Chapter 49 Translating Questions to Data Science Questions 49.1 Getting Specific with Questions 49.2 Translating Our Questions", " Chapter 49 Translating Questions to Data Science Questions The approach to data analysis that we prefer is “problem forward, not solution backward”. The main point of this approach is to start with the question that you want to ask before you look at the data or the analysis. In some cases, the question that you want to answer will be a question driven by your curiosity. For example, you may be interested in a question about your fitness. You could collect data using a Fitbit and the MyFitnessPal app. Or you may have a question about what kind of songs you like best using data from your Spotify profile. You might also be interested in where the potholes are most common in your city. You could collect information from your city’s open data website. Another really common situation is that someone else is coming up with the question. When you are working as a data scientist this might be the marketing team, an executive, or an engineer who has a question that they would like to answer with the data. You might be asked to categorize photos on a website like Airbnb. They might bring you the question, the data, or both. Part of your job as a data scientist is to translate general questions into data science questions. 49.1 Getting Specific with Questions The first step in translating a general question into a data science question is to make it as concrete as possible. For example here are some generic questions you might be interested in: When I run more do I lose weight? Are customers more likely to click on ads with puppies? Do I need to take an umbrella with me when I leave the house today? These questions are interesting but they aren’t very specific. This is how most good data science projects start. To make a question more concrete you need to think about the data you would use to answer the question. This could either be data that you have or data that you think you could find. For each of these questions you need to ask these questions: What or who am I trying to understand with data? What measurements do I have on those people or objects that help me answer the question? How do the data I have limit the type of question I can answer? What is the type of data science question we are trying to answer? What or who am I trying to understand with data? The first question is focused on figuring out who or what you are trying to study. In the world of statistics this is sometimes called the “population” you are trying to study. When you ask a question it is best to be as specific as possible about what you are trying to study. A good way to be specific is to imagine the individual people or objects you are going to measure data on. Realistically, what is the group that you have or will collect data from? What measurements do I have on those people or objects that help me answer the question? The second question focuses on figuring out which variables are measured or will be measured in the data that you have. We have discussed previously about all the different potential data types you might have, including standard quantitative or qualitative data, text, images, or videos can be data. When answering this question it helps to be specific. For example, unstructured text from a social media post may not be helpful, but words and labels for the words in that post may be the data that you are looking for. How do the data I have limit the type of question I can answer? The third question is critical for being careful in a data analysis. When you use the problem forward approach, you might start with a general question. But it might not be possible to answer that question with data we have. For example, it may be difficult to study directly the way that cigarette smoke affects children since most children don’t smoke. You might have to change your question to studying the way that second-hand smoking affects children or the way that parents smoking habits affect children. A key part of translating a general question into a data science question is identifying these limitations. What is the type of data science question we are trying to answer? The fourth question is focused on figuring out what type of analysis you are doing. We introduced the flowchart for defining the question type in an earlier lesson. The key questions to ask are how the data are summarized, what are the goals you are trying to achieve, and what does success for your analysis look like? One of the most common errors that people make in doing a data analysis is to answer the wrong type of data science question. 49.2 Translating Our Questions Let’s try this approach out on a couple of made up examples and a real example to help you understand how to translate general questions into data science questions. When I run more do I lose weight? What or who am I trying to understand with data? I’m trying to understand something about only myself, not about others. What measurements do I have on those people or objects that help me answer the question? I have data on how many steps I take with Fitbit and measure my weight with a scale. How do the data I have limit the type of question I can answer? I only have data on me. I only have measurements on my weight every day and I need to summarize my Fitbit data to understand my runs, but won’t have information on whether I ran up and down hills or any information on my diet. What is the type of data science question we are trying to answer? In this case we are looking for a relationship between two variables that we measured. We’re only using the data we have so it is an exploratory analysis. When I run more do I lose weight? Are customers more likely to click on ads with puppies? What or who am I trying to understand with data? I’m trying to understand something about customers. I would need to figure out which customers. Customers trying to buy motorbikes might be different than customers going to Pets.com. We’d have to ask further questions to figure out which customers we are talking about. What measurements do I have on those people or objects that help me answer the question? Suppose we have all the data from a set of customers who visited a website for buying dog food for a single day. Some of the customers saw a puppy ad and some didn’t. We also have data on how much dog food they bought. How do the data I have limit the type of question I can answer? I only have data on a single website and only on a single day. So I might not be able to say things about other websites or other days. What is the type of data science question we are trying to answer? In this case we are looking for a relationship between two variables and trying say something about all the customers for a website. So this is an inferential analysis. Are customers more likely to click on ads with puppies? Do I need to take an umbrella with me when I leave the house today? What or who am I trying to understand with data? I’m trying to predict the weather in my hometown on a particular day so I know whether to take an umbrella. What measurements do I have on those people or objects that help me answer the question? I could take predictions from different weather services, look out the window and see if it is cloudy, or go out and feel if it is humid outside. To build my prediction I could collect these measurements for a year and also record whether I needed an umbrella that day. How do the data I have limit the type of question I can answer? I only have data on my hometown, I’ve only collected data from a few weather prediction services, and the data are only collected over one year. So it might be hard to say things for other people, other places, or other times. What is the type of data science question we are trying to answer? In this case we are looking to use historical data to predict something about a single day. So this is a prediction analysis. Do I need to take an umbrella with me when I leave the house today? 49.2.1 A real example Let’s practice translating questions to data science questions through an example. We briefly mentioned this example in Forming Questions section. The analysis is based on data scientist Dimiter Toshkov’s blog. Dimiter Toshkov’s IMDB Blog The blog author wanted to optimize his movie watching time by not watching movies which he would not enjoy. His model used a combination of metadata (data about data) such as movie title, director, duration, year of release, genre, IMDb rating, and a few other less interesting variables. What or who am I trying to understand with data? We are trying to predict the author’s movie score based on the inputs from IMDb: Will I like a movie? I will base it on a new score. The new score combines the official rating but also the other input variables as well. What measurements do I have on those people or objects that help me answer the question? Many data we generate has what is called metadata associated with it, information about the data itself. When you watch a movie on a streaming platform, the bulk of the data will be the video file, but there are also other characteristics surrounding that file. The genre, country of origin, language, duration, and year are all metadata which hold a huge amount of information for the budding data scientist. These are all input variables into our model, but we also require an output variable; what we are trying to predict. We need a sample of data which as both the input and output data, where the latter is the personal score of the movie. Once we have these 2 parts we can use them to train out model. If the model is working well, it means we can take a new movie, unseen by us, and predict what we will think the score would be if we watched it. That is the billion dollar question that the movie streaming industry is trying to solve today! How do the data I have limit the type of question I can answer? The main limitation here is that the user needs to rate a number of movies before we can even start this process. The other limitation is that we are only trying to predict the personal score. This is okay because we aren’t trying to predict any of the other variables (for example, we don’t care about predicting the year a movie came out). What is the type of data science models are we using? This type of analysis can be broadly labelled as linear regression, where we think there is a steady and constant relationship between 1 or more variables, and the ideal solution is a straight line through it. This is usually the first port of call for most data scientists as it accomplishes 2 things: It is quick and easy to implement It will show if a simple relationship is sufficient to answer the problem at hand The simple approach To further dive into this problem, the author subsequently adds more and more complexity to his approach. He initially just started by comparing IMDb ratings to his own, this referred to as a univariate or simple linear regression. The next step to improve the prediction is to add more variables to the model. If you are using many variables (year, genre, director), to predict your own score this is called multiple linear regression. Multiple linear regression Analysis The author is also very aware that this analysis introduces selection bias, more specifically survivorship bias. Any movie made before the 1990s that he has watched has likely survived the test of time, so older movies in this sample appear to have higher scores, but it’s unlikely that a random movie from the 1970s would be any better than a random movie from the 2010s. 49.2.2 Summary In conclusion, the author has demonstrated how you can use regression to try to build a model to predict an outcome, in this case, movie ratings. Another critical point to extract from this example is how valuable it can be to have the thought process of an analysis written out, and available for others to see and give feedback on. The author wrote this up in a blog post for easy accessibility. "],["identifying-data.html", "Chapter 50 Identifying Data 50.1 The Data Science Question 50.2 The Perfect Dataset: The Data We Want 50.3 The Data We Have 50.4 The Data We Can Get (Easily) 50.5 Data Collection 50.6 The Data We Can’t Get 50.7 Questions to Ask Ourselves 50.8 Are the Data We Have Good Data? 50.9 A case study: Why were polls so wrong about the 2016 US election? 50.10 Summary 50.11 Closing notes", " Chapter 50 Identifying Data After identifying your problem and transforming it into a data science question, your next step is to identify and ensure that you have access to the data necessary to answer the question. However, good data can be hard to come by. Depending on your question, it can be costly or simply infeasible to obtain the data you need. For instance, consider wanting to know the answer to the question “Does money make people happy?” One way to get data for your problem is to give people cash and to collect data from those same individuals later to see if the money they received made happier. First, this isn’t a cheap experiment, so you may not have the money to do this. Second, how would you want to measure happiness? You can’t just step on a scale and measure someone’s happiness. If you had each individual report their happiness on a scale of 1-10, one person may rate their happiness at an 8. Another person, who could be just as happy as the person who rated their happiness at an 8 may rate their happiness at a 6. You can see quickly how getting the right data to answer this question may not be so simple. Does money make people happy? In another situation, what if you are interested in answering the question “Does texting while driving causes accidents?” It wouldn’t be ethical to run a study and tell people to text while they’re driving and then determine if those who texted had more care accidents. It’s not okay to design experiments that knowingly put participants at risk of harm. Does texting while driving cause accidents? Thus, although each of us generate data daily through the apps we use or through our activity on social media platforms, often the data most easily available are not the data we need to answer the questions we’re most interested in answering. In this lesson we’ll discuss how to determine whether or not you have the data you need to answer the question you have, limitations of the data you have, considerations to make when you don’t yet have the data you need, and what to do to get the data you need. 50.1 The Data Science Question In the last chapter we discussed how to take a problem you’re interested in understanding better and turning it into a data science question. Specifically, the last lesson looked at Predicting movie ratings with IMDb data and R by Dimiter Toshkov. Dimiter Toshkov set out to answer the specific question “How do we predict the author’s movie score based on the inputs from IMDb?” Before answering the question, Dimiter Toshkov determined what/whose data would need to be collected, what measurements he would need, the limitations to the analysis, and the type of analysis he would use. In this lesson, we’re going to really focus on the second and third questions there. What data do you need to answer your data science question and what limitations do these data have? We’ll focus on the details of how to get the data necessary to answer the question of interest. We’ll walk through this process again using the Dimiter Toshkov’s blog as an example. IMDB 50.2 The Perfect Dataset: The Data We Want Once you determine your data science question of interest, it’s helpful to envision the perfect dataset. This dataset likely doesn’t exist in the real world, but our goal will be to get data that match this optimal dataset as closely as possible. Thus to answer the question What variables from IMDB predict the author’s movie ratings? We would ideally want a dataset where each row is a different movie with each row containing information on the following variables: rating – our own rating for the movie imdb – the IMDb movie rating genre – the genre of the movie director – Director of the movie year – Year of release of the movie Having all these data in this format would enable us to answer the question of interest! 50.3 The Data We Have But, the data we have are typically not exactly the data we want. To make this analysis more robust and more generic, we would need to have the personal rating of a lot more movies (both number and variation) but also a lot more people. This would allow us to build a customized model for each person based on their preferences. Taking this analysis further, we could gather more information about the viewer (age, sex, location, etc) and try to build a customized rating model for that profile of person. Nevertheless, Dimiter did have information on this set of movies. While not exactly the data he would have likely liked to have, he was still able to use these data while noting the limitations of the data in his blog post! 50.4 The Data We Can Get (Easily) In this case, Dimiter had enough data to answer the question he was interested in. However, often you may not have the data you need right from the beginning. After looking at the data you have, it’s worth considering whether or not there are data that you need that you can easily get access to. Depending upon your data science question, you may consider obtaining data from any of the sources discussed in the Getting Data course. For example, you could look at data from: Government Data APIs Open Datasets (such as those on Kaggle or data.world) your company These are all examples of places where it’s usually pretty easy to obtain data. And, ideally, whenever possible, you’ll want to work with raw data – data that haven’t yet been cleaned (i.e. outliers have not yet been removed) so that you know you have the most complete dataset from the start. You can then clean and wrangle the data after getting the raw data. However, what if the data you want don’t exist in any of these places? When the data aren’t available, it’s up to you (your team, your boss, or your company) to collect the data. 50.5 Data Collection In practice, data are defined by how they are collected. While it may not always be up to you to determine how the data are going to be collected, it’s important to understand the different ways in which data are collected: Observational data are collected from a sample of the bigger population of interest. For instance, data on household expenditure in the U.S. are usually collected from a sample of American household. Observational data can be in the following forms: Cross-sectional data are collected from a sample of the bigger population of interest at a specific point in time. Longitudinal or panel data are collected from a sample of the population of interest but at multiple points in time. For instance, data on the effect of peer effect in school on life outcomes are measured at multiple points in the lives of the individuals. In longitudinal or panel data, the sample does not change from one point in time to another. For example, if data is collected on 1000 individuals in the year 2010, in 2015 the data is likely collected from the same 1000 individuals. Experimental data are collected through a randomized experiment. In an experiment, the researchers divide the sample into two or more (that is chosen from the bigger population of interest) and assign the treatment (let’s say a vaccination) to one group while the group receives no treatment. The outcomes are then observed and compared. This differs from observational data, which involves collecting data without changing any of the existing conditions. If you are able to generate the dataset you need or collect the data you wanted to answer your question, then you’re all set. But, what if there are limitations to you getting those data? 50.6 The Data We Can’t Get Often there will be data you would love to have to answer your data science question, but you won’t be able to get those data. There are a number of limitations to data collection. We’ll discuss a number of common ones here. 50.6.1 Limited Resources In the introduction to this lesson, we discussed an experiment where we would give people money and measure their happiness. However, if we didn’t have the money to carry out this experiment, we wouldn’t be able to run this experiment and thus, money would be a limited resource. However, money is not the only limiting factor. What if you need to know the answer to your question next month but it would take you a year to collect the necessary data? In this case time would be a limited resource. Alternatively, what if another company had the data you needed, but the company you work for is a competitor. In this case, they likely wouldn’t share the data with you. Access to data can also be limiting. 50.6.2 Ethical Limitations In the introduction to this lesson, we also talked about the case where we wanted to know if texting increases the number of accidents. We mentioned that we wouldn’t be able to carry out the experiment of telling people to text when they’re driving and then seeing if those people get into more accidents due to ethical limitations. It is unethical to collect data knowingly putting individuals in harms way. However, there are more nuanced situations as well. What if you want to collect data about reproductive health? You may want to know everything about each mother’s medical history, including very sensitive and personal information. For example, you may want to know whether or not each individual in your dataset has ever had an abortion. Asking sensitive and invasive questions should only be asked when absolutely necessary. Further, this data will always have to be stored securely, and can only be asked with approval of and oversight committee (such as an Institutional Review Board). Thus, while it may be helpful to have information about certain variables, it may be inappropriate or too invasive to obtain this information. 50.6.3 Security Mentioned briefly above, but to really spell it out here: not all data are publicly-available and not all data should be publicly-available. Personal data and data that contain sensitive information must be secured properly and cannot be shared freely. While the data you want may exist somewhere, you may not have access to it, nor will you be granted access to it for security reasons. In these cases, you will not be able to use these data. 50.7 Questions to Ask Ourselves If you run into problems obtaining or collecting the data you need to answer your question, there are a number of questions to ask yourself: Can I change the question in such a way that is still interesting but that can be answered with the data that I do have? If I use the data to which I have access, is the project still worth doing? Will the project be feasible if I change it? If it is still feasible, I’ll need to rework the question and redesign my data collection plan. What will that look like? In the case where you decide to change the question and determine that the project is still worth doing at this point, it’s important to go back through the exercise of determining what data you would optimally have, what you do have, and what you can get easily to ensure that you won’t be wasting your time by moving forward with the project. However, in some cases, you’ll have a super interesting question, but you won’t be able to do the project given the data you have. In these situations, save the idea for a later point when you may have the resources or access to the data you need, and then move onto the next one. Everyone has had to abandon projects in their career, and it’s OK to move on. It’s much better to leave a project behind in the planning stages than to spend months or years on a project/analysis that was doomed from the start! 50.8 Are the Data We Have Good Data? All that said, once you have your data science question and the data you want to use, you’ll need to determine whether the data are good enough. If they’re not, before you spend hours on it, quit and look for other sources of data. 50.8.1 The need to wrangle For example, data downloaded from other resources often need to be cleaned and wrangled. (Note: If the data that you obtain, however, are pre-processed, make sure you understand how the processing was done!) To clean your data, you’ll always want to record the steps you take to reformat the data. We suggest the following steps to check and tidy the data for your analysis. Make sure: Each variable forms a column Each observation forms a row Each table/file stores data about one kind of observation (e.g. people/hospitals). If variables are collected from multiple sources, they are merged properly Column names are easy to use and informative Apparent mistakes in the data have been removed Missing values are formatted uniformly and correctly Variable values are internally consistent Appropriate transformed variables have been added These are the concepts previously discussed in the Data Tidying lessons in an earlier chapter, so many of them should be familiar. Cleaning data to make sure the dataset is in a tidy format, that the variables are all appropriately-named, and that the values within each variable are as you expect them to be is an important step in determining whether or not the data you have will be useful in answering your data science question. These topics will be further discussed in the Descriptive Analysis and Exploratory Analysis lessons in the later lessons in this course. 50.8.2 When data aren’t good That all said, let’s assume you have the dataset that contains the variables you are looking for, and it is tidy and ready to go for your analysis. It’s always nice to step back to make sure the data is the right data before you spend hours and hours on your analysis. So, let’s discuss some of the potential and common issues people run into with their data. 50.8.2.1 Number of observations is too small It happens quite often that collecting data is expensive or not easy. For instance, in a medical study on the effect of a drug on patients with Alzheimer disease, researchers will be happy if they can get a sample of 100 people. These studies are expensive, and it’s hard to find volunteers who enroll in the study. It is also the case with most social experiments. While data are everywhere, the data you need may not be. Therefore, most data scientists at some point in their career face the curse of small sample size. Small sample size makes it hard to be confident about the results of your analysis. So when you can, and it’s feasible, a large sample is preferable to a small sample. But when your only available dataset to work with is small you will have to note that in your analysis. Although we won’t learn them in this course, there are particular methods for inferential analysis when sample size is small. 50.8.2.2 Dataset does not contain the exact variables you are looking for In data analysis, it is common that you don’t always have what you need. You may need to know individuals’ IQ, but all you have is their GPA. You may need to understand food expenditure, but you have total expenditure. You may need to know parental education, but all you have is the number of books the family owns. It is often that the variable that we need in the analysis does not exist in the dataset and we can’t measure it. In these cases, our best bet is to find the closest variables to that variable. Variables that may be different in nature but are highly correlated with (similar to) the variable of interest are what are often used in such cases. These variables are called proxy variables. For instance, if we don’t have parental education in our dataset, we can use the number of books the family has in their home as a proxy. Although the two variables are different, they are highly correlated (very similar), since more educated parents tend to have more books at home. So in most cases where you can’t have the variable you need in your analysis, you can replace it with a proxy. Again, it must always be noted clearly in your analysis why you used a proxy variable and what variable was used as your proxy. 50.8.2.3 Variables in the dataset are not collected in the same year Imagine we want to find the relationship between the effect of cab prices and the number of rides in New York City. We want to see how people react to price changes. We get a hold of data on cab prices in 2018, but we only have data on the number of rides from 2015. Can these two variables be used together in our analysis? Simply, no. If we want to answer this question, we can’t match these two sets of data. If we’re using the prices from 2018, we should find the number of rides from 2018 as well. Unfortunately, a lot of the time, this is an issue you’ll run into. You’ll either have to find a way to get the data from the same year or go back to the drawing board and ask a different question. This issue can be ignored only in cases where we’re confident the variables does not change much from year to year. 50.8.2.4 Dataset is not representative of the population that you are interested in You will hear the term representative sample, but what is it? Before defining a representative sample, let’s see what a population is in statistical terms. We have used the word population without really getting into its definition. A sample is part of a population. A population, in general, is every member of the whole group of people we are interested in. Sometimes it is possible to collect data for the entire population, like in the U.S. Census, but in most cases, we can’t. So we collect data on only a subset of the population. For example, if we are studying the effect of sugar consumption on diabetes, we can’t collect data on the entire population of the United States. Instead, we collect data on a sample of the population. Now, that we know what sample and population are, let’s go back to the definition of a representative sample. A representative sample is a sample that accurately reflects the larger population. For instance, if the population is every adult in the United States, the sample includes an appropriate share of men and women, racial groups, educational groups, age groups, geographical groups, and income groups. If the population is supposed to be every adult in the U.S., then you can’t collect data on just people in California, or just young people, or only men. This is the idea of a representative sample. It has to model the broader population in all major respects. We give you one example in politics. Most recent telephone poles in the United States have been bad at predicting election outcomes. Why? This is because by calling people’s landlines you can’t guarantee you will have a representative sample of the voting age population since younger people are not likely to have landlines. Therefore, most telephone polls are skewed toward older adults. Random sampling is a necessary approach to having a representative sample. Random sampling in data collection means that you randomly choose your subjects and don’t choose who gets to be in the sample and who doesn’t. In random sampling, you select your subjects from the population at random like based on a coin toss. The following are examples of lousy sampling: A research project on attitudes toward owning guns through a survey sent to subscribers of a gun-related magazine (gun magazine subscribers are not representative of the general population, and the sample is very biased) A research study on school meals and educational outcomes done in a neighborhood with residents mainly from one racial group (school meal can have a different effect on different income and ethnic groups) A researcher polls people as they walk by on the street. A TV show host asks the program viewers to visit the network website and respond to a poll. With this logic, most online surveys or surveys on social media has to be taken with a grain of salt because not members of all social groups have an online presentation or use social media. The moral of the story is that always think about what your population is. Your population will change from one project to the next. If you are researching the effect of smoking on pregnant women, then your population is, well, pregnant women (and not men). After you know your population, then you will always want collect data from a sample that is representative of your population. Random sampling helps. And lastly, if you have no choice but to work with a dataset that is not collected randomly and is biased, be careful not to generalize your results to the entire population. If you collect data on pregnant women of age 18-24, you can’t generalize your results to older women. If you collect data from the political attitudes of residents of Washington, DC, you can’t say anything about the whole nation. 50.8.2.5 Some variables in the dataset are measured with error Another curse of a dataset is measurement error. In simple, measurement error refers to incorrect measurement of variables in your sample. Just like measuring things in the physical world comes with error (like measuring distance, exact temperature, BMI, etc.), measuring variables in the social context can come with an error. When you ask people how many books they have read in the past year, not everyone remembers it correctly. Similarly, you may have measurement error when you ask people about their income. A good researcher recognizes measurement error in the data before any analysis and takes it into account during their analysis. 50.8.2.6 Variables are confounded What if you were interested in determining what variables lead to increases in crime? To do so, you obtain data from a US city with lots of different variables and crime rates for a particular time period. You would then wrangle the data and at first you look at the relationship between popsicle sales and crime rates. You see that the more popsicles that are sold, the higher the crime rate. popsicles and crime rate Your first thought may be that popsicles lead to crimes being committed. However, there is a confounder that’s not being considered! We will see in detail what confounders are in the lesson on Inferential Analysis but in short confounders are other variables that may affect our outcome but are also correlated with (have a relationship with) our main variable of interest. In the popsicle example, temperature is an important confounder. More crimes happen when it’s warm out and more popsicles are sold. It’s not the popsicles at all driving the relationship. Instead temperature is likely the culprit. temperature is a confounder Therefore, if we’re really interested in what increases crime rate, we should also consider the temperature. It’s important to understand the relationship between the variables in your dataset. This will be further discussed in the Exploratory Analysis lesson. 50.9 A case study: Why were polls so wrong about the 2016 US election? The results of the 2016 US presidential election came as a surprise to a lot of the media outlets and poll experts. Most polls consistently projected that Hillary Clinton would defeat Donald Trump. Election forecasters put Clinton at a winning position at a chance as high as 70-99 percent. Polls are usually trustworthy, but what happened in 2016? Why were polls so wrong about the 2016 US election? We don’t know the whole story yet. But for starters, the one issue everyone agrees is the data used in the prediction algorithms were not wholly appropriate. To be more specific, the problem was what is called the nonresponse bias. Nonresponse bias is the bias the sample caused by a particular section of the population systematically refusing to answer to the poll or survey. The consequence of nonresponse bias is a sample that is not representative of the population as we discussed above. (For instance, most rich people refuse to announce their income in surveys and as a result, there is nonresponse bias in most surveys of income). Pollsters agree that less educated voters, who constituted a bulk of Trump voters, are harder to reach. This meant that this pro-Trump segment of the population was missed in the polls during the weeks leading up to the election. Pew Research also points to dishonesty in responses to polls (people not being truthful about who they were planning to vote for) as another reason why polls failed to predict the outcome of the election accurately. While it’s always easier to look back at a failed analysis and determine what went wrong then to identify the issues up front, researchers would have benefited from determining if the data they had were the most appropriate for answering their question of interest. 50.10 Summary Once you’ve honed in on a good data science question, it’s important to determine if the data available to you are the data you need. To do this: Imagine the optimal dataset Determine what data you have Identify the data you can (easily) get Figure out limitations in the data you have Are the limitations so great that you need to re-work your question? If yes, start over and form a new data science question, then return to step 1 If not, continue with your analysis, but note all limitations in your analysis Explore and wrangle the dataset Analyze the data and answer the data science question! All in all, the data and how they’re collected matters. There is a term in statistics that says “garbage in, garbage out,” which means poor quality input will always produce poor output. A data analysis that is based on faulty data produces faulty results. Be cautious about the data that you use in your analysis, ensure that the data you need to answer your question of interest are the data you have, and always tell your listeners and readers about the deficiencies of your data. After all, you will know your data better than anyone else! 50.11 Closing notes It’s worth mentioning that the quality of the dataset you are working with doesn’t reflect the quality of data scientist you are. Data will always have limitations and that is not only okay but to be expected! The job of a data scientist is to be honest and transparent about the data’s limitations. Thoughtfulness and careful investigation into the data’s limitations are powerful methods for quality data science. "],["in-practice-using-stats.html", "Chapter 51 In Practice Using Stats 51.1 Tip number 1: Always be looking at your data 51.2 Tip number 2: Dig into weirdness 51.3 Tip number 3: Let the data inform you 51.4 How to find out how much data is missing 51.5 How do I know what test to use?", " Chapter 51 In Practice Using Stats In the upcoming chapters we will introduce you to a number of statistical terms and test types. In general, we wouldn’t stress memorizing specifics of these tests, but instead trying to get an intuitive sense for how to use and interpret the stats in the context of your statistical questions. In this chapter we are going to discuss some general strategies for how to start to apply the power of stats to your dataset. 51.1 Tip number 1: Always be looking at your data Data in real life is messy. You know this because we had a whole section about cleaning data! But even after your data is “tidy” in the sense that it is ready to be used, it still may have other oddities. We’ve discussed that data science is all about questions! This means that a new data science endeavor often starts with a lot of unknowns. There are: Knowns! - These are the things you know. Known unknowns - these are the things that you know you don’t know – like the answers to your data science question Unknown unknowns - These are the things that you don’t know that you don’t know – these can be very scary if they affect your data! So how do we make unknown unknowns a little more known? We have to do lots of exploring! There are a few initial explorations that are always a great idea to do with your data. As you work on a dataset, you may want to repeat these whenever you apply a new step or transformation to your data. Practical tips for looking at your data Look at your data in the Environment tab or by printing it out. Make a density plot or histogram to look at the shape of your data - more on this in a second! Run the summary() function on your data. Make sure to look into the documentation for functions that you use to transform your data so you understand what you are doing to it. As you do these things with your data, you may find some “weirdness” which brings us to our next tip. 51.2 Tip number 2: Dig into weirdness Often after you have done some initial exploration of your data or as you are generally working on your analysis, you may see things that make you go “huh?” or are generally just “weird”. One of the most valuable skills of data scientist can have is digging into that weirdness. As you continue to work in the field of data science you will hone your skills of identifying and digging into weirdness. Over time you will get a “spidey sense” and learn what weirdness is worth digging into and what weirdness is actually quite normal. Digging into peculiar things can lead making unknown unknowns more known. They sometimes may shift the entire strategy of your analysis! And while this may sound like it will add your work, it is how the best data science is done! At this point, we should let you know we’ve been somewhat lying to you at the beginning of each section when we’ve shown you this map: In reality, good data science doesn’t happen so step like. The best data science is a bit messier than this. Original questions lead to new questions; and some pursuits are dead ends; and whole new findings and cool things we never dreamed of sometimes pop up out of nowhere! Meaning that data science is never “write code once and done”, it is “write code, run it, rewrite it, look at your data, learn something, rewrite it again, have new questions, etc, etc, etc”. This leads us to another point. That is, the best data science is done iteratively – you won’t write your best analysis in one sitting and never come back to it. Your best analyses will be things you work on over time to incrementally improve and look at. Practical tips for digging into weirdness: Look into weird density plot shapes and points in your data (outliers) Look into results and outcomes that are unexpected - are these real or a byproduct of a mistake somewhere? Look out for results and outcomes that are too perfect – real life and real data are rarely perfect Never assume why something is the way it is without proving it to yourself by digging into it more! Ultimately, when you see weird things prove to yourself that the results are what they seem. 51.3 Tip number 3: Let the data inform you After following our first two tips, you should have a generally good sense of what your data look like and what kind of weirdness exists. Now you will be ready to use what you’ve learned about your data to inform you how to properly handle them! We discussed in the previous chapter how to translate your data science questions into a stats test. There’s a second part of this consideration, which is taking into account how your data are behaving to know what kinds of stats and questions are appropriate. Here’s some common things your data might tell you and how you might find that out: For the upcoming examples we are going to use this datasets that are included in the ggplot2 package. library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 51.4 How to find out how much data is missing Datasets often having missing data points because the collection process can be messier than we hope. There can be very good and appropriate reasons to have NAs in the data. Indeed it can often be the case that NA is a more appropriate way to note a data point than putting some other value. But, before you start doing Code example for finding missing data (this is not the only way to do this) Let’s use and set up the Texas Housing sales dataset from ggplot2. tx_df &lt;- ggplot2::txhousing head(tx_df) ## # A tibble: 6 x 9 ## city year month sales volume median listings inventory date ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abilene 2000 1 72 5380000 71400 701 6.3 2000 ## 2 Abilene 2000 2 98 6505000 58700 746 6.6 2000. ## 3 Abilene 2000 3 130 9285000 58100 784 6.8 2000. ## 4 Abilene 2000 4 98 9730000 68600 785 6.9 2000. ## 5 Abilene 2000 5 141 10590000 67300 794 6.8 2000. ## 6 Abilene 2000 6 156 13910000 66900 780 6.6 2000. If you know that all your missing data are appropriately labeled as NA you can just use something like: summary(tx_df) ## city year month sales ## Length:8602 Min. :2000 Min. : 1.000 Min. : 6.0 ## Class :character 1st Qu.:2003 1st Qu.: 3.000 1st Qu.: 86.0 ## Mode :character Median :2007 Median : 6.000 Median : 169.0 ## Mean :2007 Mean : 6.406 Mean : 549.6 ## 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.: 467.0 ## Max. :2015 Max. :12.000 Max. :8945.0 ## NA&#39;s :568 ## volume median listings inventory ## Min. :8.350e+05 Min. : 50000 Min. : 0 Min. : 0.000 ## 1st Qu.:1.084e+07 1st Qu.:100000 1st Qu.: 682 1st Qu.: 4.900 ## Median :2.299e+07 Median :123800 Median : 1283 Median : 6.200 ## Mean :1.069e+08 Mean :128131 Mean : 3217 Mean : 7.175 ## 3rd Qu.:7.512e+07 3rd Qu.:150000 3rd Qu.: 2954 3rd Qu.: 8.150 ## Max. :2.568e+09 Max. :304200 Max. :43107 Max. :55.900 ## NA&#39;s :568 NA&#39;s :616 NA&#39;s :1424 NA&#39;s :1467 ## date ## Min. :2000 ## 1st Qu.:2004 ## Median :2008 ## Mean :2008 ## 3rd Qu.:2012 ## Max. :2016 ## You’ll see this prints out the summary for each variable in this data frame, including the number of NAs. However, if your missing data are not appropriately labeled NA then you will want to convert them using code described in this article. For more on finding missing values. 51.4.1 How to find if you have outliers Outliers are data points that are extreme. They can throw off whole analyses and bring you to the wrong conclusions. You have outliers or weird samples in your data – you may want to try removing these if appropriate and re-running the test you were using. Code example for finding outliers (this is not the only way to do this) Let’s use and set up the Fuel economy dataset from ggplot2. cars_df &lt;- ggplot2::mpg head(cars_df) ## # A tibble: 6 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… We can make a boxplot with base R. boxplot(cars_df$hwy, ylab = &quot;hwy&quot;) The points in this boxplot are points you would want to look into as being outliers! You could see what these points are for sure by using dplyr::arrange() or any other number of ways. cars_df %&gt;% dplyr::arrange(dplyr::desc(hwy)) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 volkswagen jetta 1.9 1999 4 manual… f 33 44 d compa… ## 2 volkswagen new be… 1.9 1999 4 manual… f 35 44 d subco… ## 3 volkswagen new be… 1.9 1999 4 auto(l… f 29 41 d subco… ## 4 toyota corolla 1.8 2008 4 manual… f 28 37 r compa… ## 5 honda civic 1.8 2008 4 auto(l… f 25 36 r subco… ## 6 honda civic 1.8 2008 4 auto(l… f 24 36 c subco… ## 7 toyota corolla 1.8 1999 4 manual… f 26 35 r compa… ## 8 toyota corolla 1.8 2008 4 auto(l… f 26 35 r compa… ## 9 honda civic 1.8 2008 4 manual… f 26 34 r subco… ## 10 honda civic 1.6 1999 4 manual… f 28 33 r subco… ## # … with 224 more rows See this guide for more code and tips on how to detect outliers in R 51.4.2 How to know if your data is underpowered for your question In order to answer particular questions with a dataset, you need to have enough data in the first place! If you don’t have enough data that means you are underpowered. This may happen if you have a lot of missing data, a painfully small dataset, or the effect you are looking for is very small. In these cases you may need to find another dataset or see if the data collector can collect more data to add to this set. So how do you know if your dataset is underpowered? Code example exploring power (this is not the only way to do this) Let’s use and set up the Diamonds from ggplot2. diamonds_df &lt;- ggplot2::diamonds head(diamonds_df) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 Let’s say we are interested in seeing whether the carat is correlated with the price of the diamond. Before we test this, we may want to test the power of our dataset to detect this correlation. For this we will use the pwr.r.test() function from the pwr library. install.packages(&quot;pwr&quot;) ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) library(pwr) We have to tell it a few pieces of info: 1) How many samples do we have? 2) What correlation do we expect? and… 3) What significance level will we want (standard is to use 0.05 or 0.01). pwr.r.test(n = nrow(diamonds_df), # How many cases do we have r = cor(diamonds_df$carat, diamonds_df$price), sig.level = 0.01) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 53940 ## r = 0.9215913 ## sig.level = 0.01 ## power = 1 ## alternative = two.sided You’ll see this prints out a 1 for power. This dataset is not underpowered at all. Power is on a scale of 0 to 1. Where 0 means you don’t have the power to detect anything and 1 means you will absolutely see a significant result if there is one to be seen. But let’s look at a different hypothetical situation. Let’s say instead we only had 10 rows of data and the correlation we expected would be more like 0.3. pwr.r.test(n = 10, # How many cases do we have r = 0.3, sig.level = 0.01) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 10 ## r = 0.3 ## sig.level = 0.01 ## power = 0.03600302 ## alternative = two.sided Now this is telling us our power is very low – meaning even if our hypothesis is true, we don’t have enough data to see this correlation. See this chapter of a book for code and tips for how to know if your data are underpowered by doing a power analysis in R. 51.4.3 How to know how your data are distributed Perhaps you want to use a particular stats test, but when you read about this stats test, it has an assumption that the data are normally distributed – stats assumptions are really just requirements for using a method. So if something “assumes a normal distribution” it means in order to use the test on your data it has to be normally distributed. If you will be using a numeric variable for anything in your analysis it’s a very good idea to plot its density so you know what you are working with! Code example of looking at distributions Let’s return to the cars_df dataset we were looking at earlier. To make a density plot, we need to use geom_density() function. ggplot(cars_df, aes(x = cty)) + geom_density() We can see this looks like a fairly normal distribution – what does that mean? let’s discuss. 51.4.3.1 What does it mean to be “normally distributed?” How a dataset is distributed has to do with frequency of the data. So in the example below, we’ve made a probability density plot using ggplot2. See this article for details on making denisty plots. The higher the line is, the more probable that that value would occur. If your data plot looks like that normal bell-shaped curve, then you have “normally distributed” data. You will want to know what your data distribution looks like so you know what tests are appropriate. Let’s look at the distribution of a different variable, the sales in tx_df data: ggplot(tx_df, aes(x = sales)) + geom_density() ## Warning: Removed 568 rows containing non-finite values (stat_density). Is this normally distributed? There are a lot of values that are lower and only some that are higher. Looks pretty skewed. In this case, we probably don’t need to test these data, but we know they aren’t really normally distributed so we should keep that in mind. If we want a test rather than just using our eyes, we can use the ks.test() which asks for us “are my data normally distributed?” In this instance, we’ll use the iris dataset to test its normality for the variable Sepal.Width. shapiro.test(iris$Sepal.Width) ## ## Shapiro-Wilk normality test ## ## data: iris$Sepal.Width ## W = 0.98492, p-value = 0.1012 Because this p value reported is bigger than 0.05 we can consider Sepal.Width to be a normally distributed variable. One other important thing to note, if you have too small of a dataset, (say 30 or less) then you can never really consider your data normally distributed and they will always fail these normality tests. Read more about normal distributions here. There are formal ways to test for normality and these methods are described in this article that has code examples: How to Assess Normality in R In conclusion, density plots are a super handy tool to see what your data look like before you dive in to any other tests and analyses. These can inform you about what to do with your data. Practical tips for figuring out what to do with your data: Make density plots to visualize your data’s distribution. Google and find sources that discuss the problems you are seeing with your data. It is unlikely that the dataset you are working with is the only dataset that has this weirdness and others online may weigh in. Consult a more senior data scientist or statistician. Show them the the weirdness you see and ask them what they think of it and what they would do. Look for other data analysis examples online that resemble your data and its weirdness. Try to see what others did and if it makes sense to apply to your situation. 51.5 How do I know what test to use? We’re going to tell you about some common tests, but here’s a flow chart to help you get started. We advise generally having an idea of tests out there but not getting caught up in the minutia of every test. If you end up using a particular test on a set of data, then might be a good time to try to get a practical understanding of the test and how to interpret it but knowing everything about all statistical tests is just not practical and not a good use of your time. The important point about choosing a test is realizing that not all tests are appropriate. Indeed, you could use a lot of these tests for a particular dataset, but some tests may lead you to erroneous conclusions if the test is not meant to be used on data like what you are working with. And as we mentioned with the tips above, don’t be afraid to reach out to a statistician or more senior data scientist to help you choose what is appropriate! For this section, we’re going to borrow the handy cheatsheets and tables from this article by Rebecca Bevans. Don’t worry about memorizing the specifics of these tests, just generally understand this guide and how to use it and know you can come back to it for a reference. 51.5.1 Comparison tests These assume normality. 51.5.2 Regression tests We will talk more about regression in the upcoming chapters! 51.5.3 Correlation tests Highly similar to regression tests because they use a lot of the same math, correlation tests ask if two variables are related. 51.5.4 Nonparametric tests Remember how we briefly talked about data being normally distributed or not? Turns out all the tests we just mentioned above ONLY work for data that is normally distributed. Aka as statisticians like to say those tests “assume normality”. But if your data isn’t normal, you can still do things with it! There are non-parametric (things that don’t assume normality) equivalents you can use. 51.5.5 Additional Resources Which Statistical Test to Use? Follow This Cheat Sheet Demystifying Statistical Analysis 1: A Handy Cheat Sheet Open Case Studies, by Pei-Lun Kuo, Leah Jager, Margaret Taub, and Stephanie Hicks Health Expenditures Case Study Case Study on GitHub "],["descriptive-analysis.html", "Chapter 52 Descriptive Analysis", " Chapter 52 Descriptive Analysis The goal of descriptive analysis is to describe or summarize a set of data. Whenever you get a new dataset to examine, this is the first analysis you will perform. In fact, if you never summarize the data, it’s not a data analysis. Descriptive Analysis summarizes the dataset If you think of a dataset as the animal in the elephant in the middle of this picture, you doing a descriptive analysis are the blind monks examining every part of the elephant. Just like the blind monks who are examining each and every part of the elephant to understand it completely, the goal of a descriptive analysis is to understand the data you’re working with completely. examining a dataset Descriptive analysis will first and foremost generate simple summaries about the samples and their measurements to describe the data you’re working with. There are a number of common descriptive statistics that we’ll discuss in this lesson: measures of central tendency (eg: mean, median, mode) or measures of variability (eg: range, standard deviations or variance). This type of analysis is aimed at summarizing your dataset. Unlike analysis approaches we’ll discuss in coming lessons, descriptive analysis is not for generalizing the results of the analysis to a larger population nor trying to draw any conclusions. Description of data is separated from interpreting the data. Here, we’re just summarizing what we’re working with. Some examples of purely descriptive analysis can be seen in censuses. In a census, the government collects a series of measurements on all of the country’s citizens. After collecting these data, they are summarized. From this descriptive analysis, we learn a lot about a country. For example, you can learn the age distribution of the population by looking at U.S. census data. 2010 census Data broken down by age This can be further broken down (or stratified) by sex to describe the age distribution by sex. The goal of these analyses is to describe the population. No inferences are made about what this means nor are predictions made about how the data might trend in the future. The point of this (and every!) descriptive analysis is only to summarize the data collected. 2010 census Data broken down by age and sex In this lesson, we’ll discuss the steps required to carry out a descriptive analysis. As this will be the first thing you do whenever you’re working with a new dataset, it’s important to work through the examples in this lesson step-by-step. 52.0.1 How to Describe a Dataset When provided a new tabular dataset, whether it’s a CSV you’ve been sent by your boss or a table you’ve scraped from the Internet, the first thing you’ll want to do is describe the dataset. This helps you understand how big the dataset is, what information is contained in the dataset, and how each variable in the dataset is distributed. Descriptive Analysis For this lesson, we’re going to move away from the iris or mtcars dataset (since you probably already have a pretty good understanding of those datasets) and work with a dataset we haven’t used too much in this Course Set: msleep (from the ggplot2 package). This dataset includes information about mammals and their sleep habits. We’ll load that package in and assign the dataset to the object df: ## install and load package install.packages(&quot;ggplot2&quot;) library(ggplot2) ## assign to object `df` df &lt;- msleep Generally, the first thing you’ll want to know about your dataset is how many observations and how many variables you’re working with. You’ll want to understand the size of your dataset. You can always look at the Environment tab in RStudio Cloud to see how many observations and variables there are in your dataset; however, once you have many objects, you’ll have to scroll through or search this list to find the information you’re looking for. Environment tab To avoid having to do that, the simplest approach to getting this information is the dim() function, which will give you the dimensions of your data frame. The output will display with the number of rows (observations) first, followed by the number of columns (variables). ## determine the dimensions dim(df) dim() output 52.0.1.1 Variables There are additional ways to learn a bit more about the dataset using a different function. What if we wanted to know not only the dimensions of our dataset but wanted to learn a little bit more about the variables we’re working with? Well, we could use dim() and then use the colnames() function to determine what the variable names are in our dataset: ## determine variable names colnames(df) The output from colnames tells us that there are 11 variables in our dataset and lets us know what the variable names are for these data. colnames() output But, what if we didn’t want to use two different functions for this and wanted a little bit more information, such as what type of information is stored in each of these variables (columns)? To get that information, we’d turn to the function str(), which provides us information about the structure of the dataset. ## display structure str(df) str() output The output here still tells us the size of df and the variable names, but we also learn what the class of each variable is and see a few of the values for each of the variables. A very similar function to str() is the glimpse() function from the dplyr package. As you’ve been introduced to this function previously, we just wanted to remind you that glimpse() can also be used to understand the size and structure of your data frame ## install and load package install.packages(&quot;dplyr&quot;) library(dplyr) ## get a glimpse of your data glimpse(df) glimpse() output 52.0.1.2 Missing Values In any analysis after your descriptive analysis, missing data can cause a problem. Thus, it’s best to get an understanding of missingness in your data right from the start. Missingness refers to observations that are not included for a variable. In R, NA is the preferred way to specify missing data, so if you’re ever generating data, its best to include NA wherever you have a missing value. However, individuals who are less familiar with R code missingness in a number of different ways in their data: -999, N/A, ., or a blank space. As such, it’s best to check to see how missingness is coded in your dataset. A reminder: sometimes different variables within a single dataset will code missingness differently. This shouldn’t happen, but it does, so always use caution when looking for missingness. In this dataset, all missing values are coded as NA, and from the output of str(df) (or glimpse(df)), we see that at least a few variables have NA values. We’ll want to quantify this missingness though to see which variables have missing data and how many observations within each variable have missing data. To do this, we can write a function that will calculate missingness within each of our variables. To do this we’ll combine a few functions. In the code here is.na() returns a logical (TRUE/FALSE) depending upon whether or not the value is missing (TRUE if it is missing). sum() then calculates the number of TRUE values there are within an observation. We wrap this into a function and then use sapply() to calculate the number of missing values in each variable. The second bit of code does the exact same thing but divides those numbers by the total number of observations (using nrow(df). For each variable, this returns the proportion of missingness: ## calculate how many NAs there are in each variable sapply(df, function(x) sum(is.na(x))) ## calculate the proportion of missingness ## for each variable sapply(df, function(x) sum(is.na(x)))/nrow(df) missingness in msleep Running this code for our dataframe, we see that many variables having missing values. Specifically, to interpret this output for the variable brainwt, we see that 27 observations have missing data. This corresponds to 32.5% of the observations in the dataset. It’s also possible to visualize missingness so that we can see visually see how much missing data there is and determine whether or not the same samples have missing data across the dataset. You could write a function to do this yourself using ggplot2; however, Nicholas Tierney has already written one for you. He has written two helpful packages for exploratory and descriptive data analyses: naniar and visdat. For our purposes, we’ll just install and load naniar here; however, links to both have been included in the additional resources section at the end of this lesson. We recommend looking through the examples provided in the documentation to see additional capabilities within these packages. ## install naniar package install.packages(&quot;naniar&quot;) library(naniar) ## visualize missingness vis_miss(df) vis_miss() output Here, we see the variables listed along the top with percentages summarizing how many observations are missing data for that particular variable. Each row in the visualization is a different observation. Missing data are black. Non-missing values are in grey. Focusing again on brainwt, we can see the 27 missing values visually. We can also see that sleep_cycle has the most missingness, while many variables have no missing data. The relative missingness within a dataset can be easily captured with another function from the naniar package: gg_miss_var(): ## visualize relative missingness gg_miss_var(df) + theme_bw() gg_miss_var() output Here, the variables are listed along the left-hand side and the number of missing values for each variable is plotted. We can clearly see that brainwt has 27 missing values in this dataset, while sleep_cycle has the most missingness among variables in this dataset. Getting an understanding of what values are missing in your dataset is critical before moving on to any other type of analysis. 52.0.1.3 Shape Determining the shape of your variable is essential before any further analysis is done. Statistical methods used for inference (discussed in a later lesson) often require your data to be distributed in a certain manner before they can be applied to the data. Thus, being able to describe the shape of your variables is necessary during your descriptive analysis. When talking about the shape of one’s data, we’re discussing how the values (observations) within the variable are distributed. Often, we first determine how spread out the numbers are from one another (do all the observations fall between 1 and 10? 1 and 1000? -1000 and 10?). This is known as the range of the values. The range is described by the minimum and maximum values taken by observations in the variable. After establishing the range, we determine the shape or distribution of the data. More explicitly, the distribution of the data explains how the data are spread out over this range. Are most of the values all in the center of this range? Or, are they spread out evenly across the range? There are a number of distributions used commonly in data analysis to describe the values within a variable. We’ll cover just a few of them in this lesson, but keep in mind this is certainly not an exhaustive list. 52.0.1.3.1 Normal Distribution The Normal distribution (also referred to as the Gaussian distribution) is a very common distribution and is often described as a bell-shaped curve. In this distribution, the values are symmetric around the central value with a high density of the values falling right around the central value. The left hand of the curve mirrors the right hand of the curve. Normal Distribution A variable can be described as normally distributed if: there is a strong tendency for data to take a central value - many of the observations are centered around the middle of the range deviations away from the central value are equally likely in both directions the frequency of these deviations away form the central value occurs at the same rate on either side of the central value. Taking a look at the sleep_total variable within our example dataset, we see that the data are somewhat normal; however, they aren’t entirely symmetric. ggplot(df, aes(sleep_total)) + geom_density() distribution of msleep sleep_total A variable that is distributed more normally can be seen in the iris dataset, when looking at the Sepal.Width variable. ggplot(iris, aes(Sepal.Width)) + geom_density() distribution of iris Sepal.Width 52.0.1.3.2 Skewed Distribution Alternatively, sometimes data follow a skewed distribution. In a skewed distribution, most of the values fall to one end of the range, leaving a tail off to the other side. When the tail is off to the left, the distribution is said to be skewed left. When off to the right, the distribution is said to be skewed right. Skewed Distributions To see an example from the msleep dataset, we’ll look at the variable sleep_rem. Here we see that the data are skewed right, given the shift in values away from the right, leading to a long right tail. Here, most of the values are at the lower end of the range. ggplot(df, aes(sleep_rem)) + geom_density() sleep_rem is skewed right 52.0.1.3.3 Uniform Distribution Finally, in distributions we’ll discuss today, sometimes values for a variable are equally likely to be found along any portion of the distribution. The curve for this distribution looks more like a rectangle, since the likelihood of an observation taking a value is constant across the range of possible values. Uniform Distribution 52.0.1.3.4 Outliers Now that we’ve discussed distributions, it’s important to discuss outliers – or an observation that falls far away from the rest of the observations in the distribution. If you were to look at a density curve, you could visually identify outliers as observations that fall far from the rest of the observations. density curve with an outlier For example, imagine you had a sample where all of the individuals in your sample are between the ages of 18 and 65, but then you have one sample that is 1 year old and another that is 95 years old. Sample population If we were to plot the age data on a density plot, it would look something like this: example densityplot The baby and elderly individual would pop out as outliers on the plot. After identifying outliers, one must determine if the outlier samples should be included or removed from your dataset? This is something to consider when carrying out an analysis. caution It can sometimes be difficult to decide whether or not a sample should be removed from the dataset. In the simplest terms, no observation should be removed from your dataset unless there is a valid reason to do so. For a more extreme example, what if that dataset we just discussed (with all the samples having ages between 18 and 65) had one sample with the age 600? Well, if these are human data, we clearly know that is a data entry error. Maybe it was supposed to be 60 years old, but we may not know for sure. If we can follow up with that individual and double-check, it’s best to do that, correct the error, make a note of it, and continue you with the analysis. However, that’s often not possible. In the cases of obvious data entry errors, it’s likely that you’ll have to remove that observation from the dataset. It’s valid to do so in this case since you know that an error occurred and that the observation was not accurate. Outliers do not only occur due to data entry errors. Maybe you were taking weights of your observations over the course of a few weeks. On one of these days, your scale was improperly calibrated, leading to incorrect measurements. In such a case, you would have to remove these incorrect observations before analysis. Outliers can occur for a variety of reasons. Outliers can occur due human error during data entry, technical issues with tools used for measurement, as a result of weather changes that affect measurement accuracy, or due to poor sampling procedures. It’s always important to look at the distribution of your observations for a variable to see if anything is falling far away from the rest of the observations. If there are, it’s then important to think about why this occurred and determine whether or not you have a valid reason to remove the observations from the data. An important note is that observations should never be removed just to make your results look better. Wanting better results is not a valid reason for removing observations from your dataset. 52.0.1.3.4.1 Identifying Outliers To identify outliers visually, density plots and boxplots can be very helpful. For example, if we returned to the iris dataset and looked at the distribution of Petal.Length, we would see a bimodal distribution (yet another distribution!). Bimodal distributions can be identified by density plots that have two distinct humps. In these distributions, there are two different modes – this is where the term “bimodal” comes from. In this plot, the curve suggests there are a number of flowers with petal length less than 2 and many with petal length around 5. ## density plot library(ggplot) ggplot(iris, aes(Petal.Length))+ geom_density() iris density plot Since the two humps in the plot are about the same height, this shows that it’s not just one or two flowers with much smaller petal lengths, but rather that there are many. Thus, these observations aren’t likely an outlier. To investigate this further, we’ll look at petal length broken down by flower species: ## box plot ggplot(iris, aes(Species, Petal.Length))+ geom_boxplot() iris boxplot In this boxplot, we see in fact that setosa have a shorter petal length while virginica have the longest. Had we simply removed all the shorter petal length flowers from our dataset, we would have lost information about an entire species! Boxplots are also helpful because they plot “outlier” samples as points outside the box. By default, boxplots define “outliers” as observations as those that are 1.5 x IQR (interquartile range). The IQR is the distance between the first and third quartiles. This is a mathematical way to determine if a sample may be an outlier. It is visually helpful, but then it’s up to the analyst to determine if an observation should be removed. While the boxplot identifies outliers in the setosa and versicolor species, these values are all within a reasonable distance of the rest of the values, and unless I could determine why this occurred, I would not remove these observations from the dataset iris boxplot with annotations 52.0.1.4 Central Tendency Once you know how large your dataset is, what variables you have information on, how much missing data you’ve got for each variable, and the shape of your data, you’re ready to start understanding the information within the values of each variable. Some of the simplest and most informative measures you can calculate on a numeric variable are those of central tendency. The two most commonly used measures of central tendency are: mean and median. These measures provide information about the typical or central value in the variable. 52.0.1.4.1 mean The mean (often referred to as the average) is equal to the sum of all the observations in the variable divided by the total number of observations in the variable. The mean takes all the values in your variable and calculates the most common value. So if you had the following vector: a &lt;- c(1, 2, 3, 4, 5, 6), the mean would be 3.5. calculating the mean But what if we added another ‘3’ into that vector, so that it were: a &lt;- c(1, 2, 3, 3, 4, 5, 6). Now, the mean would be 3.43. It would decrease the average for this set of numbers as you can see in the calculations here: decreased average To calculate the mean in R, the function is mean(). Here, we show how to calculate the mean for a variable in R. Note that when you have NAs in a variable, you’ll need to let R know to remove the NAs (using na.rm=TRUE) before calculating your mean. Otherwise, it will return NA. ## this will return NA mean(df$sleep_cycle) ## have to tell R to ignore the NAs mean(df$sleep_cycle, na.rm=TRUE) mean(sleep_cycle) 52.0.1.4.2 median The median is the middle observation for a variable after the observations in that variable have been arranged in order of magnitude (from smallest to largest). The median is the middle value. Using the same vector as we first use to calculate median, we see that the middle value for this set of numbers is 3.5 as this is the value at the center of this set of numbers. This happens to be the same value as the mean was. However, that is not always the case. When we add that second 3 in the middle of the set of numbers, the median is now 3, as this is the value at the center of this set of numbers. 3 is the middle value. medians To calculate the median in R, use the function median(). Again, when there are NAs in the variable, you have to tell R explicitly to remove them before calculating the median. ## calculate the median median(df$sleep_cycle, na.rm=TRUE) median sleep_cycle While not the exact same value, the mean and median for sleep_cycle are similar (0.44 and 0.33). However, this is not always the case. For data that are skewed or contain outlier values – values that are very different from the rest of the values in the variable – the mean and the median will be very different from one another. In our example dataset, the mean and the median values for the variable bodywt are quite different from one another. ## calculate mean and median mean(df$bodywt) median(df$bodywt) ## look at the histogram ggplot(df, aes(bodywt)) + geom_histogram() mean vs median When we look at the histogram of the data, we see that most body weights are less than 200 lbs. Thus, the median, or value that would be in the middle if you lined all the weights up in order, is 1.6 kilograms. However, there are a few mammals that are a lot bigger than the rest of the animals. These mammals are outliers in the dataset. These outliers increase the mean. These larger animals drive the mean of the dataset to 166 kilograms. When you have outliers in the dataset, the median is typically the measure of central tendency you’ll want to use, as it’s resistant to the effects of outlier values. 52.0.1.4.3 mode There is a third, less-frequently calculated measure of central tendency for continuous variables, known as the mode. This is the value that comes up most frequently in your dataset. For example, if your dataset a were comprised of the following numbers a &lt;- c(0, 10, 10, 3, 5, 10, 10), 10 would be the mode, as it occurs four times. It doesn’t matter whether it’s the largest value, the smallest value, or somewhere in between, the most frequently value in your dataset is the mode. There is no built-in function for calculating the mode in R for a numeric value, which should suggest that, for continuous variables, knowing the mode of a variable is often less crucial than knowing the mean and median (which is true)! However, you could write a function to calculate it. For the above vector a, which.max(tabulate(a)) would return the mode: 10. (Note that this would not work if you had two values that were found in the dataset at the same frequency. A more eloquent approach would be required.) mode of a continuous variable However, for categorical variables, the level with the most observations would be the mode. This can be determined using the table() function, which breaks down the number of observations within the categorical variable table() output Further, the mode for a categorical variable can be visualized by generating a barplot: ## plot categorical variable to visualize mode ggplot(df, aes(order)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) geom_bar visually displays the mode 52.0.1.5 Variability In addition to measures of central tendency, measures of variability are key in describing the values within a variable. Two common and helpful measures of variability are: standard deviation and variance. Both of these are measures of how spread out the values in a variable are. 52.0.1.5.1 Variance The variance tells you how spread out the values are. If all the values within your variable are exactly the same, that variable’s variance will be zero. The larger your variance, the more spread out your values are. Take the following vector and calculate its variance in R using the var() function: ## variance of a vector where all values are the same a &lt;- c(29, 29, 29, 29) var(a) ## variance of a vector with one very different value b &lt;- c(29, 29, 29, 29, 723678) var(b) variance The only difference between the two vectors is that the second one has one value that is much larger than “29”. The variance for this vector is thus much higher. 52.0.1.5.2 Standard Deviation By definition, the standard deviation is the square root of the variance, thus if we were to calculate the standard deviation in R using the sd() function, we’d see that the sd() function is equal to the square root of the variance: ## calculate standard deviation sd(b) ## this is the same as the square root of the variance sqrt(var(b)) Standard Deviation For both measures of variance, the minimum value is 0. The larger the number, the more spread out the values in the valuable are. 52.0.2 Summarizing Your Data Often, you’ll want to include tables in your reports summarizing your dataset. These will include the number of observations in your dataset and maybe the mean/median and standard deviation of a few variables. These could be organized into a table using what you learned in the data visualization course about generating tables. 52.0.2.1 skimr Alternatively, there is a helpful package that will summarize all the variables within your dataset. The skimr package provides a tidy output with information about your dataset. To use skimr, you’ll have to install and load the package before using the helpful function skim() to get a snapshot of your dataset. install.packages(&quot;skimr&quot;) library(skimr) skim(df) skim() output The output from skim separately summarizes categorical and continuous variables. For continuous variables you get information about the mean and median (p50) column. You know what the range of the variable is (p0 is the minimum value, p100 is the maximum value for continuous variables). You also get a measure of variability with the standard deviation (sd). It even quantifies the number of missing values (missing) and shows you the distribution of each variable (hist)! This function can be incredibly useful to get a quick snapshot of what’s going on with your dataset. 52.0.2.2 Published Descriptive Analyses In academic papers, descriptive analyses often lead to the information included in the first table of the paper. These tables summarize information about the samples used for the analysis in the paper. Here, we’re looking at the first table in a paper published in the New England Journal of Medicine by The CATT Research Group. Table 1 We can see that there is a lot of descriptive information being summarized in this table just by glancing at it. If we zoom in and just focus on the top of the table, we see that the authors have broken down a number of the variables (the rows) and summarized how many patients they had in each of their experimental categories (the columns). Focusing on Sex specifically, we can see that there were 183 females and 118 males in their first experimental group. In the parentheses, they summarize what percent of their sample that was. In this same category, the sample was 60.8% female and 39.2% male. Table 1 - just the top We provide this here as an example of how someone would include a descriptive analysis in a report or publication. It doesn’t always have to be this long, but you should always describe your data when sharing it with others. 52.0.3 Summary This lesson covered the necessary parts of carrying out a descriptive analysis. Generally, describing a dataset involves describing the numbers of observations and variables in your dataset, getting an understanding of missingness, and, understanding the shape, central tendency, and variability of each variable. Description must happen as the first step in any data analysis. 52.0.4 Additional Resources Visualizing Incomplete &amp; Missing Data, by Nathan Yau Getting Started with the naniar package, from Nicholas Tierney visdat package, also from Nicholas Tierney to further visualize datasets during exploratory and descriptive analyses Using the skimr package, by Elin Waring "],["exploratory-analysis.html", "Chapter 53 Exploratory Analysis", " Chapter 53 Exploratory Analysis The goal of an exploratory analysis is to examine, or explore the data and find relationships that weren’t previously known. Exploratory analyses explore how different measures might be related to each other but do not confirm that relationship as causal, i.e., one variable causing another. You’ve probably heard the phrase “Correlation does not imply causation,” and exploratory analyses lie at the root of this saying. Just because you observe a relationship between two variables during exploratory analysis, it does not mean that one necessarily causes the other. Because of this, exploratory analyses, while useful for discovering new connections, should not be the final say in answering a question! It can allow you to formulate hypotheses and drive the design of future studies and data collection, but exploratory analysis alone should never be used as the final say on why or how data might be related to each other. In short, exploratory analysis helps us ask better questions, but it does not answer questions. More specifically, we explore data in order to: Understand data properties such as nonlinear relationships, the existence of missing values, the existence of outliers, etc. Find patterns in data such as associations, group differences, confounders, etc. Suggest modeling strategies such as linear vs. nonlinear models, transformation “Debug” analyses Communicate results 53.0.1 General principles of exploratory analysis We can summarize the general principles of exploratory analysis as follows: Look for missing values Look for outlier values Use plots to explore relationships Use tables to explore relationships If necessary, transform variables These principles may be more clear in an example. We will use a dataset from Kaggle.com that contains 120 years of Olympics history on athletes and results. If you don’t have an account on Kaggle, create one and go to the link https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results and under “Data Sources” download the athlete_events.csv to your computer. Dataset on 120 years of Olympics history on athletes and results Upload the data on RStudio.cloud and import the CSV file using the commands you have learned. Unfortunately, you cannot download the CSV file directly from the web address since downloading datasets on Kaggle requires logging in. Importing data using read_csv() As we learned before, we can use the package skimr to take a look at the data. Using the skimr package to have a summary of the data We see that the dataset contains 15 variables and 271,116 observations. Some of the variables are of factor type and others are of integer or numeric type. The dataset includes variables on athletes such as name, sex, the sport played, whether they received a medal, age, and height. We first need to understand the data properties. So let’s start with missing values. We have different types of variables in our data First, the results of the skim() function indicate that some of our variables have lots of missing values. For instance, the variable Medal has 231,333 missing values. Generally, this is a place for concern since most statistical analyses ignore observations with missing values. However, it is obvious that the missing values for the variable Medal are mainly because the athlete didn’t receive any medals. So this kind of missing value should not be a problem. However, we have missing values in the variables Height and Age. Since we are going to use these variables in our analysis in this lesson, observations with missing values for these two variables will be dropped from our analysis. Remember that NA is the most common character for missing values, but sometimes they are coded as spaces, 999, -1 or “missing”. Check for missing values in a variety of ways. There are some missing values in the data Second, we can see that there are some outliers in some of the numerical variables. For example, look at the summary of the variable Age. Although the average age among all the athletes is around 25, there is an individual who is 97 years old (fun fact: use the command subset(df, df$Age == 97) to check out the information about this athlete. You will see that the name of the athlete is John Quincy Adams Ward and he competed in the sport(!) Art Competitions Mixed Sculpturing in 1928. This artist is known for his George Washington statue in front of Federal Hall in Wall Street in New York City.) It is always good to know about the existence of outliers in your sample. Outliers can significantly skew the results of your analysis. You can find outliers by looking at the distribution of your variable too. There is an outlier in the Age variable Histograms, in general, are one of the best ways to look at a variable and find abnormalities. You can see that the age of most individuals in the sample are between 18-35. Histogram of the variable Age Now, rather than just summarizing the data points within a single variable, we can look at how two or more variables might be related to each other. For instance, we like to know if there is an association between age of athletes and their gender. One of the ways to do this is to look at a boxplot of age grouped by gender, i.e., the distribution of age separated for male and female athletes. Boxplot shows the distribution of the variable age for the gender groups. You can see that the average age is slightly higher for men than for women. Boxplot of the variable Age for male and female individuals If we are interested in looking at the distribution of male and female athletes over time, we can use frequency tables. Let us first create a frequency table of the share of women in each Olympic event. Tables are good for looking at factor or character variables. Wrangling data to find the share of female athletes over time Now, if we want to plot this trend, we can use geom_line() from ggplot. It’s interesting that the share of women among all athletes that was once at a very low level in the early 1900s has gone up to almost 50% in modern times. Plot of the share of female athletes over time In general, the most important plots in exploratory data analysis are: Scatterplots (geom_point()) Histograms (geom_histogram()) Density plots (geom_density()) Boxplots (geom_boxplot()) Barplots (geom_bar()) To end our lesson on exploratory analysis, let’s consider a question: are taller athletes more likely to win a medal? To answer this question we can use different methods. We can look at the distribution of height for those who received a medal and those who didn’t. We can use boxplots or barplots. The choice is yours but because boxplots are more informative, we will use them. We can first create a variable that indicates whether the athlete has any medal (the variable Medal indicates the type of medals). Note that the variable has.medal is a transformation of the variable Medal. Creating a variable that shows whether the athlete has a medal or not And now, we use the following code to create the boxplot. Boxplot for the relationship between height and having won a medal What is obvious is that those who have a medal are taller. Can we say that being tall increases the probability of winning a medal in the Olympics? The answer to this question is that we don’t know. There are some possible scenarios. For instance, it could be true that being tall increase the chances of winning medals. But it could also be that there are more medals awarded in sports such as volleyball or basketball that require taller athletes. In these sports, every member of the winning team gets a medal (even if country counts only one medal is counted for the country). As a result, we may end up having so many tall athletes with a medal in each Olympics. It could also be that there are other confounding factors involved that explain why an athlete wins a medal. We will learn about confounding variables in future lessons. For now, it’s important to know, as we said in the beginning of this lesson, that association or correlation does not mean causation. "],["inference-overview.html", "Chapter 54 Inference: Overview", " Chapter 54 Inference: Overview Inferential Analysis is what analysts carry out after they’ve described and explored their data set. After understanding your dataset better, analysts often try to infer something from the data. This is done using statistical tests. While we’ll only be discussing linear regression in this lesson, there are many different statistical tests, each of which (when appropriately applied) can be used for inferential data analysis. We’ll cover inference in general in this lesson and then cover more of the details you’ll need to truly understand regression in the following few lessons. Across all the lessons discussing inference, we’ll cover: Random Sampling &amp; Inference Uncertainty Simple Linear Regression Multiple Linear Regression Confounding Correlation While it is certainly a lot, it will be incredibly important for you to understand this material going forward, so take your time on this overview and be sure to understand each paragraph, example, and image. Then, in the following lessons, you’ll be able to learn more inference concepts, work through additional examples, and get more practice with inferential analysis. 54.0.1 Getting Started with Inference Ok, so let’s break this all down a little bit. The goal of inferential analyses is to use a relatively small sample of data to infer or say something about the population at large. This is required because often we want to answer questions about a population. Let’s take a dummy example here where we have a population of 14 shapes. Here, in this graphic, the shapes represent individuals in the population and the colors of the shapes can be either pink or grey: The population In this example we only have fourteen shapes in the population; however, in inferential data analysis, it’s not usually possible to sample everyone in the population. Consider if this population were everyone in the United States or every college student in the world. As getting information from every individual would be infeasible. Data are instead collected on a subset, or a sample of the individuals in the larger population. A sample is collected from the population In our example, we’ve been showing you how many pink and how many gray shapes are in the larger population. However, in real life, we don’t know what the answer is in the larger population. That’s why we collected the sample! We don’t know what the truth is in the population This is where inference comes into play. We analyze the data collected in our sample and then do our best to infer what the answer is in the larger population. In other words, inferential data analysis uses data from a sample to make its best guess as to what the answer would be in the population if we were able to measure every individual. Inference from the sample makes its best guess as to what the truth is in the population 54.0.2 Uncertainty Because we haven’t directly measured the population but have only been able to take measurements on a sample of the data, when making our inference we can’t be exactly sure that our inference about the population is exact. For example, in our sample one-third of the shapes are grey. We’d expect about one-third of the shapes in our population to be grey then too! Well, one-third of 14 (the number of shapes in our population) is 4.667. Does this mean four shapes are truly gray? Inference gives us a good guess as to what the truth is in the population Or maybe five shapes in the population are grey? Maybe the population really has five grey shapes in it… Given the sample we’ve taken, we can guess that 4-5 shapes in our population will be grey, but we aren’t certain exactly what that number is. In statistics, this “best guess” is known as an estimate. This means that we estimate that 4.667 shapes will be gray. But, there is uncertainty in that number. Because we’re taking our best guess at figuring out what that estimate should be, there’s also a measure of uncertainty in that estimate. Inferential data analysis includes generating the estimate and the measure of uncertainty around that estimate. Let’s return back to the example where we know the truth in the population. Hey look! There were actually only three grey shapes after all. It is totally possible that if you put all those shapes into a bag and pulled three out that two would be pink and one would be grey. As statisticians, we’d say that getting this sample was probable (it’s within the realm of possibility), but it’s not the most likely (The most likely was either 4 or 5.) This really drives home why it’s important to add uncertainty to your estimate whenever you’re doing inferential analysis! There actually were only three grey shapes in our population after all 54.0.3 Random Sampling Since you are moving from a small amount of data and trying to generalize to a larger population, your ability to accurately infer information about the larger population depends heavily on how the data were sampled. We briefly touched on this in a previous lesson, but let’s discuss it fully now. The data in your sample must be representative of your larger population to be used for inferential data analysis. Let’s discuss what this means. Using the same example, what if, in your larger population, you didn’t just have grey and pink shapes, but you also had blue shapes? What if your larger population had three different color shapes? Well, if your sample only has pink and grey shapes, when you go to make an inference, there’s no way you’d infer that there should be blue shapes in your population since you didn’t capture any in your sample. In this case, your sample is not representative of your larger population. In cases where you do not have a representative sample, you can not carry out inference, since you will not be able to correctly infer information about the larger population. You can only carry out an inferential analysis when your sample is representative of the population This means that you have to design your analysis so that you’re collecting representative data and that you have to check your data after data collection to make sure that you were successful. You may at this point be thinking to yourself. “Wait a second. I thought I didn’t know what the truth was in the population. How can I make sure it’s representative?” Good point! With regards to the measurement you’re making (color distribution of the shapes, in this example), you don’t know the truth. But, you should know other information about the population. What is the age distribution of your population? Your sample should have a similar age distribution. What proportion of your population is female? If it’s half, then your sample should be comprised of half females. Your data collection procedure should be set up to ensure that the sample you collect is representative (very similar to) your larger population. Then, once the data are collected, your descriptive analysis should check to ensure that the data you’ve collected are in fact representative of your larger population. By randomly sampling your larger population, then ensures that the inference you make about the measurement of interest (color distribution of the shapes) will be most accurate. To reiterate: If the data you collect is not from a representative sample of the population, the generalizations you infer won’t be accurate for the population. 54.0.4 A real-life example of inferential data analysis Unlike in our previous examples, Census data wouldn’t be used for inferential analysis. By definition, a census already collects information on (functionally) the entire population. Thus, there is no population on which to infer. Census data are the rare exception where a whole population is included in the dataset. Further, using data from the US census to infer information about another country would not be a good idea because the US isn’t necessarily representative of the other country. Instead, a better example of a dataset on which to carry out inferential analysis would be the data used in the study: The Effect of Air Pollution Control on Life Expectancy in the the United States: An Analysis of 545 US counties for the period 2000 to 2007. In this study, researchers set out to understand the effect of air pollution on everyone in the United States Study question looks to learn something about entire US population To answer this question, a subset of the US population was studied, and the researchers looked at the level of air pollution experienced and life expectancy. It would have been nearly impossible to study every individual in the United States year after year. Instead, this study used the data they collected from a sample of the US population to infer how air pollution might be impacting life expectancy in the entire US! Studies use representative samples to infer information about the larger population 54.0.5 Summary In this first inference lesson, we introduced simply an overview of inferential thinking. We introduced the idea that if you want to learn something about a population, often analyzing a representative sample of the population can help get you to your answer. With this conceptual overview, in the following lessons we’ll see what that looks like at the level of code and how to interpret the output. "],["inference-linear-regression.html", "Chapter 55 Inference: Linear Regression", " Chapter 55 Inference: Linear Regression Inferential analysis is commonly the goal of statistical modeling, where you have a small amount of information to extrapolate and generalize that information to a larger group. One of the most common approaches used in statistical modeling is known as linear regression. Here, we’ll discuss when using linear regression is appropriate, how to carry out the analysis in R, and how to interpret the results from this statistical approach. When discussing linear regression, we’re trying to describe (model) the relationship between a dependent variable and an independent variable. linear regression models relationship between two variables When visualizing a linear relationship, the independent variable is plotted along the bottom of the graph, on the x-axis and the dependent variable is plotted along the side of the plot, on the y-axis. independent on the x-axis; dependent on the y-axis When carrying out linear regression, a best-fitting line is drawn through the data points to describe the relationship between the variables. A best-fitting line describes the relationship between the variables A best-fitting line, technically-speaking, minimizes the sum of the squared errors. In simpler terms, this means that the line that minimizes the distance of all the points from the line is the best-fitting line. Or, most simply, there are the same number of points above the line as there are below the line. In total, the distance from the line to the points above the line will be the same as the distance from the points below the line. Note that the best fitting line does not have to go through any points to be the best-fitting line. Here, on the right, we see a line that goes through seven points on the plot (rather than the four the best-fitting line goes through, on the left). However, this is not a best-fitting line, as there are way more points above the line than there are below the line. A best-fitting line does NOT have to go through the most points possible This line describes the relationship between the two variables. If you look at the direction of the line, it will tell you whether there is a positive or a negative relationship between the variables. In this case, the larger the value of the independent variable, the larger the value of the dependent variable. Similarly, the smaller the value of the independent variable, the smaller the value of the dependent variable. When this is the case, there is a positive relationship between the two variables. A positive relationship will have points that trend up and to the right An example of variables that have a positive relationship would be the height of fathers and their sons. In general, the taller a father is, the taller his son will be. And, the shorter a father is the more likely his son is to be short. Father and son height demonstrate a positive linear relationship Alternatively, when the higher the value of the independent variable, the lower the value of the dependent variable, this is a negative relationship. A positive relationship will have points that trend up and to the left An example of variables that have a negative relationship would be the relationship between a students’ absences and their grades. The more absences a student has, the lower their grades tend to be. Student absences and grades show a negative linear relationship Linear regression, in addition to describing the direction of the relationship, it can also be used to determine the strength of that relationship. This is because the assumption with linear regression is that the true relationship is being described by the best-fitting line. Any points that fall away from the line do so due to random error. This means that if all the points fall directly on the line, there is no error. The further the points fall from the line, the greater the error. When points are further from the best-fitting line, the relationship between the two variables is weaker than when the points fall closer to the line. Correlation is weaker on the left and stronger on the right In this example, the pink line is exactly the same best-fitting line in each graph. However, on the left, where the points fall further from the line, the strength of the relationship between these two variables is weaker than on the right, where the points fall closer to the line, where the relationship is stronger. The strength of this relationship is measured using correlation. The closer the points are to the line the more correlated the two variables are, meaning the relationship between the two variables is stronger. 55.0.0.1 What is the association? Often when people are carrying out linear regression, they are looking to better understand the relationship between two variables. When looking at this relationship, analysts are specifically asking “What is the association between these two variables?” Association between variables describes the trend in the relationship (positive, neutral, or negative) and the strength of that relationship (how correlated the two variables are). After determining that the assumptions of linear regression are met, in order to determine the association between two variables, one would carry out a linear regression. From the linear regression, one would then interpret the Beta estimate and the standard error from the model. Beta estimate - determines the direction and strength of the relationship between the two variables as shown in the following formula. A beta of zero suggests there is no association between the two variables. However, if the beta value is positive, the relationship is positive. If the value is negative, the relationship is negative. Further, the larger the number, the bigger the effect is. We’ll discuss effect size and how to interpret the value in more detail later in this lesson. Beta estimates describe the size and strength of the effect Standard error - determines how uncertain the beta estimate is. The larger the standard error, the more uncertain we are in the estimate. The smaller the standard error, the less uncertain we are in the estimate. Standard errors are calculated based on how well the best-fitting line models the data. The closer the points are to the line, the lower the standard error will be, reflecting our decreased uncertainty. However, as the points are further from the regression line, our uncertainty in the estimate will increase, and the standard error will be larger. Standard errors explain how uncertain the estimate is A reminder that when carrying out inferential data analysis, you will always want to report an estimate and a measure of uncertainty. For linear regression, this will be the beta estimate and the standard error. You may have heard talk of p-values at some point. People tend to use p-values to describe the strength of their association due to its simplicity. The p-value is a single number that takes into account both the estimate (beta estimate) and the uncertainty in that estimate (SE). The lower a p-value the more significant the association is between two variables. However, while it is a simple value, it doesn’t tell you nearly as much information as reporting the estimates and standard errors directly. Thus, if you’re reporting p-values, it’s best to also include the estimate and standard errors as well. That said, the general interpretation of a p-value is “the probability of getting the observed results (or results more extreme) by chance alone.” Since it’s a probability, the value will always be between 0 and 1. Then, for example, a p-value of 0.05, means that 5 percent of the time (or 1 in 20), you’d observe results this extreme simply by chance. 55.0.0.1.1 Association Testing in R Now that we’ve discussed what you can learn from an association test, let’s look at an example in R. For this example we’ll use the trees dataset available in R, which includes girth, height, and volume measurements for 31 black cherry trees. With this dataset, we’ll answer the question: Can we infer the height of a tree given its girth? Presumably, it’s easier to measure a trees girth (width around) than it is to measure its height. Thus, here we want to know whether or not height and girth are associated. In this case, since we’re asking if we can infer height from girth, girth is the independent variable and height is the dependent variable. In other words, we’re asking does height depend on girth? First, before carrying out the linear regression to test for association and answer this question, we have to be sure linear regression is appropriate. We’ll test for linearity and homoscedasticity. To do so, we’ll first use ggplot2 to generate a scatterplot of the variables of interest. library(ggplot2) ggplot(trees) + geom_point(aes(Height, Girth)) scatterplot of trees dataset From the looks of this plot, the relationship looks approximately linear, but to visually make this a little easier, we’ll add a line of best first to the plot. ggplot(trees, aes(Height, Girth)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) scatterplot with line of best fit On this graph, the relationship looks approximately linear and the variance (distance from points to the line) is constant across the data. Given this, it’s appropriate to use linear regression for these data. Fitting the model Now that that’s established, we can run the linear regression. To do so, we’ll use the lm() function to fit the model. The syntax for this function is lm(dependent_variable ~ independent_variable, data = dataset). ## run the regression fit &lt;- lm(Girth ~ Height , data = trees) A histogram (or densityplot) is a great way to see the shape of your data when you are first getting acquainted with it. This can tell you whether or not your data truly looks normally distributed or not. If your data is not normally distributed, it won’t be appropriate to use linear regression. library(ggplot2) ggplot(fit, aes(fit$residuals)) + geom_histogram(bins = 5) Interpreting the model While the relationship in our example appears to be linear, does not indicate being driven by outliers, is approximately homoscedastic and has residuals that are not perfectly Normally distributed, but fall close to the line in the QQ plot, we can discuss how to interpret the results of the model. ## take a look at the output summary(fit) The summary() function summarizes the model as well as the output of the model. We can see the values we’re interested in in this summary, including the beta estimate, the standard error (SE), and the p-value. Specifically, from the beta estimate, which is positive, we confirm that the relationship is positive (which we could also tell from the scatterplot). We can also interpret this beta estimate explicitly. Beta, SE, and p-value all included in summary() output Specifically, the beta estimate (also known as the beta coefficient or coefficient in the Estimate column) is the amount the dependent variable will change given a one unit increase in the independent variable. In the case of the trees, a beta estimate of 0.256, says that for every inch a tree’s girth increases, its height will increase by 0.256 inches. Thus, we not only know that there’s a positive relationship between the two variables, but we know by precisely how much one variable will change given a single unit increase in the other variable. Note that we’re looking at the second row in the output here, where the row label is “Height”. This row quantifies the relationship between our two variables. The first row quantifies the intercept, or where the line crosses the y-axis. The standard error and p-value are also included in this output. Error is typically something we want to minimize (in life and statistical analyses), so the smaller the error, the more confident we are in the association between these two variables. The beta estimate and the standard error are then both considered in the calculation of the p-value (found in the column Pr[&gt;|t|]). The smaller this value is, the more confident we are that this relationship is not due to random chance alone. Variance Explained Additionally, the strength of this relationship is summarized using the adjusted R-squared metric. This metric explains how much of the variance this regression line explains. The more variance explained, the closer this value is to 1. And, the closer this value is to 1, the closer the points in your dataset fall to the line of best fit. The further they are from the line, the closer this value will be to zero. Adjusted R-squared specifies how closely the data fall are to the regression line As we saw in the scatterplot, the data are not right up against the regression line, so a value of 0.2445 seems reasonable, suggesting that this model (this regression line) explains 24.45% of the variance in the data. Using `broom Finally, while the summary() output are visually helpful, if you want to get any of the numbers out from that model, it’s not always straightforward. Thankfully, there is a package to help you with that! The tidy() function from the broom package helps take the summary output from a statistical model and organize it into a tabular output. #install.packages(&quot;broom&quot;) library(broom) tidy(fit) tidy() helps organize output from statistical models Note that the values haven’t changed. They’re just organized into an easy-to-use table. It’s helpful to keep in mind that this function and package exist as you work with statistical models. Finally, it’s important to always keep in mind that the interpretation of your inferential data analysis is incredibly important. When you use linear regression to test for association, you’re looking at the relationship between the two variables. While girth can be used to infer a tree’s height, this is just a correlation. It does not mean that an increase in girth causes the tree to grow more. Associations are correlations. They are not causal. We’ll discuss this more later in the lesson. For now, however, in response to our question, can we infer a tree’s height from its girth, the answer is yes. We would expect, on average, a tree’s height to increase 0.255 inches for every one inch increase in girth. "],["inference-examples.html", "Chapter 56 Inference: Examples", " Chapter 56 Inference: Examples In the last lesson, we went through a number of the basics of simple linear regression, discussed confounding, introduced multiple linear regression, and discussed analytical approaches built off of linear regression. Through that lesson and in the swirl modules in the quiz, we went through a few simple examples; however, since linear regression is the foundation of tons of work being done in the real world, we wanted to walk through two additional examples - which we’ll call case studies - in this lesson. This lesson will help you: See how linear regression can be used Understand the types of questions where it’s applicable Learn to interpret results you find in the world Analyze your own data in the future 56.0.1 Introduction Data scientists, statisticians, social scientists, economists, and natural scientists are all related to one another in that they all ask questions about the world and use data to answer them. In this lesson, through three case studies, we’ll happen focus on questions that economists have asked in recent years, but these approaches are used by all types of people analyzing data and answering questions. In this lesson, we’ll discuss the data economists have collected to answer their questions of interest, the analyses they’ve carried out, and their results. 56.0.2 Case Study #1: The Effects of Watching Sesame Street on Educational Outcomes In an earlier lesson we mentioned that when carrying out inferential analyses, analysts are taking a small sample of data to say something about what would happen if we collected more data, say, from the entire population. The example question we introduced was trying to measure and understand the effect of Sesame Street on children’s learning. To better understand this relationship, we proposed that an analyst could set out to answer the question: “Is there a relationship between watching Sesame Street and test scores among children?” In fact, many economists have studied exactly this! In our first case study, we’ll walk through a recent analysis looking at the effects of Sesame Street on educational outcomes. Eary Childhood Education by Television 56.0.2.1 Question The general question the economists we’ll be talking about today (Melissa S. Kearney and Phillip B. Levine) set out to answer was: “Does exposure to Sesame Street improve educational and labor market outcomes?” Now, this group was not the first to study a question involving Sesame Street and educational outcomes, but they did build upon the work of others’ in their analysis. This means that they had to figure out what specific question they were going to ask to add something new to the discussion. Rather than looking at a small sample of children (as others had done previously), the economists we’ll be discussing here set out to answer the question for a larger group: How was an entire generation of children impacted by having access to Sesame Street? While this question is getting more specific, the actual questions they’d ask was driven by the data to which they had access. 56.0.2.2 Data When asking questions about populations across an entire country on data that were generated more than 50 years ago (Sesame Street debuted in 1969), often the data you need are not all in a single location. Thus, to have measurements on availability of Sesame Street as well as educational and labor outcomes, these economists had to use a number of different data sources: Census Data - 1980, 1990, and 2000 Census data 1980 High School and Beyond Survey Broadcast exposure data - when Sesame Street was broadcast, only certain parts of the country had access to the channels that broadcast Sesame Street Among these data, the economists would be most interested in the following variables: 1. Sesame Street Coverage Rates (or, how available was Sesame Street in the county in 1969) 2. Educational outcome 3. Labor market outcomes 4. Policy changes (indicator for whether Food Stamp program had been introduced and Head Start Program expenditures had increased) 5. Individual demographic information - race/ethnicity, age, and socioeconomic status (SES) Sesame Street Coverage Rates 56.0.2.3 Analysis These economists used two primary regression models to analyze these data. It was a multiple regression model; however, admittedly, it is a tad more complex than those models we’ve considered so far. Now, fundamentally it is just a multiple regression model where the effect of Sesame Street coverage on outcome (educational or labor market) is being measured while controlling for policy changes and demographic factors. However, I will note that the way in which policy and demographic factors were included goes beyond the scope of this lesson. 56.0.2.4 Results Nevertheless, we can still interpret the results from this analysis! The results, as they often are for inferential analyses, are presented in the form of a table. Before presenting the results of the table, the economists first demonstrate (using a figure) that before 1969 when Sesame Street was introduced, there was no difference in students at grade level between areas that would go on to have Sesame Street coverage and those that would not. However, as time goes on, their analysis demonstrates an increase in students at grade level in areas where Sesame Street was available. Sesame Street availability correlated with areas where more students were at grade level after 1969 Table 4 from this paper then summarizes the results looking at the impact Sesame Street coverage (meaning how available Sesame Street was in an area) has on educational outcomes. The authors present the effect sizes and standard errors in parentheses. To intercept these, values greater than 0 have a positive relationship, less than zero a negative relationship. And, the smaller the standard error, the more confident we are in the estimate. Table 4 summarizes results looking at impact of Sesame Street on Educational Outcomes For example, if we just focus on the first row measuring the aggregate effect, we see that the effect size is 0.105. Being greater than 0, we see that there is some positive effect. This means that the more Sesame Street was available, the better the educational outcomes. The standard error here is 0.041, a value close to but not exactly zero, suggesting that we’re relatively confident that this effect is truly close to 0.105. Sesame Street Availability is associated with an increase in educational attainment To interpret what an effect size of 0.105 means in the context of this analysis, a 1 point increase in coverage rates would lead to a 0.105 percentage point increase in the rate of grade-for-age status (their measure of educational attainment). The economists who carried out the study, however, point out that a 1 point increase in coverage rates is not typical. Rather, if a child were to move from a low coverage area to a strong coverage area, this would actually be a 30 point increase in coverage rates. This means that the effect for moving from a low coverage to a high coverage area would actually result in a 3.2% (0.3 * 0.105 = 0.032) increase in the rate of grade-for-age status. The authors also stratified their results by gender and race. By comparing the effect sizes, we can see that the effect is larger in males than it is in females and in black children relative to white children. The results are strongest in males and black children To demonstrate that this effect has anything to do with Sesame Street, the authors carried out the same analysis before Sesame Street was released. In doing this, the economists are looking for effect sizes close to zero, suggesting that the availability of Sesame Street in 1969 is truly playing a role in improving educational outcomes. This is what we call a negative control, meaning they’re doing this analysis expecting no effect (effect sizes near zero). The authors present their results from this null analysis in a table again. Before Sesame Street availability, coverage did not demonstrate a difference in educational attainment 56.0.2.5 Conclusion In this work, the economists set out to understand at the population level how the availability of Sesame Street affects the educational outcomes of children across the country. They had to figure out how to measure Sesame Street availability, educational outcomes, policy changes, and demographic information before being able to use linear regression and answer their question. Once they determined that they were able to measure all of their needed variables at the county level, they carried out their regression and demonstrated that when Sesame Street was available, a higher percentage students started performing at their grade level - a positive relationship between Sesame Street availability and educational attainment! 56.0.3 Case Study #2: Effects of Religion on Happiness Economists have long been interested in measuring happiness and understanding happiness. Sometimes they’re interested in understanding a single person’s happiness and what contributes to that; however, more often they’re looking at populations - cities, states, or whole countries and trying to understand why certain populations are happier than others. A recent analysis in the American Economic Review set out to study the relationship between religiosity and happiness. 56.0.3.1 Question The general question the economists (Deaton and Stone) started with was “Is religion good for you?” However, as we’ve discussed in previous courses, in order to analyze this question using data, we really need a more specific question, one where we know what we’re measuring in order to answer this question. To answer this question, the researchers knew they needed, at a minimum, information on people’s religiosity as well as reports of how happy they are (also known as their “well-being”). 56.0.3.2 Data The authors used data from two different sources: Gallup Healthways Well-being Index survey (N=1,000 randomly-selected Americans surveyed daily since January 2008) Gallup World Poll (N=1,000+ people from 160 different countries) Religiosity People responded either yes or no to the question “Is religion an important part of your daily life?” Well-being ladder - people rate their lives on a scale from 0 (worst possible life) to 10 (best possible life) happiness - people respond either yes or no to the question “Did you experience a lot of happiness yesterday?” The economists running this study had to determine what they were going to measure and where those data were going to come from before ever being able to answer their question. They settled on two different measures of well-being and a single measure of religiosity. Note that these are survey data, so we’re relying on individuals reporting information accurately, something that does not always happen with surveys…and certainly something to always keep in mind when interpreting results. 56.0.3.3 Analysis To analyze these data, the researchers had to decide which variables to use (meaning what to measure, discussed above) and how to analyze it. They chose to utilize linear regression, with religiosity as the independent variable and well-being as the dependent variable. The authors were particularly interested in understanding a paradox. The authors know that individuals who are religious tend to have higher self-reported well-being; however, regions (states or countries) that are more religious tend to have lower well-being as a population. The individual effect is completely opposite to what we see at the population level. While this paradox has been reported before, these authors wanted to look in this much larger dataset to determine the effect of religion on your health. 56.0.3.4 Results The authors carried out a number of linear regressions. At times it was simple linear regression (i.e. looking at the relationship between individuals religiosity and self-reported well-being) and sometimes it involved multiple linear regression (the same model, but accounting for differences in income). They reported their findings in a table where they include the beta coefficient, standard error (SE), and number of observations used to calculate the statistics. They do this for both the ladder measurement of well-being as well as the happiness measure and reported all of these findings in a single table. Table 2 summarizes relationship between self-reported well-being and religiosity 56.0.3.5 Interpretation To understand the answer to the question “Is religion good for you?,” readers have to know how to interpret this type of table. 56.0.3.5.1 Religiosity and Ladder-happiness in the USA Let’s first try to interpret the relationship between people’s religiosity (independent variable) how people rank their lives (the dependent variable fist focusing on ladder score - a measure from 0 to 10) among individuals in the USA. Religious individuals in the USA have higher self-reported well-being Focusing on the pink box here, we see that person-level data has a positive beta coefficient, indicating a positive relationship between religiosity and well-being. This means that religious people (on average) tend to have higher ladder measure of well-being than non-religious individuals. The second row here demonstrates that when accounting for income (with multiple regression), this result remains and in fact gets stronger (0.263 &gt; 0.248). We can also see that these results come from analyzing individual data from more than 1 million individuals(the Observations column) and that the error around this estimate is quite small (SE column). Moving to the third row here, we see that at the state level (rather than the individual level), there is a slight, negative relationship between religiosity and well-being. This result suggests that the more religious a state is, the less happy they are as a state. However, once accounting for income, we see that this negative relationship goes away, suggesting that income, rather than religiosity, drives this effect. 56.0.3.5.2 Religiosity and Yesterday’s happiness in the USA Now, remember that well-being was measured in more than one way. We can now look at the USA-level results where there were only two possible options - yes or no to the question: “Did you experience a lot of happiness yesterday?” Little or no effect of religiosity on yesterday’s happiness in USA Here the results are all positive, but also very close to zero, suggesting that the effect of religiosity on yesterday’s measure of happiness is quite small. 56.0.3.5.3 Worldwide religiosity and happiness One benefit of this study was that the researchers had measurements from many people worldwide, so they were able to study the effect of religiosity on well-being across countries. Religious individuals in the USA have higher self-reported well-being Again, we turn to the table to look at this relationship. Looking at the beta coefficients, we see that the results suggest that more religious countries tend to have lower well-being measurements (measured by the negative beta coefficient); however, once income was accounted for, this result demonstrated that more religious countries (when accounting for income) tend to have higher measures of ladder well-being. The same general trend held across countries when looking at the effects of religiosity on yesterday’s happiness. 56.0.3.6 Conclusion The economists of this study discuss a number of confusing findings related to happiness and suggest possible explanations of these results. They use regression to understand the effects of the independent variable (religiosity or income) on the dependent variable (well-being / happiness). However, as with all studies, one set of analyses does not answer all the questions on the subject - additional analyses and experiments would be needed to understand these results fully. 56.0.4 Summary In this lesson we walked through two real-life examples of inference being used to answer questions. In the first we looked at the relationship between the availability of Sesame Street and educational attainment. In the second, we looked at the effect of religiosity on well-being. Both studies used regression in their inferential analyses to answer their questions of interest. We discussed effect sizes (beta coefficients) and their interpretation in the previous lesson as well as this lesson. Understanding regression will get you a great start in understanding inferential (and even predictive!) analysis. 56.0.5 Additional Resources Early Childhood Education by Television: Lessons from Sesame Street, by Melissa S. Kearney and Phillip B. Levine Why Kids Who Watched Sesame Street Did Better In School, article in Quartz by Annabelle Timsit Two Happiness Puzzles, by Angus Deaton and Arthur A. Stone "],["inference-multiple-regression.html", "Chapter 57 Inference: Multiple Regression", " Chapter 57 Inference: Multiple Regression In the last lesson, we finished discussing how to use lm() to assess the association between two numeric variables. However, there is one incredibly important topic that we have to discuss before moving on from linear regression. Confounding in something to watch out for in any analysis you’re doing that looks at the relationship between two more more variables. So…what is confounding? 57.0.1 Confounding Well, let’s consider an example. What if we were interested in understanding the relationship between shoe size and literacy. To do so, we took a look at this small sample of two humans, one who wears small shoes and is not literate and one adult who wears big shoes and is literate. Two humans and their respective shoe sizes and literacy levels If we were to diagram this question, we may ask “Can we infer literacy rates from shoe size?” Possible to infer literacy rate from shoe size? If we return to our sample, it’d be important to note that one of the humans is a young child and the other is an adult. Adult and child with their respective shoe sizes and literacy levels Our initial diagram failed to take into consideration the fact that these humans differed in their age. Age affects their shoe size and their literacy rates. In this example, age is a confounder. Age is a confounder Any time you have a variable that affects both your dependent and independent variables, it’s a confounder. Ignoring confounders is not appropriate when analyzing data. In fact, in this example, you would have concluded that people who wear small shoes have lower literacy rates than those who wear large shoes. That would have been incorrect. In fact, that analysis was confounded by age. Failing to correct for confounding has led to misreporting in the media and retraction of scientific studies. You don’t want to be in that situation. So, always consider and check for confounding among the variables in your dataset. Confounders are variables that affect both your dependent and independent variables 57.0.2 Multiple Linear Regression There are ways to effectively handle confounders within an analysis. Confounders can be included in your linear regression model. When included, the analysis takes into account the fact that these variables are confounders and carries out the regression, removing the effect of the confounding variable from the estimates calculated for the variable of interest. This type of analysis is known as multiple linear regression, and the general format is: lm(dependent_variable ~ independent_variable + confounder , data = dataset). As a simple example, let’s return to the mtcars dataset, which we’ve worked with before. In this dataset, we have data from 32 automobiles, including their weight (wt), miles per gallon (mpg), and Engine (vs, where 0 is “V-shaped” and 1 is “straight”). Suppose we were interested in inferring the mpg a car would get based on its weight. We’d first look at the relationship graphically: ## take a look at scatterplot ggplot(mtcars, aes(wt, mpg)) + geom_point() scatterplot From the scatterplot, the relationship looks approximately linear and the variance looks constant. Thus, we could model this using linear regression: ## model the data without confounder fit &lt;- lm(mpg ~ wt, data = mtcars) tidy(fit) mtcars linear regression output From this analysis, we would infer that for every increase 1000 lbs more a car weighs, it gets 5.34 miles less per gallon. However, we know that the weight of a car doesn’t necessarily tell the whole story. The type of engine in the car likely affects both the weight of the car and the miles per gallon the car gets. Graphically, we could see if this were the case by looking at these scatterplots: ## look at the difference in relationship ## between Engine types ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_wrap(~vs) Scatterplot faceting for engine type From this plot, we can see that V-shaped engines (vs= 0), tend to be heavier and get fewer miles per gallon while straight engines (vs = 1) tend to weigh less and get more miles per gallon. Importantly, however, we see that a car that weighs 3000 points (wt = 3) and has a V-Shaped engine (vs = 0) gets fewer miles per gallon than a car of the same weight with a straight engine (vs = 1), suggesting that simply modeling a linear relationship between weight and mpg is not appropriate. Let’s then model the data, taking this confounding into account: ## include engine (vs) as a confounder fit &lt;- lm(mpg ~ wt + vs, data = mtcars) tidy(fit) confounding model taken into account Here, we get a more accurate picture of what’s going on. Interpreting multiple regression models is slightly more complicated since there are more variables; however, we’ll practice how to do so now. The best way to interpret the coefficients in a multiple linear regression model is to focus on a single variable of interest and hold all other variables constant. For instance, we’ll focus on weight (wt) while holding (vs) constant to interpret. This means that for a V-shaped engine, we expect to see a 4.44 miles per gallon decrease for every 1000 lb increase in weight. We can similarly interpret the coefficients by focusing on the engines (vs). For example, for two cars that weigh the same, we’d expect a straight engine (vs = 1) to get 3.5 more miles per gallon than a V-Shaped engine (vs= 0). confounding model taken into account Finally, we’ll point out that the p-value for wt decreased in this model relative to the model where we didn’t account for confounding. This is because the model was not initially taking into account the engine difference. Sometimes when confounders are accounted for, your variable of interest will become more significant; however, frequently, the p-value will increase, and that’s OK. What’s important is that the data are most appropriately modeled. 57.0.3 Correlation is not Causation You’ve likely heard someone say before that “correlation is not causation,” and it’s true! In fact, there are whole websites dedicated to this concept. Let’s make sure we know exactly what that means before moving on. In the plot you see here, as the divorce rate in Maine decreases, so does per capita consumption of margarine. These two lines are clearly correlated; however, there isn’t really a strong (or any) argument to say that one caused the other. Thus, just because you see two things with the same trend does not mean that one caused the other. These are simply spurious correlations – things that trend together by chance. Always keep this in mind when you’re doing inferential analysis, and be sure that you never draw causal claims when all you have are associations. Correlation does not equal causation In fact, one could argue that the only time you can make causal claims are when you have carried out a randomized experiment. Randomized experiments are studies that are designed and carried out by randomly assigning certain subjects to one treatment and the rest of the individuals to another treatment. The treatment is then applied and the results are then analyzed. In the case of a randomized experiment, causal claims can start to be made. Short of this, however, be careful with the language you choose and do not overstate your findings. 57.0.4 Beyond Linear Regression While we’ve focused on linear regression in this lesson on inference, linear regression isn’t the only analytical approach out there. However, it is arguably the most commonly used. And, beyond that, there are many statistical tests and approaches that are slight variations on linear regression, so having a solid foundation and understanding of linear regression makes understanding these other tests and approaches much simpler. For example, what if you didn’t want to measure the linear relationship between two variables, but instead wanted to know whether or not the average observed is different from expectation… 57.0.5 Mean different from expectation? To answer a question like this, let’s consider the case where you’re interested in analyzing data about a single numeric variable. If you were doing descriptive statistics on this dataset, you’d likely calculate the mean for that variable. But, what if, in addition to knowing the mean, you wanted to know if the values in that variable were all within the bounds of normal variation. You could calculate that using inferential data analysis. You could use the data you have to infer whether or not the data are within the expected bounds. For example, let’s say you had a dataset that included the number of ounces actually included in 100 cans of a soft drink. You’d expect that each can have exactly 12 oz of liquid; however, there is some variation in the process. So, let’s test whether or not you’re consistently getting shorted on the amount of liquid in your can. In fact, let’s go ahead and generate the dataset ourselves! ## generate the dataset set.seed(34) soda_ounces &lt;- rnorm(100, mean = 12, sd = 0.04) head(soda_ounces) In this code, we’re specifying that we want to take a random draw of 100 different values (representing our 100 cans of soft drink), where the mean is 12 (representing the 12 ounces of soda expected to be within each can), and allowing for some variation (we’ve set the standard deviation to be 0.04). output looking at soda_ounces dataset We can see that the values are approximately, but not always exactly equal to the expected 12 ounces. 57.0.5.1 Testing mean difference from expectation in R To make an inference as to whether or not we’re consistently getting shorted, we’re going to use this sample of 100 cans. Note that we’re using this sample of cans to infer something about all cans of this soft drink, since we aren’t able to measure the number of ounces in all cans of the soft drink generated. To carry out this statistical test, we’ll use a t-test. Wait, we haven’t talked about that statistical test yet. So, let’s take a quick detour to discuss t-tests and how they relate to linear regression. 57.0.5.2 Detour: Linear Regression &amp; t-tests R has a built in t-test function: t.test(). However, I mentioned earlier that many statistical tests are simply extension of linear regression. In fact, a t-test is simply a linear model where we specify to only fit an intercept (where the data crosses the y-axis). In other words, this specifies to calculate the mean…which is exactly what we’re looking to do here with our t-test! We’ll compare these two approaches below. However, before we can do so, we have to ensure that the data follow a normal distribution, since this is the primary assumption of the t-test. library(ggplot2) ## check for normality ggplot(as.data.frame(soda_ounces))+ geom_histogram(aes(soda_ounces), bins = 10) Here, we see that the data are approximately normally distributed, allowing for a t-test to be used. histogram of soda_ounces A t-test will check whether the observed ounces differs from the expected mean (12 oz). As mentioned above, to run a t-test in R, most people use the built-in function: t.test() ## carry out t-test t.test(soda_ounces, mu = 12) In the output from this function, we’ll focus on the 95 percent confidence interval. Confidence Intervals provide the range of values likely to contain the unknown population parameter. Here, the population parameter we’re interested in is the mean. Thus, the 95% Confidence Intervals provides us the range where, upon repeated sampling, the calculated mean would fall 95 percent of the time. More specifically, if the 95 percent confidence interval contains the expected mean (12 oz), then we can be confident that the company is not shorting us on the amount of liquid they’re putting into each can. Here, since 12 is between 11.99187 and 12.00754, we can see that the amounts in the 100 sampled cans are within the expected variation. We could infer from this sample that the population of all cans of this soft drink are likely to have an appropriate amount of liquid in the cans. However, as mentioned previously, t-tests are an extension of linear regression. We could also look to see whether or not the cans had the expected average of 12 oz in the data collected using lm(). # from linear regression regression_output &lt;- lm(soda_ounces ~ 1) # calculate confidence interval confint(regression_output) Note that the confidence interval is the exact same here using lm() as above when we used t.test()! We bring this up not to confuse you, but to guide you away from trying to memorize each individual statistical test and instead understand how they relate to one another. 57.0.6 More Statistical Tests Now that you’ve seen how to measure the linear relationship between variables (linear regression) and how to determine if the mean of a dataset differs from expectation (t-test), it’s important to know that you can ask lots of different questions using extensions of linear regression. These have been nicely summarized by Jonas Kristoffer Lindelov in is blog post Common statistical tests are linear models (or: how to teach stats). We’ve included the table summarizing his post here, but we recommend you check it out and the examples included within it carefully! Common statistical tests are linear models 57.0.7 Summary In these few lessons on inference, we have covered a lot. We started off discussing that inferential data analysis is required since we often cannot take measurements from everyone in our population of interest. Thus, we take representative samples and estimate what the measure is in the larger population. Since this is an estimate, there is always some measure of uncertainty surrounding this estimate. We talked about how inferences are drawn using a single numeric variable. We talked in detail about simple linear regression and touched on multiple linear regression, with a brief discussion on confounding. We discussed that linear regression helps describe the relationship between two numeric variables (trees example) and that multiple linear regression helps account for confounding in an analysis (mtcars example). We discussed t-tests (the soda example), including how t-tests (and many other common statistical tests!) are variations on linear regression. We’ll end this lesson by reiterating that being able to draw conclusions and clear interpretations of your analyses is critical and that you should never draw causal conclusions when you have associations. 57.0.8 Additional Resources Common statistical tests are linear models, blog post by Jonas Kristoffer Lindelov "],["prediction-and-machine-learning.html", "Chapter 58 Prediction and Machine Learning", " Chapter 58 Prediction and Machine Learning In the last lesson, we discussed that inferential data analysis looks to learn something about a population by making inferences from a representative sample. While the goal in inference is to learn something about the population, when we’re talking about prediction, the focus is on the individual. The goal of predictive analysis and machine learning approaches is to train a model using data to make predictions about an individual. In other words, the goal of predictive analysis is to use data you have now to make predictions about future data. We spend a lot of time trying to predict things in daily life- the upcoming weather, the outcomes of sports events, and the outcomes of elections. We’ve previously mentioned Nate Silver at FiveThirtyEight, where they try and predict the outcomes of U.S. elections (and sporting events too!). Using historical polling data and trends and current polling, FiveThirtyEight builds models to predict the outcomes and the next US Presidential vote - and has been fairly accurate at doing so! FiveThirtyEight’s models accurately predicted the 2008 and 2012 elections and was widely considered an outlier in the 2016 US elections, as it was one of the few models to suggest Donald Trump had a chance of winning. Predicting the outcome of elections is a key example of predictive analysis, where historical data (data they have now) are used to predict something about the future. Basics of Predictive Analysis In this lesson we’ll walk through the important pieces of carrying out a predictive analysis, what considerations should be made when making predictions, discuss what machine learning is, and talk about how to assess accuracy within a predictive analysis. 58.0.1 What is Machine Learning? So far we’ve been discussing predictive analysis. But, you may have heard people on the news or in daily life talking about “machine learning.” The goal of machine learning is to build models (often referred to as algorithms) from the patterns in data that can be used for predictions in the future. For our purposes, it’s safe to argue that when doing predictive analysis, we’re actually doing machine learning. As such, we’ll use machine learning throughout the rest of this lesson. Here, machine learning refers to using the relationships within a dataset to build a model that can be used for prediction. That said, there is without a doubt an entire field of individuals dedicating themselves to machine learning. This lesson will just touch on the very basics within the field. 58.0.2 Machine Learning In order to make predictions for the future using data you have now, there are four general steps: Data Splitting - what data are you going to use to train your model? To tune your model? To test your model? Variable Selection - what variable(s) from the data you have now are you going to use to predict future outcomes? Model Selection - How are you going to model the data? Accuracy Assessment - How are you going to assess accuracy of your predictions? Basic Steps 58.0.3 Data Splitting For predictive analysis (or machine learning), you need data on which to train your model. These are the set of observations and corresponding variables that you’re going to use to build your predictive model. But, a predictive model is only worth something if in can predict accurately in a future dataset. Thus, often, in machine learning there are three datasets used to build a predictive model: train, tune, and test. 58.0.3.1 Train, Tune, Test Okay, admittedly, these are not the most common words to use for this process. Many people use train, validate, and test. However, almost as many people use train, test, and validate, as evidenced by this Twitter poll: Twitter poll As such, we’re mentioning those terms so that you’re familiar with them, but since machine learning people can’t agree on the order of the words, in this lesson, we’ve decided to go with more helpful terminology, as suggested by Carl de Boer: train, tune, and test. Train, Tune, Test 58.0.3.1.1 Train Training data are the data we described above. The data used to build your predictive model. These data are referred to as your training set. training data 58.0.3.1.2 Tune Before getting started, your original dataset is often split. Some (often 70%) of the observations in your dataset are used to train the model, while 30% are held out. This held-out set of observations from your original dataset are then used to improve (tune) the accuracy of model. These hold-out samples are used to see whether or not your predictive model accurately makes predictions in the set of samples not used to train the model. tuning data 58.0.3.1.3 Test Finally, an independent dataset – one that is not from the same experiment or source as the data used to train and tune your model are used to see whether or not your predictive model makes accurate predictions in a completely new dataset. Predictive models that can be generalized to and make accurate predictions in new datasets are the best predictive models. testing data 58.0.4 Variable Selection For predictive analysis to be worth anything, you have to be able to predict an outcome accurately with the data you have on hand. If all the data you have on hand are the heights of elephants in Asia, you’re likely not going to be able to predict the outcome of the next US election. Thus, the variables in the data you have on hand have to be related to the outcome you’re interested in predicting in some way (which is not the case for the heights of elephants and US elections). Instead, to predict US elections, you’d likely want some data on outcomes of previous elections, maybe some demographic information about the voting districts, and maybe some information about the ages or professions of the people voting. All of these variables are likely to be helpful in predicting the outcome in a future election, but which ones are actually predictive? All of them? Some of them? The process of deciding which variables to use for prediction is called variable selection. Variable Selection You ideally want to include the fewest variables in your model as possible. Only having a few variables in your model avoids you having to collect a ton of data or build a really complicated model. But, you want the model to be as accurate as possible in making predictions. Thus, there’s always a balance between minimizing the variables included (to only include the most predictive variables!) and maximizing your model’s predictive accuracy. In other words, like in inferential analysis, your ability to make accurate predictions is dependent on whether or not you have measurements on the right variables. If you aren’t measuring the right variables to predict an outcome, your predictions aren’t going to be accurate. Thus, variable selection, is incredibly important. All that said, there are machine learning approaches that carry out variable selection for you, using all the data to determine which variables in the dataset are most helpful for prediction. Nevertheless, whether you are deciding on the variables to include or the computer is deciding for you, variable selection is important to accurate prediction. 58.0.4.1 Lack of Causality Reminder As a reminder, as was discussed in the inferential analysis, just because one variable may predict another, it does not mean that one causes the other. In predictive analysis, you are taking advantage of the relationship between two variables, using one variable (or one set of variables) to predict a second variable. Just because one variable accurately predicts another variable does not mean that they are causally related. 58.0.5 Model Selection Additionally, there are many ways to generate prediction models. Each model was developed for a different and specific purpose. We’ll discuss a few types of predictive models here, with a focus on using linear regression. However, regardless of which model you choose to use for prediction, it’s best to keep in mind that, in general, the more data you have and the simpler your model is, the best chance you have at accurately predicting future outcomes: More data - The more observations you have and the more variables you have to choose from to include in your model, the more likely you are to generate an accurate predictive model. Note, however, large datasets with lots of missing data or data that have been incorrectly entered are not better than small, complete, and accurate datasets. Having a trustworthy dataset to build your model is critical. Simple Models - If you can accurately predict an individual’s height by only considering that person’s parents height, then go for it. There’s no need to include other variables if a single variable generates accurate predictions. A simple model that predicts accurately (regardless of the dataset in which you’re predicting) is better than a complicated model. 58.0.5.1 Regression vs. Classification Before we jump into discussing the various models you can use for predictive analysis, it’s important to first note the difference between regression and classification. Regression is used when you’re trying to predict a continuous variable. For example if you’re trying to predict an individual’s age, you would use regression. On the other hand classification is used for categorical variables, as it predicts which group an individual belongs to. An example of a classification would be predicting someone’s education level, as there are only a limited number of groups into which one would be. Regression vs. Classification With regards to machine learning, certain methods can be used for both regression and classification, while others are designed exclusively for one or the other. In this lesson we’ll discuss one regression model and one classification model. However, there are literally hundreds of models available for predictive modeling. Thus, it’s important to keep in mind that we’re really just scratching the surface here. 58.0.5.2 Linear Regression Just like in the previous lesson in inferential analysis, linear regression is an incredibly powerful method in machine learning! The concept here is the same as it was in the last lesson: we’re going to capitalize on the linear relationship between variables. However, instead of using linear regression to estimate something about a larger population, we’re going to use linear regression for prediction of a continuous variable. linear regression To better understand this, let’s use a conceptual example. Consider trying to predict a child’s age from their height. You’d likely expect that a taller child was older. So, let’s imagine that we’re looking here at the training data. We see the expected relationship between height and age in this scatterplot. training data for age and height example Using the training data, linear regression is then carried out to model the relationship. linear regression models the relationship between a child’s height and their age in the training data Now that we have our model, we no longer care about the individual data points in the training data. We’ll simply use the linear regression model to make our predictions. Our linear regression model will be used for prediction Then, in the future when we know a child’s height, we can return to our linear regression, supply it with the new child’s height and it will return the child’s age using the model we’ve built. predicting age from height using linear regression Conceptually, this is what will happen whenever we use linear regression for machine learning. However, it will be carried out mathematically, rather than graphically. This means you won’t have to look on the graph to see your predictions. You’ll just have to run a few lines of code that will carry out the necessary calculations to generate predictions. Additionally, here we’re using a single variable (height) to model age. Clearly, there are other variables (such as a child’s sex) that could affect this prediction. Often, regression models will include multiple predictor variables that will improve prediction accuracy of the outcome variable. 58.0.5.3 Classification and Regression Trees (CART) Alternatively, when trying to predict a categorical variable, you’ll want to look at classification methods, rather than regression (which is for continuous variables). In these cases you may consider using a classification and regression tree (CART) for prediction. While not the only classification method for machine learning, CARTs are a basic and commonly-used approach to prediction for categorical variables. Conceptually, when using a CART for prediction, a decision tree is generated from the training data. A decision tree branches the data based on variables within the data. For example, if we were trying to predict an individual’s education level, we would likely use a dataset with information about many different people’s income level, job title, and the number of children they have. These variable would then be used to generate the tree. For example, maybe the first branch would separate individuals who make less than 40,000 dollars a year. All of those in the training data who made less than 40K would go down the left-hand branch, while everyone else would go down the right-hand branch. Start to generate branches for your decision tree using the data to make decisions At each level, the data will continue to be split, using the information in the training data. Branches continue to be generated from the training data Finally, a full decision tree will be constructed, such that there will be a label for the variable we’re trying to predict at the end of each branch. labels are assigned at the end of each tree This CART will then be used for prediction in future samples. Thus, if you follow the path along the decision tree, for this example CART, an individual who made more than $40,000 a year, was in a manual labor profession, and had children, this CART would predict that that individual’s education level were “High School.” Predictions are then made following the decisions on the tree Again, this is conceptually and graphically how a CART works; however, when generating a CART yourself, it again only takes a few lines of code to generate the model and carry out the necessary math. 58.0.6 Model Accuracy A common saying is that prediction is hard, especially about the future. This is true in predictive analysis. Thus, it’s important to always carefully evaluate the accuracy of your model and to never overstate how well you are able to make predictions. Generally, if your predictions are correct, you’re doing well! If your predictions are wrong, you’re not doing ass well. But, how do we define “well”? 58.0.6.1 Error Rates To assess whether or not our predictive models are doing well, we calculate error rates. The two most common ways to assess how well our predictive models are doing are: RMSE (Root-mean-square Error) Accuracy We’ll note here that in order to assess error, you have to know the truth (the actual value) in addition to the predicted value. Thus, RMSE and Accuracy are assessed in the training and tuning data, where you know the actual value as well as the predicted value. 58.0.6.1.1 RMSE The root-mean-square error (RMSE) is a measure used to assess prediction error for continuous variables. Generally, we want to minimize error in prediction. Thus, a small RMSE is better than a large RMSE. RMSE Mathematically speaking, the RMSE is the square root of the variance. From earlier lessons, we know that variance has something to do with how confident we are in our estimate. Since we’re trying to determine how close our predictions are to the actual value, this seems like a good place to start. When we look at the equation, we can see that the difference between the predicted and actual values is calculated (Predicted - Actual) and that this value is then squared (Predicted - Actual)^2. These differences squared are then added for every individual in your dataset (that’s what the sigma, or big E says). This value (the sum of all the errors squared) is then divided by the number of individuals in your dataset (N). This square root of this value is then taken. This is how RMSE is calculated. We went through that description because we want to point out that when differences are squared (Predicted - Actual)^2, outliers, or samples whose prediction was far off from their actual value are going to increase the RMSE a lot. Thus, a few outliers can lead to really high RMSE values, even if all the other predictions were pretty good. This means it’s important to check to see if a few outliers (meaning a few bad predictions) are leading to a high RMSE value. 58.0.6.1.2 Accuracy Alternatively, to assess error in the prediction of categorical variables, accuracy is frequently used. Accuracy looks to determine the number of predictions that match their actual values. Accuracy The closer this value is to 100%, the better your predictive model was. The closer to 0%, the worse your model’s predictions are. Accuracy is a helpful way to assess error in categorical variables, but it can be used for numeric values too. However, it will only account a prediction “correct” if it matches exactly. In the case of age, if a sample’s age is 10 and the model predicts it to be 10, the model will say it’s been predicted correctly. However, if a sample’s age is 10 and it is predicted to be 9, it will be counted as incorrect, even though it was close. A prediction off by a year will be marked just as incorrect as a sample predicted off by 50 years. Due to this, RMSE is often opted for instead of accuracy for continuous variables. 58.0.7 Machine Learning Examples To better understand all of the concepts we’ve just discussed, we’ll walk through two examples, one for prediction of a continuous variable using linear regression and a second for prediction of a categorical value using a CART. 58.0.7.1 The caret package There is an incredibly helpful package available in R thanks to the work of Max Kuhn. As mentioned above, there are hundreds of different machine learning algorithms. Max’s R package caret has compiled all of them into a single framework, allowing you to use many different machine learning models via a single package. Additionally, he has written a very helpful book to accompany the package. We’ll be using this package throughout these examples. 58.0.7.2 Continuous Variable Prediction: Linear Regression For this example, we’ll keep it simple and use a dataset you’ve seen before: the iris dataset. This way you can focus on the syntax used in the caret package and the steps of predictive analysis. In this example, we’ll attempt to use the data in the iris dataset to predict Sepal.Length 58.0.7.2.1 Data Splitting As mentioned above, one of the first steps is often to take your dataset and split it into a training set and a tuning set. To do this, we’ll load the caret package and use the createDataPartition() function to split the dataset. ## install and load packages install.packages(&quot;caret&quot;) library(caret) library(dplyr) ## get Index for training set set.seed(123) trainIndex &lt;- createDataPartition(iris$Species, p = .7, list = FALSE, times = 1) ## split into training and tuning set iris_train &lt;- iris %&gt;% slice(trainIndex) iris_tune &lt;- iris %&gt;% slice(-trainIndex) ## take a a look str(iris_train) str(iris_tune) After running this code , if we take a look at the training and tuning datasets, we can see that 70% of our observations are in the training dataset and the other 30% are in the tuning dataset, as we specified. structure of training and tuning data 58.0.7.2.2 Variable Selection What if we first try to predict Sepal.Length in our training data from Sepal.Width. To do that, we provide the train function with the model and specify that the dataset we’ll be using is the iris dataset. Additionally, we let the train function know that we want to run linear regression (lm) and that we want to assess our model’s accuracy using the RMSE metric. ## train regression model set.seed(123) fit.lm &lt;- train(Sepal.Length ~ Sepal.Width, data = iris, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) After training the model, we take a look at our RMSE, and see that it is 0.82 for this dataset. ## look at RMSE fit.lm$results RMSE Using this model, we would then generate predictions of Sepal.Length in the tuning dataset using the predict() function. Since we know the actual Sepal.Length in the tuning set, these predictions can then be visualized using a scatterplot. ## make predictions in tuning data set predictions &lt;- predict(fit.lm, iris_tune) ## visualize results iris_tune %&gt;% mutate(predictions = predictions) %&gt;% ggplot() + geom_point(aes(Sepal.Length,predictions)) Scatterplot shows Sepal.Length is not predicted well from Sepal.Width alone Given the lack of correlation, we can see that this model does not predict sepal length in our tuning set well. In this first attempt, we specified which variable to use for prediction; however, what if we provided our regression model with all the variables in the dataset (specified by the . in the code here: ## train regression model set.seed(123) fit.lm2 &lt;- train(Sepal.Length ~ ., data=iris, method=&quot;lm&quot;, metric= &quot;RMSE&quot;) ## look at RMSE fit.lm2$results ## make predictions in tuning data set predictions2 &lt;- predict(fit.lm2, iris_tune) ## visualize results iris_tune %&gt;% mutate(predictions = predictions2) %&gt;% ggplot() + geom_point(aes(Sepal.Length,predictions2)) Scatterplot 58.0.7.2.3 Accuracy Assessment Now when we look at the results we visually see improvement in the Sepal.Length predictions within the tuning dataset, which is also reflected in the decreased RMSE (0.324). Here, by including additional variables (often referred to as features in machine learning), we see improved prediction accuracy. There are more robust ways than trying a number of different variables in your model to select which should be included in your predictive model. These will be covered in lessons in the advanced track of this Course Set. 58.0.7.2.4 Model Selection In this example (and the example below), we’ve pre-specified which model we were going to use for the example ahead of time. However, there are many different regression models from which we could have chosen and a number of parameters in each that can be tuned, each of which can improve the predictive accuracy of your model. Learning how to choose and tune the best model will be discussed in lessons in the advanced track of this Course Set; however, for now we’ll note that, as specified in the caret book, the train() function has a number of capabilities. It can: evaluate how different tuning parameters in the model affect performance choose the “optimal” model, given these parameters estimate model performance (given a training set) Here, we haven’t played around much with the tuning parameters; however, checking out the documentation on how to do this can lead to improved prediction as you generate predictive models on your own. 58.0.7.3 Categorical Variable Prediction: CART A more natural prediction model given this dataset may be to predict what Species a flower is, given its measurements. We’ll use the iris dataset to carry out this classification prediction here, using a CART. 58.0.7.3.1 Data Splitting Data splitting from above will be used here. Thus, our training set will still be iris_train and our tuning set iris_tune. 58.0.7.3.2 Variable Selection Given the relatively small nature of this dataset, we’ll build the CART using all of the data; however, further and more robust optimization of what variables are included in the model is possible within the caret package. Here we specify that we want to predict Species, that we want to use a CART to do so by setting the method to rpart, and that, since it’s a categorical variable, we’re going to use Accuracy to as our assessment metric. ## CART set.seed(7) fit.cart &lt;- train(Species~., data = iris, method = &quot;rpart&quot;, metric = &quot;Accuracy&quot;) ## look at Accuracy fit.cart$results ## make predictions in tuning data set predictions_cart &lt;- predict(fit.cart, iris_tune) table(iris_tune$Species, predictions_cart) 58.0.7.4 Accuracy Assessment table() output Here, we see that in the tuning data, the CART accurately predicted the Species of most flowers using the model generated from the training data; however, it did make two incorrect predictions (the 1s in the table). 58.0.8 Summary In this lesson we have covered the basics of what predictive analysis is, what types of prediction are commonly done, and how to carry out a predictive analysis using the caret package. Surely, this lesson has only introduced the very basics of machine learning, and there is still a lot out there to learn beyond what is in this lesson! 58.0.9 Additional Resources The caret Package book Example of machine learning using caret and the iris dataset for classification "],["stats-project.html", "Chapter 59 Stats Project", " Chapter 59 Stats Project In this project, we are going to try to answer this question with data: “What relationships are there between different countries’ social stats?”” Note that this is a very open ended question. Given that we have discussed different kinds of of statistical models and questions, what kind of question is this? To explore this question, we’ll return to the dataset we used from the United Nations in our project from the Cleaning the Data chapter. 59.0.1 Starting up this project Go to the DataTrail workspace. Return to your own DataTrail_Projects project in RStudio. For this project, go to the 05_Get_the_Stats folder. Click on the file countries_stats_project.Rmd to open this file. 59.0.2 Your objectives! To complete this project you’ll need to do a few things within this file. Go through this notebook, reading along. Fill in empty or incomplete code chunks when prompted to do so. Run each code chunk as you come across it by clicking the tiny green triangles at the right of each chunk. You should see the code chunks print out various output when you do this. At the very top of this file, put your own name in the author: place. Currently it says \"DataTrail Team\". Be sure to put your name in quotes. In the Conclusions section, write up responses to each of these questions posed here. When you are satisfied with what you’ve written and added to this document you’ll need to save it. In the menu, go to File &gt; Save. Now the nb.html output resulting file will have your new output saved to it. Open up the resulting countries_stats.nb.html file and click View in Web Browser. Does it look good to you? Did all the changes appear here as you expected. Upload your Rmd and your nb.html to your assignment folder (this is something that will be dependent on what your instructors have told you – or if you are taking this on your own, just collect these projects in one spot, preferably a Google Drive)! Pat yourself on the back for finishing this project! "],["sharing-results.html", "Chapter 60 Sharing Results 60.1 Learning Objectives", " Chapter 60 Sharing Results 60.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Understand what reproducibility means Value the importance of making your analyses reproducible Understand the importance of version control Navigate GitHub comfortably Create a GitHub repository and link it to RStudio Cloud Use the basic git command to version control an analysis Recognize the different methods available for communicating data science results Create a data analysis results report including all the important components Give a presentation using Google slides and be able to tailor this presentation for a particular audience Create a blog that fosters conversation Recognize common expectations for participating in meetings "],["reproducibility.html", "Chapter 61 Reproducibility 61.1 Examples about reproducibility 61.2 Reproducibility doesn’t happen overnight!", " Chapter 61 Reproducibility A key aspect of science, and by extension data science, is that it should be consistent. For a finding to be considered scientific, it should be able to be repeated and found again. If you drop the apple and it falls to the ground because of gravity, you should find observe the same thing if you drop the apple again. The more a scientific result can be found again and again and the inferences from it applied elsewhere, the more we can trust that result! When it comes to a data analysis however, the methods are a bit more involved than dropping an apple. So making a data science result repeatable is not something that happens without a lot of meticulous work and well-thought-out methods. A repeatable analysis is one that can be repeated by the same researcher, with the same code and same data and achieve the same result. However, a result being repeatable is not enough! As mentioned previously, data science is best done as a part of a community. This means, once you know you can reliably repeat your analysis and get the same result, you will want others to investigate the result for themselves. Other people will be able to see things that you have missed or may have different perspectives that are valuable (another reason why community is helpful in data science). If you can’t achieve the same result twice, then others won’t be able to replicate your result either. This means that you want others to be able to take your code and data and be able to re-run your analysis AND achieve the same result! A reproducible analysis is one that can be repeated by a different researcher, with the same code and same data and achieve the same result. 61.1 Examples about reproducibility Let’s say that Maria is a data scientist who’s found a really cool result in her data. She’s very excited about this result, done some polishing and now shares her analysis code and original data with a fellow data scientist, Avi. Avi is going to attempt to re-run Maria’s analysis. If he is able to get the same result, then Maria’s result will have been reproduced and considered that much more reliable. Reproducing an analysis result may seem like it should be easy and straightforward. But unfortunately it often is a very frustrating process for the person trying to run someone else’s code. Avi in this scenario is struggling to re-run Maria’s code. Perhaps eventually, with some help from Maria, Avi does finally get the code running and the data re-analyzed. Just because code doesn’t throw an error doesn’t mean it ran the same way or was successful. It’s also not uncommon that Avi will obtain a different result. Even if the result is only slightly different, this is worth Maria digging into; because now we don’t know whether Maria’s run of the analysis or Avi’s run is more reliable. And it could be the source of the difference is something we care about. Reproducibility and repeatability is not only important when working with other scientists, but it’s also important working with your future/past self! If you recall, we mentioned one of your closest collaborators is you of yesterday! But past you doesn’t reply to emails! This is why working toward reproducible analyses is also so important! It will save future you time and stress. At this point you may be thinking – my code runs fine right now, what could possibly happen between now and then that would cause it to break? We will discuss a variety of aspects of your analysis that you should keep in mind that will affect reproducibility. 61.1.1 Things that affect reproducibility 61.1.1.1 Usage A huge and unavoidable variable in data science is humans! Humans make errors all the time (and that is expected!). Sometimes people just make mistakes. Making your analysis reproducible also means trying to account for humans’ behavior and minimize the number of areas that are left up to human error. This means documenting things, automating others, and trying to communicate efficiently. And also sometimes, knowing when to give our brains a rest! (brains get tired and make mistakes more when they are tired). 61.1.1.2 Readable, efficient code Code that is clear and understandable is more easily able to be troubleshooted, re-run, and repurposed elsewhere. But don’t worry about writing perfect code on the first try. Writing code that works is step one in your project. Writing code that works AND is understandable takes more time and work. Some aspects of readable include 61.1.1.2.1 Use informative variables names Variables names like df or x or tmp don’t tell much about what is stored in there. variable names should give you an idea of what’s in there so when you are reading code it is easier to understand! 61.1.1.2.2 Aim for DRY code *This section is paraphrased from the ITCR Intro to Reproducibility course. DRY is an acronym: “Don’t repeat yourself” (Smith2013?). “I hate code, and I want as little of it as possible in our product.” – (Diederich2012?) If you find yourself writing something more than once, you might want to write a function, or store something as a variable. The added benefit of writing a function is you might be able to borrow it in another project. DRY code is easier to fix and maintain because if it breaks, its easier to fix something in one place, than in 10 places. DRY code is easier to understand because you don’t have to read the same thing twice, but also because you don’t have to read the same thing twice. ;) Here’s a modified example from (Bernardo2021?) for what DRY vs non-DRY code might look like: paste(&#39;Hello&#39;,&#39;John&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Susan&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Matt&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Anne&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Joe&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Tyson&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Julia&#39;, &#39;welcome to this course&#39;) paste(&#39;Hello&#39;,&#39;Cathy&#39;, &#39;welcome to this course&#39;) Could be functional-ized and rewritten as: GreetStudent &lt;- function(name) { greeting &lt;- paste(&#39;Hello&#39;, name, &#39;welcome to this course&#39;) return(greeting) } class_names &lt;- c(&#39;John&#39;, &#39;Susan&#39;, &#39;Matt&#39; ,&#39;Anne&#39;, &#39;Joe&#39;, &#39;Tyson&#39;, &#39;Julia&#39;, &#39;Cathy&#39;) lapply(class_names, GreetStudent) Now, if you wanted to edit the greeting, you’d only need to edit it in the function, instead of in each instance. DRY Programming Practices by (Klinefelter2016?). Keeping R Code DRY with functions by (Riffomonas2021?). Write efficient R code for science by (Joseph2017?). 61.1.1.2.3 Code that follows a style As discussed elsewhere, code is much easier to read if it follows a style. Much like how sentences are easier to read if they follow punctuation and grammar conventions. 61.1.1.3 Frequency of re-running code What’s a simple way to know if your analysis will re-run? Re-run it all the time while you are developing it. As we’ve mentioned, if you can’t run your analysis, other people won’t be able to for sure. But if you can reliably re-run your analysis then you are well on your way! This is also a very handy development skill. Frequently re-running your code, especially in the early development stages of your code, can help you catch problems earlier before they get out of control! Second step of this is having others try to understand, evaluate, run and re-run your code – after all that is the true test of reproducibility! 61.1.1.4 Documentation As you are in the thick of writing your analysis, everything probably makes perfect sense to you. But if you come back to it a week after not looking at it, would you know where you left off? What about a month later? A year? Documentation is a great tool to combat irreproducibility. Not only can documentation save future you time and frustration but it can also help others get going and help with your project. Do you remember how we talked about READMEs? They are a crucial part of making a reproducible analysis. Code comments are also crucial! Err on the side of over-documenting! You can save yourself so much time! 61.1.1.5 Manual steps Pointing and clicking to do a thing can be tempting to do when you are just trying to get a thing done. But if it is a critical step in the analysis don’t do it! Every step that is needed to re-run an analysis should be in a script. Ideally to re-run your analysis you will have one main script that will run the whole thing by one command. If a step absolutely can’t be done through a script, then the next best thing is to document the heck out of that step and put it in large bold letters! See the above section about documentation. 61.1.1.6 Software and package versions Software and library packages are always being developed. Not only does your own use of your computer mean that you are probably updating things on your computer all the time, but even if you don’t change anything on your computer. Software and packages deteriorate and are deprecated over time as things are continually being developed on the world wide web. These packages influence how your analysis runs and ultimately your final results. Let’s return to our example of Maria and Avi. It’s very likely that the reason Avi might get a slightly different result when he re-runs Maria’s analysis is that he has different software packages and versions installed on his computer. Maria developed her analysis on her computer with a variety of different software packages and versions installed. It is impossible that Avi has the same exact set up on his computer by chance. So how can you keep this from totally breaking your code? Well short answer is you can’t fully. But you can do things to help. One thing is printing out session info (Something we’ve been trying to illustrate in examples throughout the course). How does session info help? If we have both Maria and Avi’s session info we can compare them! This helps give us clues as to why they may have gotten slightly different results. We can further dig into these package version differences and try to pinpoint what happened that influenced the results. We can further dig into these package version differences and try to pinpoint what happened that influenced the results. 61.1.1.7 Data changed Did the data get updated and we missed that? If so that’s a very understandable reason for a result not being reproducible. Just make sure to look into those data changes and see if that means your analysis should be altered at all. Also what are the reason for those data changes. Digging into questions like these is a skill that you want to build on for improving your data science skills! 61.1.1.8 File organization As discussed elsewhere, file organization is a big part of making a project understandable and also reproducible! Keep your organizational schemes in mind. And remember that organizing your files is a great use of time and crucial to a successful and reproducible project! 61.1.1.9 Version control The last thing on this list is what we are going to build on for a lot of this next section. As you are working on your files, you will likely find they will change a lot. You may have a tendency to want to keep all the different iterations of your analysis. But this can lead to some organizational problems. Today you might remember which file was particularly important for different things, but future you might have no idea what today you was up to. It’s great to keep a record of files and how your analysis has developed, but how do you keep it from becoming unruly? The antidote for this problem is version control. We will give you the methods and techniques for using version control for your project. Version control is a method of tracking files throughout the development of your project. We will cover this more in this section. 61.1.1.10 Share you code If you want to know if others can run your code, the best way to find out is to ask others! Others can help you improve your analysis and see things you didn’t notice. Data science is done best as a community activity! We will discuss tactics for this goal as well! 61.2 Reproducibility doesn’t happen overnight! This may sound overwhelming to implement. But don’t fear! A reproducible analysis is not written on the 1st try (or the second, third fourth… 10th edits to your analysis). No analysis is perfect! The idea here is to encourage you to think about these things and work on them iteratively, over time so that the analyses you work on will become more and more reproducible sooner into your projects. No analysis is perfect! But the best advice for making your analyses more reproducible is by working with other data scientists! Share your results and ask lots of questions! After all, data science is all about questions! 61.2.1 Additional Resources Building a Data Pipeline from Scratch, by Alan Marazzi Parameterized R Markdown reports with RStudio Connect RMarkdown Parameterized Reports Chapter 15: Parameterized Reports in R Markdown: The Definitive Guide, by Yihui Xie, J.J. Allaire, and Garrett Grolemund "],["version-control-1.html", "Chapter 62 Version Control", " Chapter 62 Version Control In this lesson, we’ll give you a basic understanding of version control. We’ll look at what version control is, some of its benefits, and give an introduction to Git and GitHub, the main way you will interface with version control systems in these courses. 62.0.1 What is version control? One common mantra from data analysis and writing code is “save early, save often”. What is meant by this is that you do not want to create content without saving it frequently. This way, if you lose your internet connection, or your computer crashes, you will not lose the hard work you have put in. On the other hand, sometimes we create some content, only to realize that it is not what we want, and we want to undo the changes we have just made. Version control allows us to save frequently, while also taking snapshots of our work at any point in time, allowing us to go back to a previous version if necessary. A good version control system will do this without you having to save multiple copies of your work (avoiding having “myFile_version1”, “myFile_version2”, etc. saved to your computer) or having to keep track of what the latest version of the file is. If you’ve ever used the “Track changes” feature in Microsoft Word, you have seen a rudimentary type of version control, in which the changes to a file are tracked, and you can either chose to keep those edits or revert to the original format. Version control systems, like Git, can be thought of as a more sophisticated “Track changes” - in that they are far more powerful and are capable of meticulously tracking successive changes on many files, with potentially many people working on the same groups of files at the same time. If you’ve ever worked collaboratively on a document before, this comic from PHD Comics might resonate with you. Hopefully, once you’ve mastered version control software, Paper_Final_FINAL2_actually_FINAL.docx will be a thing of the past for you! 62.0.2 Version control on your own As mentioned above, even if you are the only one working on a project, saving frequently and preserving snapshots of your work at a specific time prevents you from losing work or making changes that cause the work you have already done to not work anymore. Or, you might be keeping multiple, very similar, copies of a file. And this could be dangerous - you might start editing the wrong version, not recognizing that the document labelled “FINAL” has been further edited to “FINAL2” - and now all your new changes have been applied to the wrong file! Version control systems help to solve this problem by keeping a single, updated version of each file, with a record of all previous versions AND a record of exactly what changed between the versions. 62.0.3 Version control for collaboration or sharing One advantage of using version control for your projects is that multiple users can work on different parts of the same project at the same time. Each individual can edit different files that make up a project simultaneously, and all the results can be saved in the same place. Two (or more) individuals can even work on editing the same file simultaneously, and a good version control system will allow them to then resolve any differences between the two file versions that result. Additionally, since version control systems keep a record of all changes made to the files, this can be of great help when you are collaborating with many people on the same files - the version control software keeps track of who, when, and why those specific changes were made. It’s like “Track changes” to the extreme! 62.0.4 Summarizing the benefits of version control systems It gives you an easy way of saving a backup of your work as you go, making sure you don’t lose anything you have created. It allows you to go back to an earlier version of your work if something gets broken due to the changes you have made. It allows multiple people to edit code at the same time, without worrying that what they’re doing might be messing someone else’s code up. 62.0.5 What is Git? Why should you use it? In this course, we will introduce version control software called Git. Git is a free and open source version control system. It was developed in 2005 and has since become the most commonly used version control system around! StackOverflow surveyed over 60,000 respondents on which version control system they use, and as you can tell from the chart below, Git is by far the winner. Git is far and away the most commonly used version control system according to StackOverflow As you become more familiar with Git and how it works and interfaces with your projects, you’ll begin to see why it has risen to the height of popularity. One of the main benefits of Git is that it keeps a local copy of your work and revisions, which you can then edit offline, and then once you return to internet service, you can sync your copy of the work, with all of your new edits and tracked changes to the main repository online. And since Git is automatically taking these snapshots, there is no need for you to save your own copy with different versions or different dates. Additionally, since all collaborators on a project have their own local copy of the code, everybody can simultaneously work on their own parts of the code, without disturbing that common repository. Another big benefit that we’ll definitely be taking advantage of is the ease with which RStudio and Git interface with each other. 62.0.6 What is GitHub? GitHub is a website that provides an interface to a cloud-based repository management system. In other words, GitHub is an online interface for Git. Git is software used locally on your computer to record changes. GitHub is a host for your files and the records of the changes made. You can sort of think of it as being similar to DropBox - the files are on your computer, but they are also hosted online and are accessible from any computer. GitHub has the added benefit of interfacing with Git to keep track of all of your file versions and changes. Another way of thinking of it is as a big room with lots of large filing cabinets. Each filing cabinet contains the files associated with a particular project. These files have been created and are being edited by one or more people who have access to that particular filing cabinet or repository. GitHub allows you to control who can see and access your files. GitHub uses the version control system Git, described above, to manage version control, and provides users with a web-based interface for creating projects, sharing them, updating code, etc. In this course, we will be teaching you how to do version control using GitHub. We will introduce you to the basics of working with GitHub, both through your browser and through RStudio. For our purposes, you will not need to think about Git itself, and will instead just use GitHub. 62.0.7 Version control vocabulary There is a lot of vocabulary involved in working with Git, and often the understanding of one word relies on your understanding of a different Git concept. Take some time to familiarize yourself with the words below and read over it a few times to see how the concepts relate. Repository: Equivalent to the project’s folder/directory - all of your version controlled files (and the recorded changes) are located in a repository. This is often shortened to repo. Repositories are what are hosted on GitHub and through this interface you can either keep your repositories private and share them with select collaborators, or you can make them public - anybody can see your files and their history. Commit: To commit is to save your edits and the changes made. A commit is like a snapshot of your files. Git compares the previous version of all of your files in the repo to the current version and identifies those that have changed since then. For those that have not changed, Git maintains that previously stored file, leaving it untouched. For those that have changed, Git compares the files, logs the changes and uploads the new version of your file to GitHub. We’ll touch on this in the next section, but when you commit a file, typically you accompany that file change with a little note about what you changed and why. When we talk about version control systems, commits are at the heart of them. If you find a mistake, you revert your files to a previous commit. If you want to see what has changed in a file over, you compare the commits and look at the messages to see why the changes were made and who made them. Push: Updating the repository with your edits. Since Git involves making changes locally, you need to be able to share your changes with the common, online, repository. Pushing is sending those committed changes to that repository, so now everybody has access to your edits. Pull: Updating your local version of the repository to the current version, since others may have edited in the meanwhile. Because the shared repository is hosted online and any of your collaborators (or even yourself on a different computer!) could have made changes to the files and then pushed them to the shared repository, you are behind the times! The files you have locally on your computer may be outdated, so you pull to check if you are up to date with the main repository. Staging: The act of preparing a file for a commit. For example, if since your last commit you have edited three files for completely different reasons, you don’t want to commit all of the changes in one go, or your message on why you are making the commit and what has changed will be complicated, since three files have been changed for different reasons. So instead, you can stage just one of the files and prepare it for committing. Once you’ve committed that file, you can stage the second file and commit it. And so on. Staging allows you to separate out file changes into separate commits. Very helpful! To summarize these commonly used terms so far and to test whether you’ve got the hang of this, files are hosted in a repository that is shared online with collaborators. You pull the repository’s contents so that you have a local copy of the files that you can edit. Once you are happy with your changes to a file, you stage the file and then commit it. You push this commit to the shared repository. This uploads your new file and all of the changes and is accompanied by a message explaining what changed, why and by whom. Visual representation summarizing repository, commit, push, and pull Branch: When the same file has two simultaneous copies. When you are working locally and editing a file, you have created a branch where your edits are not shared with the main repository (yet) - so there are two versions of the file: the version that everybody has access to on the repository and your local edited version of the file. Until you push your changes and merge them back into the main repository, you are working on a branch. Following a branch point, the version history splits into two and tracks the independent changes made to both the original file in the repository that others may be editing, and tracking your changes on your branch, and then merges the files together. Merge: Independent edits of the same file are incorporated into a single, unified file. Independent edits are identified by Git and are brought together into a single file, with both sets of edits incorporated. But, you can see a potential problem here! If both people made an edit to the same sentence that precludes one of the edits from being possible, we have a problem! Git recognizes this disparity (conflict) and asks for user assistance in picking which edit to keep. Conflict: When multiple people make changes to the same file and Git is unable to merge the edits. You are presented with the option to manually try and merge the edits or to keep one edit over the other. A visual representation of these concepts, from https://www.atlassian.com/git/tutorials/using-branches/git-merge Clone: Making a copy of an existing Git repository. If you have just been brought on to a project that has been tracked with version control, you would clone the repository to get access to and create a local version of all of the repository’s files and all of the tracked changes. Fork: A personal copy of a repository that you have taken from another person. If somebody is working on a cool project and you want to play around with it, you can fork their repository and then when you make changes, the edits are logged on your repository, not theirs. 62.0.8 Best practices It can take some time to get used to working with version control software like Git, but there are a few things to keep in mind to help establish good habits that will help you out in the future. One of those things is to make purposeful commits. Each commit should only address a single issue. This way if you need to identify when you changed a certain line of code, there is only one place to look to identify the change and you can easily see how to revert the code. Similarly, making sure you write informative messages on each commit is a helpful habit to get into. If each message is precise in what was being changed, anybody can examine the committed file and identify the purpose for your change. For example, if you edited your code to use a new dataset, an informative commit message may be “updated file to use dataset from www.wheredatacamefrom.com.” On the other hand, an unhelpful commit messaged would be “data” or “update.” These types of commit messages are to be avoided. Further, by using helpful commit messages, if you are looking for a specific edit you made in the past, you can easily scan through all of your commits to identify those changes related to the desired edit. You don’t want to get in the same habit that xkcd has! Finally, be cognizant of the version of files you are working on. Check that you are up to date with the current repo by frequently pulling. Additionally, don’t hoard your edited files - once you have committed your files (and written that helpful message!), you should push those changes to the common repository. If you are done editing a section of code and are planning on moving on to an unrelated problem, you need to share that edit with your collaborators! 62.0.9 Summary Now that we’ve covered what version control is and some of the benefits, you should be able to understand why we have an entire course dedicated to understanding it! Additionally, we looked at what Git and GitHub are, and then covered much of the commonly used (and sometimes confusing!) vocabulary inherent to version control work. We then quickly went over some best practices to using Git – but the best way to get a hang of this all is to use it! Hopefully you feel like you have a better handle on how Git works than the people in this xkcd comic! "],["github.html", "Chapter 63 GitHub 63.1 Create a GitHub Account", " Chapter 63 GitHub Now that we have a handle on what version control is and how we can interface with it, we’ll take some time to look at GitHub and get familiar with their website. 63.1 Create a GitHub Account GitHub is a website that hosts computer code and allows for version control. We’ll get back to what version control is later, but as for now, know that GitHub is where you’ll be ‘saving’ all of the code you write. It’s also a place where you can look at other people’s code. And, throughout this program, you’ll realize that you can learn a lot from other people’s code! To get a GitHub account, first type www.github.com into the web address bar at the top of your Chrome window and hit ‘Enter’. GitHub web address You will be brought to a page where you should fill in your information. As with the other accounts, try to use the same Username if possible. Enter your Gmail Email address. And, create a password that cannot be easily guessed by others. Then, click ‘Sign up for GitHub.’ One final note about GitHub usernames in particular. This will be used for your website (which you’ll build later) and all the code you write. You’ll use GitHub a lot, so this is a case where it is particularly helpful to choose a good username, particularly one that has something to do with your name and not much else. For example, the person writing this lesson is named Shannon Ellis. Her GitHub username is “ShanEllis.” While it is possible to change your GitHub username down the line, it’s a bit of a pain, so choose wisely now! You now have a GitHub account! 63.1.1 Logging in to GitHub Use your account username and password to log in to GitHub (if you are not still logged in). To log in, go to github.com, where you will be presented with the homepage. If you aren’t already logged in, click on the “Sign in” link at the top. GitHub’s homepage at github.com Once you’ve done that, you will see the log in page where you will enter in your username and password that you created in the first course in this series. GitHub’s log in page Once logged in, you will be back at github.com, but this time the screen should look like this: If you are logged in, GitHub should look like this 63.1.2 The Homepage We’re going to take a quick tour of the GitHub website, and we’ll particularly focus on these sections of the interface: User settings Notifications Help files The GitHub guide Following this tour, we’ll make your very first repository using the GitHub guide! Some major features of GitHub 63.1.3 User Settings Now that you’ve logged on to GitHub, we should fill out some of your profile information and get acquainted with the account settings. In the upper right corner, there is a an icon with an arrow beside it, click this and go to “Your profile” Where to find user settings This is where you control your account from and can view your contribution histories and repositories. Where to find user settings Since you are just starting out, you aren’t going to have any repositories or contributions yet - but hopefully we’ll change that soon enough! What we can do right now is edit your profile. To orient you a bit, the Overview tab provides a summary of your work on GitHub. You can showcase up to 6 of your repositories by clicking the link labeled “Customize your pinned repositories.” When you are logged into GitHub, the Repositories tab lists all public and private repositories you have. If your are not logged in, it will only show your public repositories. The Stars tab shows repositories you have starred, similar to bookmarking a web page in your browser. If you find the contents of a repository interesting, you can go to the main repository page and click the Star button near the top right. The Followers and Following tabs shows users that you follow and who follow you for activity updates. Following others is is a nice way to explore other people’s work. Go to “Edit profile” along the lefthand edge of the page. Here, take some time and fill out your name and a little description of yourself in the “Bio” box, and if you like, upload a picture of yourself! When you are done, click “Update profile” Your profile page Along the lefthand side of this page, there are many options for you to explore. Click through each of these menus to get familiar with the options available to you. To get you started, go to the account page. Your account page Here, you can edit your password or if you are unhappy with your username, change it. Be careful though, there can be unintended consequences when you change your username - if you are just starting out and don’t have any content yet, you’ll probably be safe though. The Account tab lets you change your username or password. The Emails tab lets you modify what e-mail address(es) to use for notifications and has some privacy settings relating to where your e-mail addresses can appear publicly. The Notifications tab allows you to fine tune what GitHub updates send notifications and where they are sent. Continue looking through the rest of the personal setting options on your own. When you are done, go back to your profile. Once you’ve had a bit more experience with GitHub, you’ll eventually end up with some repositories to your name. To find those, click on the “Repositories” link on your profile. For now, it will probably look like this: Your repositories page 63.1.4 Notifications Next, we’ll check out the notifications menu. Along the menu bar across the top of your window, there is a bell icon, representing your notifications. Click on the bell. Your notifications Once you become more active on GitHub and are collaborating with others, here is where you can find messages and notifications for all the repositories, teams, and conversations you are a part of. 63.1.5 Help Files Along the bottom of every. single. page. there is the “Help” button. GitHub has a great help system in place - if you ever have a question about GitHub, this should be your first point to search! Take some time now and look through the various help files, and see if any catch your eye. GitHub’s help files 63.1.6 Summary In this lesson we looked at GitHub and its interface. We took a tour of the website and its interface. We customized your profile to give people some more information on who you are. We made our very first repository by following the GitHub guide and explored the various options GitHub provides for exploring repositories. "],["creating-a-repository.html", "Chapter 64 Creating a Repository", " Chapter 64 Creating a Repository Now that you have learned about version control and the GitHub website, we will do some actual work with the GitHub website to create a new repository and set up your user profile. 64.0.1 What is a Repository? First let’s review what a repository is. A repository houses the entirety of a project. Imagine a filing cabinet. A repository for this project would be the filing cabinet itself. Inside the cabinet are various folders and files that make up the project. As you begin working on various projects, each one will likely have its own repository, and any work that you do will be housed in that repository. In the first part of this lesson, we will discuss how to create a repository using the GitHub website. 64.0.2 How Do I Create a GitHub Repository? 64.0.2.1 Step 1 First, navigate to the GitHub website: GitHub.com. Click the green button on the right hand side of the page that allows you create a new repository. Creating a new repository 64.0.2.2 Step 2 You will be taken to a new page where you can set up information about this new repository. Most importantly, create a descriptive name for the repository that relates to your project. This will help you remember what repository holds your files for this project if you have many repositories or come back to the project at a later time. Including a description for your repository is optional. You have a choice to make your repository public or private. Public repositories are viewable by the public. Anyone can see what code you have used for this project. You can still control who can contribute to this particular repository, but the repository is open to being forked by other people who may want to build off of your work in their own repositories. Private repositories are not viewable by the public, and you can still control who can contribute to this particular repository. It is generally a nice choice to make a repository public to share your work with others, but if you are working with sensitive information, it would be best to make the repository private. Keep in mind that you can have private repositories on GitHub only if you are a paid subscriber. You can leave the other options at their default settings. When you are satisfied with these settings, click the “Create repository” button at the bottom. Setting up a new repository And you’re done! You will be taken to a page that looks as below. This page contains information about the commands that will be useful when adding files to your repository later on. We will cover the use of these commands from RStudio Cloud in subsequent lessons. Landing page 64.0.3 Adding a README File If you click on the link labeled README indicated in the picture below, you can add a README file to your currently empty repository. The README file is the first thing someone sees when they navigate to the page for your repository. It is useful for providing an overview for what is contained within it. Creating a README When you click on that link, you will be taken to a page pictured below where you can edit the text of this file and preview what it will look like on the GitHub website. It has been automatically filled in with the title of our repository and the optional description we entered previously. Feel free to add additional description. The README file is a markdown document so you will have to use the markdown syntax you learned in the last course to edit this document. Editing a README When you are finished editing your README file, scroll to the bottom of the page to commit your changes. Here, GitHub automatically suggests that “Create README” is your commit messages. As this is informative of what you have done, you do not need to add additional text; however, you’re welcome to make this commit message even more informative if you’d like. Committing changes Click the “Commit new file button” to add this README to your repository. You will be taken to the homepage for your repository, as pictured below. Repository homepage Congratulations! You have created and set up your first repository! 64.0.4 The GitHub Guide GitHub also recognizes that this can be an overwhelming process for new users, and as such have developed a mini tutorial to get you started with GitHub. You can go through this guide to get further process setting up repositories on GitHub! "],["cloning-a-repository.html", "Chapter 65 Cloning a Repository", " Chapter 65 Cloning a Repository Now that you have learned how to create a GitHub repository, you will learn how to obtain a copy of that repository in a location that you use for writing code such as RStudio Cloud. This process is called “cloning” a repository from GitHub. 65.0.1 Step 1: Obtain the URL for the repository to clone Navigate to the GitHub webpage for your repository. This URL always has the form https://github.com/github_username/repository_name On this page, you will see a button on the right hand side that says “Clone or download”. When you click this, highlight the URL in the box, and copy it to the clipboard. Make sure it starts with “https” rather than “git@”. If you see “git@”, click the link in the top right corner of the box that says “Use HTTPS”. 65.0.2 Step 2: Use the RStudio interface to clone the repository In RStudio cloud, go to your workspace by clicking the appropriate button on the left side navigation bar. Click the New Project button and choose New Project from Git Repository Place the URL that you copied in Step 1 in the URL of your Git Repository box, and click OK. You will a Deploying Project progress screen, typically after a few seconds, the RStudio interface for your project will appear. Also note that in the bottom right hand corner of RStudio, in the Files pane, you will have the files that are in your GitHub repository, now available in this RStudio cloud workspace. 65.0.3 Step 3: Set up GitHub Credentials In order to be able to access everything in your GitHub repository from RStudio cloud, you will need to set up GitHub credentials. You should only need to do this once per project. In your RStudio interface, make sure that you are in the Console tab. Now use the command below to install the package usethis. Copy and paste it in the Console window and click Enter on your keyboard. This package will help us manage our GitHub credentials from RStudio more easily. install.packages(&quot;usethis&quot;) This will take a minute or so to install. Remember that red text doesn’t mean an error necessarily. Now to use this package, we need to attach its library using the following command: library(&quot;usethis&quot;) RStudio and GitHub require you make a special fancy password to use as credentials called a GitHub Personal Access Token (sometimes abbreviated as a “PAT”). To create a ‘PAT’ from RStudio we can run this handy command: usethis::create_github_token() Running this command will open up a window in your GitHub that will ask you for your password. Login to GitHub as you normally would. This will open up a page in GitHub for creating a New personal access token. Underneath the Note put something that reminds you what this PAT is for. Something like RStudioCloud Access. (Note that each PAT you make needs its own unique Note though). Underneath the Select scopes section you don’t need to do anything, usethis package already chose the permissions we need. Scroll all the way down on this page and click Generate Token. You’ve created your first PAT! Do not close this window, keep it handy for now. Note that in the image below we blocked out our PAT, but yours will show a jumble of letters and numbers Return back to your RStudio Cloud project while keeping your PAT handy. In the Console window, run this command: gitcreds::gitcreds_set() It will ask you to ? Enter password or token. Copy your PAT and paste it into the command window and press Enter. After you enter your PAT here you should get a message like: -&gt; Adding new credentials... -&gt; Removing credetials from cache... -&gt; Done. You are now free to close that GitHub PAT window. Note that you will want to be very careful with your PAT. Do not share it or put it anywhere that others could see it or access it! Now we also need to add your username and email to the RStudio GitHub credentials by running a command like below. But replace the example username and email with what corresponds to your GitHub account. use_git_config(user.name = &quot;Jane&quot;, user.email = &quot;jane@example.org&quot;) Run this in the Console tab as well and click Enter. Now to double check that everything is set, we can run this command to have the usethis package echo back our credentials: git_sitrep() It will give you output that looks similar to this: (but note it will have your own user name, and repository name and etc.) Git config (global) • Name: &#39;Jane&#39; • Email: &#39;jane@example.org&#39; • Global (user-level) gitignore file: &lt;unset&gt; • Vaccinated: FALSE ℹ See `?git_vaccinate` to learn more • Default Git protocol: &#39;https&#39; • Default initial branch name: &lt;unset&gt; GitHub • Default GitHub host: &#39;https://github.com&#39; • Personal access token for &#39;https://github.com&#39;: &#39;&lt;discovered&gt;&#39; • GitHub user: &#39;Jane&#39; • Token scopes: &#39;gist, repo, user, workflow&#39; • Email(s): &#39;jane@example.org (primary)&#39; ✖ Local Git user&#39;s email (&#39;jane@example.org&#39;) doesn&#39;t appear to be registered with GitHub. Git repo for current project • Active usethis project: &#39;/cloud/project&#39; • Default branch: &#39;master&#39; • Current local branch -&gt; remote tracking branch: &#39;master&#39; -&gt; &#39;origin/master&#39; GitHub remote configuration • Type = &#39;theirs&#39; • Host = &#39;https://github.com&#39; • Config supports a pull request = FALSE • origin = &#39;JaneEverydayDoe/first_project&#39; (can not push) • upstream = &lt;not configured&gt; • Desc = The only configured GitHub remote is &#39;origin&#39;, which you cannot push to. If your goal is to make a pull request, you must fork-and-clone. `usethis::create_from_github()` can do this. Read more about the GitHub remote configurations that usethis supports at: &#39;https://happygitwithr.com/common-remote-setups.html&#39; You should see that Name, email have your credentials set as well as a Personal access token for 'https://github.com': '&lt;discovered&gt;' You can run git_sitrep() at anytime to see what your credentials and settings are. Yay! Now you should be able to use GitHub from RStudioCloud! 65.0.4 Directory/Folder Organization A big part of staying organized with your files is understanding how folders in your project are organized. Also important is the concept of a working directory. Whenever you are working in R or the Terminal, files are housed in some folder. This folder is called the working directory. Knowing this is important so that you know how to specify paths to other important folders in your project. We can see what the current working directory is in a number of ways. In the image below, we can see from the Terminal prompt that the working directory is /cloud/project. We can also see this in the Files pane. We see that the first level folder is cloud and that the second level folder is project. Congratulations! You have cloned your first repository! "],["pushing-and-pulling-changes.html", "Chapter 66 Pushing and Pulling Changes", " Chapter 66 Pushing and Pulling Changes Now that you have learned how to create a repository and clone it, it’s time to start using repositories to manage projects and work with collaborators. In this lesson you will learn about new git commands for publishing changes so that others can see them. You will also learn a git command for incorporating changes made by others in your local copy of a repository. 66.0.0.1 Get Status A helpful git command is git status. If you type this in the terminal and press “enter” on your keyboard, the files that have been modified or added since you last updated changes on GitHub will be displayed. However, before working with the git commands that will track and publish our changes, we need to change the working directory in the Terminal to be the first_project folder. We can do this with the cd command. Using the command cd PATH changes the working directory to the folder specified by PATH. For example, if the first_project folder contained a folder called analysis, we could set our Terminal working directory to this folder with cd first_project/analysis. To continue change the working directory by typing the command cd first_project in the prompt. Changing the working directory in the Terminal As you’ve just cloned a repository in the last lesson, there are no changes yet. You can type git status at any time in the Terminal to see what files have been modified locally that are not yet on GitHub. This is particularly helpful when you want to decide which files to stage, which is discussed in more detail later in this lesson. git status output 66.0.1 Pushing Over the course of working on a project, you will be creating a number of files containing code, results, documentation, or other information. How do you add these files to the Git version control system and make this information available to the public? In the first part of this lesson, you will learn about three git commands that allow you to do this: add, commit, and push. 66.0.1.1 Create a File Let’s start by creating a file within the project. Below we’ve created a text file containing a list of tasks. This is saved as tasks.txt within the first_project folder. This is the repository that we cloned in the previous lesson. Creating a file in our project repository 66.0.1.2 Staging and Committing Now that we have added a file to this repository, let’s put it under version control and publish these changes. First we need to move to the Terminal pane. Recall that the text at the beginning of the line is called the prompt. The Terminal prompt shows the current working directory within the Terminal. Initially, the working directory is the /cloud/project folder. We can see the same information by entering the pwd command. We can use the ls command to list the files and folders that are in the current working directory. If you haven’t previously added any files or folders here, you’ll see that the first_project folder is the only thing present. We will first use the git add command to tell Git that the tasks.txt file is to be tracked for version control. The syntax for the add command is: git add file_or_folder1 file_or_folder2 This “stages” the the specified files or folders, adding them to the list of directories and files that should be added to GitHub. Any number of files and/or folders can be specified in this way. To add the tasks.txt file, we use git add tasks.txt While you only have a file or two to add at this point and typing them each individually is not much work, as you start work on larger projects and edit multiple files at a time, typing out each file you want to add individually will become a pain. To get around this, you can specify that you want to stage multiple files at once using the following syntax: git add . Here, a period has been added after git add. What this does is stage (add) new and modified files. However, this does not stage files that have been deleted. Alternatively, to add modified and deleted files but to not include new files, you would use the following syntax: git add -u To add all files, adding new, modified, and deleted files, you would use the following syntax: git add -A The final shortcut we’ll discuss for staging files is the ability to use the wildcard () character with git add. Here, wildcard character () followed by .csv specifies that you want to stage any file that has the file extension “.csv”. git add *.csv You could alternatively use git add *. However, if you remember back to the lesson on removing files, you were cautioned against ever using rm * because you can very easily but accidentally delete files you didn’t mean to delete. The same logic works here. git add * will add everything, including hidden files, and thus should be used with caution. Instead, use options discussed above to stage your files. git add summary table Now that we have let git know that which file should be tracked for version control, we will actually record those changes with git commit. The commit command takes the files that we added with add and updates the Git version history record with these changes. When we use the commit command we also supply a descriptive message about the changes that were made by specifying the -m option followed by the message in quotes. You can write anything as the message, however, since you may go back to your previous commits some day, it’s considered good practice to write a descriptive message that has some information about the nature of the changed you made. git commit -m &quot;Add task list&quot; Adding a file and committing changes 66.0.1.3 Verifying Your Identity On RStudio Cloud, you may have to verify who you are when you make changes to a repository. You’ll know you have to do this when you try to git commit and you get an error that starts with: “Please tell me who you are,” followed with some suggestions about using git config. When this occurs, you’ll be prompted to type the following into the terminal: git config user.name &quot;UserName&quot; git config user.email &quot;email@domain.com&quot; To do this, you’ll first specify your username. To do this, if your GitHub username were JaneEverydayDoe you would type the following and then press enter: git config user.name &quot;JaneEverydayDoe&quot; If your email address connected to your GitHub account were “janeeverydaydoe@gmail.com”, you would then type the following and press enter: git config user.email &quot;email@domain.com&quot; Note: If you’re asked for your password, you should type in your GitHub password and hit Enter, but know that you will not actually see the characters show up on the screen as you type them. 66.0.1.4 Publishing changes (pushing) We are finally ready to publish our changes to a remote repository. We’ve been doing work in a local copy of the repository on RStudio Cloud. This personal copy is called a local repository. To make changes available to others, there needs to be an external version of the repository accessible by others. This is called a remote repository. Our remote repository is the one available on GitHub. To publish our changes, we use the git push command. Because we cloned this repository from GitHub, Git automatically knows that this is the remote repository. A local repository can have multiple remotes, and it is possible to push changes to a specified remote. We will not cover this here though. After you run git push, you will see some status text and you will be prompted for your GitHub username and password. After you enter these, the push will be complete. 66.0.2 Pulling While working on a project, we may have a collaborator working on files in the same repository. If they create and edit files in their own local repositories and push changes to our shared remote repository on GitHub, we will want to incorporate their changes into our files. We can do this with git pull. For example, a collaborator might have added an additional task to the tasks.txt file. Seeing on GitHub that a file has been edited When we run the git pull command, the changes present in the remote repository are incorporated into our local repository. So when we open the tasks.txt file, we will see the additional task. Pulling changes from the remote to our local repository 66.0.3 Practice To get more practice with these concepts, as we know they are not the easiest, we suggest you go practice at GitHub Skills. Here you can practices your git commands and get feedback as to what you’ve done correctly and where you steered off course. It’s a great way to get better at using git commands before you move onto the quiz! "],["organization-with-issues-on-github.html", "Chapter 67 Organization with Issues on GitHub", " Chapter 67 Organization with Issues on GitHub We’ve covered the basics of how to set up a repository for a project and synchronize changes between local and remote versions of the repository. In this lesson, we will introduce a tool called Issues on GitHub that is useful for organization and communication within a project. 67.0.1 What are Issues? Issues are essentially a task system that come with GitHub repositories. They allow specification of a certain task or item to complete and have discussion threads, assigned workers, and labels. They are a convenient way to stay organized and communicate with others involved in or interested in your project. 67.0.2 Creating an Issue First navigate to the Issues tab on the main repository webpage. Navigating to the Issues tab Then click the green button that says “New issue.” Where to create a new issue On the resulting page, there are forms that allow you to type a concise description of the task and a more detailed description. This description could be used to elaborate specific steps that are needed to fulfill this task. There are also options on the right that allow you to assign GitHub users to work on this issue. On the right hand side, you can also assign labels to an issue. This can be useful if your project has many issues, and you later want to view only certain ones. Creating a new issue After you create this issue, you will be taken to the webpage for your issue which contains a discussion thread. You can use this to communicate with others about the status of this task. When the task is complete, you can close the issue to mark it as resolved. Discussion thread for an issue Over the course of the project, you can come back to the Issues tab on your repository webpage to view the status of different tasks. Main board for all issues "],["setting-up-a-project-on-github.html", "Chapter 68 Setting Up a Project on GitHub", " Chapter 68 Setting Up a Project on GitHub Having learned how to organize a data science projects in the last course and how to navigate GitHub and git commands in this course, in this lesson, we’ll put all of this together and walk through setting up an example project and pushing the content to GitHub. 68.0.1 Setting up a GitHub Repository First things first, we need to create a new repository. We’ll do this as discussed in the “Creating A Repository Lesson,” by going to GitHub, logging in to your account, and creating a new repository. Create a new repository You can also create a new repository by clicking on the plus sign in the top right corner of GitHub homepage and clicking on “New repository”. Second way of creating a new repository On the page that follows, type the repository name. Choose the name my_first_project, add a description if you wish (always recommended), make it public, and choose to initialize with a README file. At the end click on the button Create repository. Add repository information The next step is to clone this repository into RStudio Cloud but as you may remember, we first need the url to this repository we just created. On the repository page, click on Clone or download and copy the link address. Make sure the address starts with https rather than git. If it starts with git, click on Use HTTPS on the corner of the small window to have the link starting with https. Repository link 68.0.2 Creating a New Project from a GitHub Repository Ok, now that we have everything set up on the GitHub end, Go to RStudio Cloud and login with your account. When you’re redirected to your Projects page, Next to the New Project button, click on the drop down button and then click on New Project from Git Repo. Create new project from Git repo on RStudio Cloud On the popup window, paste the repository url that you just copied and click Ok. Enter repository url This will automatically clone the remote repository and create a new project on RStudio Cloud. Note that the repository is still unnamed so you may want to change the name to something else. If you click on the README file under Files, you will see that the README file contains the description you added on the GitHub website. All the files in the remote repository are cloned in RStudio Cloud Start editing the README file. Specifically, replace the content with the following lines. # This is the README file for my_first_project The folders in this project are: * _data_ - is the folder where you will put all the data you have collected or been given to analyze. * _figures_ - is where you will put plots, data pictures, and other images you have created to show data to other people. * _code_ - is where you will create code files for collecting, cleaning up, or analyzing data. * _products_ - this is the place where you will place any reports, presentations, or products you create for sharing with other people. 68.0.3 Pushing Local Changes to the Remote Repository in RStudio Cloud Now, it’s time to stage, commit, and push the changes we made to the remote repository. As we’ve seen we should follow the steps here. Type the following commands in the terminal one by one. Note that here we’re using git add . which means we are tracking changes in all new and modified files and folders in the project. git add . git commit -m &quot;changed readme file&quot; git push Once you enter git push, you will see a prompt asking you for your repository user name and then password. Enter those correctly and voila, you will see something similar to this which means that all the changes are pushed to the remote repository. Counting objects: 3, done. Delta compression using up to 16 threads. Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 338 bytes | 0 bytes/s, done. Total 3 (delta 1), reused 0 (delta 0) remote: Resolving deltas: 100% (1/1), completed with 1 local object. To https://github.com/JaneEverydayDoe/my_first_project.git e80844c..24fec16 master -&gt; master You should be able to see your changes on the repository on GitHub.com. Changes can be seen on the remote repository We have learned about the importance of organization and folder structure in data science projects. Let’s practice creating the main folders required in a data science project. As you may remember, these folders represent the four parts of any data science project: data, figures, code, and products. Let’s create them on RStudio Cloud. Data science project folders Now go ahead and further create the folder structure that we learned. An important note is that if you push the changed you created in the project to GitHub, you won’t see the folder structure that you created. Unfortunately, a folder structure won’t be pushed to GitHub unless the folders contain at least a file. So don’t worry. Once you populate this project with your data, code, figures, and writings and push them to GitHub you will see the folders your just created. data/ raw_data/ tidy_data/ code/ raw_code/ final_code/ figures/ exploratory_figures/ explanatory_figures/ products/ writing/ "],["pull-requests.html", "Chapter 69 Pull Requests", " Chapter 69 Pull Requests 69.0.1 What is a pull request As a reminder, there are two parts to the git and GitHub system. Git is focused on version control - basically keeping track of the different versions of all the files you are working on. You can track a git repo on just for yourself to make sure you always have your file history available to you. Git keeps track of your files GitHub is two things, first it is a way to back up and store all of your projects on the Internet. GitHub stores and backs up your files online But, it is also a social network for coders. You can use GitHub to contribute to other people’s projects and get feedback on your projects as well. GitHub also lets you work with others In an earlier lesson we learned about issues. Issues are comments that you can make on either one of your own repos or on someone else’s repos. Pull requests are a different kind of interaction that involve making a change directly to someone’s code. First you would fork the repo to make your own copy. Forking a repo means you can change the code Then you can send the owner of the repo a “request” to “pull” your change into their code. If they accept your pull request, then their code will be updated with your suggestion! You can send a request to have your code change pulled into theirs Pull requests are a great way to contribute to projects that are being worked on by more than one person. This may be because you are working directly with a team and you all need to edit a document. Or it may be because you see someone else’s project and want to make a suggestion. This is in fact one of the best ways to start to introduce yourself to the R community on GitHub. Find a project or a repo where you see something small you can help with - even if it is just fixing a typo or adding a comments - and send them a pull request. Pull requests are a great way to start contributing to the coding community online 69.0.2 Forking a repo Now let’s talk about how to make a pull request. The first thing that you will need to do is to create a “fork” of the repository you want to edit. This is a little different than “cloning” which you learned about in an earlier lesson. When you clone a repository you are just getting a copy of the project on RStudio Cloud. You are not creating a new version that you can work on. Cloning a project just gets you a copy on RStudio Cloud When you fork a repo, something different happens. First a new repo is created under your account on GitHub. You can then clone that copy to your computer. Now when you make edits to the files on RStudio Cloud and push them, they will be pushed to your fork. Forking a repo makes a copy on GitHub that you can push and pull from Let’s try this out. We are going to make some changes to a repo and send a pull request. We will use https://github.com/jtleek/newproject as an example project to send a pull request to. The first step is to create a fork of this repository. To do this, go to the project webpage and click on Fork in the top right hand corner of the screen. We are going to fork the newproject repo from jtleek This may ask you where to fork the repository if you are on multiple teams. Select your main account and wait while the repository is forked. Select your main account to fork to You will see now that you have your own version of the newproject repository on your GitHub profile and you will be able to see where the repository was forked from right under the repository name on the upper left. You now have a forked copy of the repo Now that you have a copy of this repository on your GitHub account, you can clone it to RStudio Cloud as we learned about in a previous lesson. You can clone it by navigating to the terminal in your RStudio Cloud account and typing the command git clone https://github.com/your_username/newproject.git Go to RStudio Cloud and find the terminal in your project, and use the git clone command to get your fork where you replace your_username with your GitHub username. This should create a folder on your RStudio Cloud account called newproject. You may have to enter your username and password. You may have to use your username and password to clone Now you have successfully “forked” and “cloned” the newproject repository. Once you have cloned, you have your forked repo on RStudio Cloud 69.0.3 Making edits The next step is to make some edits to one or more of the files. This could be as simple as fixing a typo or as complicated as adding a whole new function. For now, let’s open up the file myfile.Rmd and make an edit. For example, we could change the line of code that reads: summary(cars) to a line that loads the dplyr package and uses the glimpse function to look at the data set. library(dplyr) glimpse(cars) After making this edit you can save the file. Now we are ready to start the process of sending a pull request. Edit myfile.Rmd and save the file 69.0.4 Pushing your changes to your fork Remember that when we forked the repo from jtleek’s GitHub account, we got a new repo under your account. The first step in sending a pull request is to commit and push the edits to your fork (or copy) of the repo. To do this, navigate to the terminal and make sure that you are in the right folder. To do this you can use the Unix commands pwd and cd to make sure you are in the right place. When you type the command: pwd you should hopefully see that you are in the folder for newproject. If you are not, then you will need to use the cd command to make sure you are in that folder. Use pwd and cd to get into the newproject folder To start a pull request, you need to make sure you use git add to add any new files you might have changed or deleted. git add -A This will make sure that git is now monitoring those files you might have added. Use add to tell Git to monitor your file Next you can commit your changes with a message describing what you did. Since we are going to be sending a pull request to someone else, it is a good idea to have a very clear commit message. So for example we could use the git commit command like this. git commit -m &quot;I edited myfile.Rmd to use glimpse() to summarize the cars data&quot; Use git commit to save a version of your files on RStudio Cloud Now you will have saved the file on RStudio Cloud, but you still need to make the change to your fork of the repo. To do this we need to use the git push command. git push Use push to send the files to your forked repo You might be asked to input your username and password for GitHub to make this push. But, once it has happened you should be able to go to your forked version of the repository at https://github.com/your_username/newproject/ and see the changes in the file myfile.Rmd on the GitHub website. When you have successfully pushed the files appear on your forked repo on GitHub 69.0.5 Sending a pull request Now you have successfully forked jtleek’s newproject repo, cloned it to RStudio Cloud and made some edits. You have added, committed, and pushed those changes to your own fork. The next step is to send a pull request to the user who created the original repo. To do this you should go to the website with your fork. Now you can click on the “Pull requests” tab for your repo. Click on the pull requests tab for your forked repo Then click on “New Pull Request”. If there have been no changes made to the original repo from jtleek other than yours you should see that your pull request is “able to be merged” in green. You will also see a “diff” which is all of the files and changes to those files that you have made. You can then click on “Create Pull Request” to send the change to jtleek for him to review. Check to see if the pull request is able to merge and click Create Pull request You will be shown a screen where you can describe your pull request with a title and with a description of what you did. It is a good idea to put a pretty complete description of what you did - this will be the only description that jtleek can use to decide whether to merge the changes to his repo or not. Describe your changes for the original owner of the repo so they know what you did. When you have finished typing your message click “Create Pull Request” to send the pull request to jtleek. Click “Create Pull Request” to send the changes to jtleek. Now that you have created the pull request you can go to the original repo https://github.com/jtleek/newproject and click on the pull requests tab. You will be able to find your pull request among those that are open. In this case, since this is a repo that is used as an example for a MOOC there may be many open pull requests. On the original repo your pull request appears The author of the original repo can then either accept your change, or they can ask you questions about your pull request just below the place where they can decide whether to merge the change. You can communicate with the author using the Write box. Once they click on “Merge pull request” your changes will automatically be incorporated into their code. Congratulations you have helped someone fix their code and made a new friend on GitHub! Once the original author logs in they can merge your pull request if they want. 69.0.6 When it gets more complicated This is the easiest case of a pull request. The author of the original repo hasn’t made any changes to the files that conflict with the changes that you made. In general, it is much nicer for the owner of the repo to accept your pull request if they don’t have to fix any conflicts. So when you decide to make a change to someone’s code, it is a good idea to make sure you make a fresh fork with all the latest changes in their code. Then clone, edit, push, and send your pull request in a timely way. This should ensure that there are no conflicts when you send the pull request to the original author. Sometimes it is more difficult than that since you may be editing a repo that is in active development or is being edited by multiple people. In this case, you will need to keep your forked version of the repo up to date with the original version. GitHub has instructions for keeping your fork up to date and Jenny Bryan’s book Happy Git and GitHub for the useR also has a section on forking that may be useful when encountering these more complicated scenarios. GitHub has instructions for syncing a fork. 69.0.7 Summary This lesson covered how to collaborate on someone else’s code. In the process of discussing what a pull request is and walking through the steps of submitting a pull request, this lesson has reviewed a number of git commands including git pull, git push, and git commit. And, by submitting good pull requests in a timely fashion that help authors fix bugs in their code or typos in their documentation, you’ll be contributing to the R community! "],["version-control-help.html", "Chapter 70 Version Control Help", " Chapter 70 Version Control Help We’ve previously discussed how to get help generally (Google!) and in the future we’ll cover how to get help when you’re stuck coding in R; however, at this point it’s important to briefly discuss getting help when working with a version control system like GitHub. In the lessons in this course we’ve covered a number of important git commands, such as clone, status, pull, push, add, commit, etc. As you’re learning these commands and getting more comfortable with how to use them, you will certainly make mistakes. There are ways using git commands to revert back to previous versions (previous commits) of your GitHub repo and to fix merge conflicts (which often arise when multiple people are editing the same file at the same time). However, the ways in which you will make mistakes using git commands is not necessarily predictable. Because of this, we aren’t going to walk you through any “this is how you fix that common error” tutorials. Instead, we’re going to walk you through how to find the answer to the mistake you made yourself. While there are many ways to make a mistake on GitHub, take solace in the fact that you probably aren’t the first person to have made this mistake. This means that the answer to your error is likely to be found on the Internet! 70.0.1 Google for git help When you’re getting merge conflicts or errors when trying to run git commands in the Terminal, Googling the specific error message you’re getting can be very helpful. For example, if you’re getting a merge conflict error on GitHub, you may want to Google “git merge conflict help” to start you on your path to git merge conflict recovery. Google: “git merge conflict help” Or, if you’ve changed some documents, realized you didn’t actually want to make those changes, and want to revert back to a previous commit, you may google “git revert to previous commit” Google: “git revert to previous commit” In both of these cases, links that will be helpful in figuring out your issue are provided and what git commands you’ll want to try to recover from your mistake. By clicking on the links and following the instructions provided, you’ll often be able to solve your problem. However, some links will be more helpful than others. Help from places like Stack Overflow (www.stackoverflow.com) and help.github.com are good places to start. Additionally, as you practice and get more comfortable with git and GitHub you’ll make fewer mistakes and understand the process even better! 70.0.2 Burn it Down That all said, while sometimes Googling can be incredibly helpful, it’s important to remember that git and GitHub are tackling incredibly difficult problems (version control and tracking changes is no simple task!). This means that while trying to track all your changes and use git commands, you may run into trouble that Googling won’t help you solve. For times when you’ve made a lot of changes and are struggling to even track down what it is you’ve done and what you want the GitHub repository to look like when it’s all better. For these cases, it’s best to just do what is referred to as “burning it all down,” the process of which is described in a chapter of Jenny Bryan’s Happy Git with R and captured perfectly in this xkcd cartoon. Briefly here, for those worst-case scenarios when your GitHub repository and your local copy just cannot get to a happy place where you can push and pull with ease, you’ll likely just want to: move the files and directories from your local copy to a safe space that is separate from the directory you’ve cloned from GitHub. It’s best at this point to rename the directory. Once your files are in a safe (and different) directory, you’ll want to clone the repo from GitHub again (which takes the last happy copy from GitHub and makes it available to you locally). You are now back to a safe state where you can easily push and pull. You’re then ready to copy all the files from the safe directory back to this newly-cloned and happy repository. After the files have been returned, you’ll want to stage (git add), commit, and push your files back to GitHub. While this is not a particularly elegant solution, it works in your most desperate times of need! Practice these steps now so that you know how to use them when you actually need them. And trust me, if you’re new to git and GitHub, you’ll probably need them. 70.0.3 Summary This lesson is intentionally short. There are lots of ways that git and GitHub will seem frustrating as you learn to use them. Remember that this is because git and GitHub are tackling difficult problems. When you run into issues with git and GitHub, the best thing is to look to the Internet for help. Read the answers online, think about whether they’ll work for the issue you’re having, and then give them a try. As you get more comfortable working with git for version control, you’ll run into fewer and fewer of these problems. However, in the meantime, use the Internet for help, and when all else fails, just burn it all down. 70.0.4 Additional Resources Happy Git and GitHub for the useR, by Jenny Bryan GitHub Help "],["types-of-communication.html", "Chapter 71 Types of Communication", " Chapter 71 Types of Communication At this point in the Course Set, you’ve mastered Google Products and version control with GitHub. You’ve become comfortable working with data in R, understand the importance of tidy data, and know how to visualize your data to better understand it. These are all incredibly important skills for a data scientist; however, having these skills is simply not enough. Another crucial skill set for a data scientist to master is that of communication. The most interesting results in the world are only the most interesting results in the world if the world knows about them. To ensure that your interesting results move effectively from your brain and computer to the brains of the rest of the world, we’re dedicating an entire course to data science communication. In this course we’ll discuss the various ways in which data scientists regularly communicate and provide details on how to master each of these types of communication. We’ll start in this lesson by providing an overview of the many types of communication. The rest of the lessons will then describe in detail the considerations to make when executing each of these types of communication. 71.0.1 Opinionated Communication Before we dive into the various types of data science communication, let’s take just a second and talk about opinionated software development. When people talk about software being opinionated, it means that the piece of software forces you to do things their way. For example, if you’re familiar with the company Apple, they develop opinionated products. For example, on an iPhone, the general appearance from one iPhone to the next is very similar and there is often only one way to accomplish a task (for example, to open an app, you click on the app on the touch screen). This simplifies things for the user, since there’s generally only one way to accomplish a task. It also allows one iPhone user to quickly figure out how to work someone else’s iPhone. Alternatively, un-opinionated software is a lot more flexible. Un-opinionated software allows developers to accomplish goals in many different ways, allowing different styles and approaches to be implemented. Let’s use an arrow as our example. Say I was teaching you to draw an arrow. If I only allowed you to draw one type of arrow, that would be an opinionated approach. If, however, you were able to draw lots of different arrows, that would be an un-opinionated approach. While the second approach is more flexible and provides more options, simply knowing how to draw one arrow is enough to get you started! For the purposes of the lessons in this course, we’re going to be very opinionated. By using this approach, we can provide you with the necessary information for effective data science communication without providing tons of caveats or confusing alternative approaches. We’re going to provide you with tips and details that have worked for us over the years as data scientists. However, this does not mean this is the only way to do things! We are confident that the information we’re providing is a good place to start; however, in practice, your style and approach to communication will evolve over time to best suit you, and that’s great! We recommend giving our approach a try at first. Then, as you gain experience, you can allow your own style to develop over time into whatever works best for you. Additionally, we’ll do our best to provide links in each lesson to others’ thoughts on the topics we’re covering. This way, if you have time (either now or in the future), you’ll be able to see how others approach data science communication! 71.0.2 Types of Communication Effective data science communication should always communicate your message clearly to your intended audience. This can be accomplished through a number of different types of communication. In this lesson we’ll be discussing the following general types of data science communication: Reports Presentations Blog Posts Meetings 71.0.3 Reports Data science reports are a written form of communication used to summarize a data science project through text and figures. Written reports should always tell a story. What we mean here is that a finished report should have a beginning, middle, and an end. It should start with the question you’re answering and relevant background information (the beginning), continue with what you did for the project, which will often include both plots and text (the middle), and finish with a summary of your results (the end). Reports are most often sent to your teammates or manager to communicate what you’ve been working on. 71.0.4 Presentations Presentations also tell a story; however, they do so in a different format and for a different audience. Presentations are generally comprised of slides with images and text on them that you present orally; however, they can include brief videos or GIFs as well. The audience for a presentation is generally larger than those who read your report. Presentations can be made to groups of individuals at your company or to a group of attendees at a conference. 71.0.5 Blog Posts Blog posts are an effective way to reach a wide audience. Like reports, these also tell a story through text and figures; however, rather than update co-workers on your progress for a project at work blogposts are often on topics that are more helpful to a general audience. Data science blog posts are generally either “how to” posts that teach others how to do something related to data science or are “analysis” posts that summarize a cool analysis. Writing blog posts on your website or personal blog can be a great way to share you work with others in the community you don’t typically work with directly. 71.0.6 Meetings Data scientists often find them in a number of meetings. Meetings can be with team members where you discuss the plan for a project. Or, they can be with someone who’s less familiar with data analysis but wants your help. Further, data scientists can end up in larger meetings with groups of people in a company where they as the data scientist are the only person familiar with the data or analysis in the room and are expected to communicate results to everyone. As a participant in all of these cases, it’s the job of the data scientist to communicate effectively with others in the room. In lessons in this course we will discuss how to communicate effectively in meetings as well as discuss how to both run a meeting and how to participate in someone else’s meeting. "],["data-science-reports.html", "Chapter 72 Data Science Reports", " Chapter 72 Data Science Reports The goal of a written report is to effectively communicate your full analysis to your readers. This means that your writing should be clear, your figures should be helpful, and your audience should be able to understand what you did to reach the conclusions your presenting. In the Course Set thus far we’ve discussed Markdown and R Markdown documents. We’ll continue to use this format throughout this lesson. We won’t discuss the basics of R Markdown, however, as that has been covered in an earlier lesson. Specifically, in this lesson we’ll discuss what to include in your data science reports, using the R Markdown format to guide us. 72.0.1 What To Include Every data science report should include the following elements: A Title An Introduction How you analyzed the data Your results Any conclusions All references 72.0.1.1 The Title The title of your report should be as informative and as short as possible. The title should let readers know what question is being answered in the report and the answer to that question. But, details should be left to the report itself. For example, what if you had completed a project analyzing the NHANES dataset? Which of the following titles would be better? “Analyzing NHANES” “Data from the NHANES study shows that diet is related to overall health” Well, “Analyzing NHANES” is shorter, but it’s not informative at all. Rather “Data from the NHANES study shows that diet is related to overall health” is an informative but concise title that lets readers know that the data science question being asked as to do with diet and health as well as letting readers know that there is a relationship between diet and overall health. How about another example – which title is best out of the following?: Public Data and Prediction Improving the Value of Public Genomic Data with Phenotype Prediction Using Public Genomic Data Phenotype Prediction is Helpful Sample Genomic Data that Are Available Publicly on the Internet Can Be Used For Prediction of Critical Phenotype Information Let’s first talk about which one is the best title. Then we can discuss the issues with the other possible titles. “Improving the Value of Public Genomic Data with Phenotype Prediction” is the best title here. Even if you don’t know what genomic data are or what a phenotype is, you know that publicly-available data are going to be used for some type of prediction. And, you know that in this presentation you’ll learn about what genomic data and phenotypes are. As for the other titles, “Public Data and Prediction”, “Using Public Genomic Data”, and “Phenotype Prediction is Helpful” are short but not fully informative. They don’t tell you what the question and the answer of what will be included in the report are. On the other end of the spectrum, “Sample Genomic Data that Are Available Publicly on the Internet Can Be Used For Prediction of Critical Phenotype Information” tells you the question and the answer but it does so in a very long and windy way. This title is too long and confusing and should be edited before being used in a report. In your your R Markdown document where you’re writing your report, you’ll want to include a helpful title. The title of your report can always be edited in the YAML of your R Markdown document. Edit title within the YAML of your R Markdown document 72.0.1.2 The Introduction After your title, your report should include a brief introduction to your report. This section should include the motivation for your project. Any necessary background information explaining the why you’re writing this report would be included here. Additionally, let the readers of your report know explicitly what question you’re answering with this analysis. Be as clear and concise as possible Finally, this section should include a description of the dataset you’re using. How many people are included in your dataset? What variables are included? A description of your dataset is a critical (but often overlooked) portion of a good introduction In your R Markdown document, an H1 header (#) can be used to demarcate this new section. Then, Markdown format can be used to include all the necessary information in your Introduction. A new section with an H1 header should be included for your Introduction 72.0.1.3 The Analysis After introducing the motivation for your project and the data you’ll be using, you’ll want to describe your approach. This section should explain what methods you’ll be using to analyze your data (including references to any methods used - details below on how to do that!). If the introduction section answered why you’re writing this report, this section should answer how? and what?. How did you analyze these data? What methods did you use? In your R Markdown Document, this can be separated using a new H1 header. Throughout your report, additional subheadings can be demarcated using H2 (##) headers. Multiple Parts of an analysis can be separated using H2 headers Code to complete your analysis would be included here, if appropriate. Not everyone wants to look at the code you ran. For example, your manager may just want a summary of your analysis. In this case you wouldn’t include all your code. However, a teammate may want to see all the details. As above, different parts of your analysis should be separated by different headers and separate code chunks for each part should be included. Text briefly explaining each what is being done in each code chunk should be included before each code chunk. Results from the code chunk can be explained with text after the code chunk and before the next section. Code chunks can be included in your report 72.0.1.4 The Results After describing how you’ve analyzed the data, it’s important to describe the results from your analysis and explain them. Simply putting tables and figures in this section is not enough. Explanations to guide the reader and help them understand the results in this section are required. A reminder that all results that for every numeric estimate reported, there should be a corresponding measurement of uncertainty included. And, for every plot, best practices should be followed. These were discussed previously in the data visualization course, but as a general reminder here: colors used on any plot should be distinct from one another colors should be chosen such that figures are interpretable by individuals who are color-blind all axes and text on figures should be large enough the appropriate plot for your data should be used explain as much on the plot itself as possible with annotations to guide the reader If you want to generate the plot in your R Markdown document (you want the code to be evaluated) but do not want the code or its output to be displayed, you would include include=FALSE in the code chunk. (Note that results =\"hide\" hides output but displays code, echo=FALSE does not show the code but does display any results or output, and eval=FALSE displays the code but doesn’t actually evaluate it (does not run the code)) code chunk arguments summary Tables of results would also be included in this section. Guidelines previously discussed for good tables should also be followed. To briefly summarize: comparisons should be made top to bottom rather than left to right The number of digits displayed should be limited Tables should be organized to help guide the viewer to see what is important Including good figures and tables and helpful explanations in the results is critical 72.0.1.5 Conclusions After discussing how you analyzed the data and proving readers with the results of your analysis, it’s important to summarize the findings and conclusions from your analysis for the reader. In this section, be sure to clearly explain the most important things you want the reader to take away from this analysis. Typically, figures and tables are not included in this section. Bullet points can be used to make your points stand out in this section. Be sure not to make this section any longer than is absolutely necessary. Conclusion section should summarize the most important points from your analysis 72.0.1.6 References In all of your work it’s important to give credit where credit is due. This is important for two reasons. Most importantly, it gives credit to others for their work and their ideas. Second, it let’s others know where to look to learn more (and reminds future you of where you found the information originally, which can save you a lot of time). 72.0.1.6.1 Hyperlinks At the very least, it’s helpful to include hyperlinks in Markdown format to others’ work within your R Markdown document. As a reminder the Markdown format for a hyperlink is: [text to display](https://url_to_reference.com). Including hyperlinks is the way to go when you’re linking to information on the Internet, such as general websites or blog posts. For example, if we mentioned in our report that “All analyses were carried out in R,” we’d want to hyper link to the R Project’s website, so that anyone unfamiliar with R could click on this link and learn more Including hyperlinks in reports helps readers learn more and gives credit 72.0.1.6.2 References However, when linking to scholarly articles, the best approach is to use an external bibliography that will automatically populate the references used in your analysis and include them in your report. We’ll walk through how to do that now! For example, consider our general report where within the Data Analysis header of our Methods portion we stat that “We’ve used ggplot2 to generate all figures in this report.” In that case, we’d want to include a reference to ggplot2! Where we want to include the reference in our report To do so we’d first go to Google Scholar at scholar.google.com. Google Scholar contains scholarly articles across many different fields. References to articles, theses, books, abstracts, and academic journals are all searchable on Google Scholar. In the search box type ‘ggplot2’ and then click the magnifying glass to search Google Scholar. Search through Google Scholar When we searched, the first link references the ggplot2 book. This is the reference we want to include in our R Markdown document. To do so, we’ll first save it to our Google Scholar library by clicking on the “star” icon. Click on the Favorites (star) icon You can then go to your library by clicking on “My library” in the top right-hand corner. Click on “My library” Any entries in your library will be listed here. Click on the references you’d like to include in your document using the checkmarks at left (here, we’ll only select the ggplot2 reference). Then click on the download icon and select “BibTeX” from the dropdown menu. Download the citation The citation(s) selected will then appear in a new window. Copy this text. Note that the first thing within the curly brackets is how we’ll refer back to this later in our R Markdown document. Copy text for BibTeX citation Then, return to RStudio Cloud and click on “File” &gt; “New File” &gt; “Text File”. Open a new text file We’ll then want to save this file. Do this by clicking on “File” &gt; “Save As” Open Save as window We’ll save the file as “references.bib.” Type that into the File name box. Then click “Save” Save file with .bib extension Once the file is saved, you’ll want to paste in the reference text you previously copied and save the changes to this file. add references to this .bib file You’ll then have to specify within your YAML (the top of your R Markdown document) what the name of your bibliography file is using the format you see here: add bibliography to YAML Note: The .bib file must be in the same directory as your .Rmd file. Now that our bibliography is set up, we’ll add the reference into our document. References use the following format [@identifier] They always include square brackets. Within the brackets we specify that it is a reference using the @ symbol, followed by the identifier specified within our .bib file. Here that identifier is wickham2016ggplot2. In-line reference To finalize this document, we’ll want to include a header at the end of our R Markdown document, since any references will be automatically added to the end of the Knit document. Add # References header to end of the R Markdown document Now we’re ready to Knit. When we Knit this document, we see our in-line citation and the reference automatically add to the end of the document! Knit document shows reference automatically formatted and added While there are a lot of steps in this process, it’s important to include references to others’ work when applicable and this process will get simpler the more you do it! 72.0.2 What to Avoid Now that we’ve discussed what to include in your data science reports, let’s briefly discuss what you should avoid. In generating data science reports, you should avoid: explaining every single analysis you tried - only include the necessary analysis and results being too wordy - keep it simple and concise. Avoid unnecessarily long sentences. Be clear in your explanations. using the wrong type of plot – If your data are looking at the relationship between two numeric variables, consider a scatterplot. If you’re comparing values between five different groups, use a barplot (not a pie chart). Be sure you’re appropriately representing your data. presenting bad figures - consider the colors, be consistent with them throughout your presentation, and make sure all text on figures is large enough using gratuitous flourishes - 3D figures may look cool to some, but they can often be hard to interpret. Use faceting to avoid unnecessary 3D figures. Make sure anything you add to your plots helps the reader understand your analysis and does not detract from it. 72.0.3 Brief Reports Above we walked through portions that should be included in a data science report. All of these components should be included in a brief report; however, for a brief report (often referred to as an executive summary), you would limit the details included in the report. For a brief report, the nitty-gritty details of the dataset you used would be omitted. Similarly, you wouldn’t include all the code you used to analyze the data. Your results would only present the most essential tables and figures. And your conclusions would only include the essential take-aways. You would still include and introduction, a description of your analysis, results, and a conclusion. In other words, your brief report should still tell a story. It just does so in a shorter document with fewer details. 72.0.3.1 Sample Brief Report To understand exactly what a brief report is, let’s return to a data set with which you’re already familiar and walk through an example of a brief report. In the data tidying lessons in this Course Set, you were introduced to a dataset that included survey data from Americans about how they prefer their steak prepared. We’ll return to that dataset to walk through an example of a brief report that sets out to answer the question: How do Americans prefer their steak to be prepared? In this example, you see the R Markdown document on the left and then, once we knit the R Markdown document, the brief report that could be shared with others on the right. Specifically we see that, the report itself is pretty short, that only the necessary details are presented, and that everything the reader needs to know is presented clearly. For the purposes of this example there weren’t many details in the means of what we did for the analysis in total; however, for most projects you will likely have a slightly longer analysis or fewer results. Brief Reports present readers with only the essential information 72.0.4 Full Report Alternatively, a full report would include the in-depth details of your analysis. This would include details about the data used, a longer explanation of your analytic approach, and the code used to generate your results. A few more figures and tables would likely be included in the results section of a full report. Again, just like a brief report, data science reports should always tell a story. The motivation behind the report, the analysis that was done, and the conclusions drawn should always be included. 72.0.4.1 Sample Full Report Returning to the steak preference data and the question “How do Americans prefer their steak to be prepared?,” we’ll now take a look at how a full report for this analysis could look. While this analysis is simpler than most analyses you’ll likely be doing, we’re using it as an example to demonstrate that a full report will likely have more explanation and more details than a brief report. There will be more description regarding how the analysis was carried out and likely more tables and figures. Here, we see the full report is similar but goes into more detail and presents more results than the brief report Full reports contain more details but still tell a story Note: The code used to generate these reports can be viewed on RStudio Cloud here. 72.0.5 Summary In this lesson we’ve discussed what should be included in all reports and what should generally be avoided. We’ve described how to generate reports in R Markdown. And we’ve discussed brief reports (also known as executive summaries) and full reports as well as the differences between the two. As a data scientist, you’ll likely have to generate reports throughout your career, and now you have a good baseline from which to start! 72.0.6 Additional Resources Chapter 9: Written analyses in Elements of Data Analytic Style, by Jeff Leek Bibliographies and Citations in R Markdown documents, from RStudio 72.0.7 Slides and Video Data Science Reports Slides "],["google-slides.html", "Chapter 73 Google Slides", " Chapter 73 Google Slides In addition to Google Docs for word documents and Google Sheets for spreadsheets, Google also has a web-based slide presentation editor called Google Slides. Google Slides allows users to format and edit slide presentations online and with Slide’s sharing capabilities, multiple people can edit the slides at the same time. As a data scientist, Google Slides can be a powerful tool to present your work to your employer or contribute to a presentation with your colleagues. If you’ve ever used Microsoft Powerpoint, you can think of Slides as an online version of Powerpoint, but with the additional capability to work simultaneously with other users on the same set of slides! It is compatible with Microsoft Powerpoint files (.pptx and .ppt). 73.0.1 Presentation Guidelines Before we talk details about how to work within Google Slides, we want to review a few best practices for slide presentation. To make a good slide show, it’s best to keep a few guidelines in mind: Minimize the number of words you put on any slide. Whenever possible, use pictures instead of words. Make sure images and text are as large as possible. Use consistent text font and colors throughout the presentation. 73.0.2 Accessing Google Slides Like Docs and Sheets, Google Slides can be accessed from within Google Drive. To create a new presentation through Drive, simply click the “New” button in the top left corner of the Google Drive home page and then select “Google Slides.” Slides on Google Drive Like Docs and Sheets, however, you can also access Google Slides directly through its own URL, slides.google.com. Remember, Google Drive contains all of the files you have stored on the cloud, while Google Slides will only contain your presentation files. Your Google Slides homepage will have the same structure as for Docs and Sheets. You can see there is a TEMPLATE GALLERY at the top of the screen and recent presentations at the bottom. To start a new presentation, you can select the “Blank” option or any of the available templates. Google Slides Page Clicking on “TEMPLATE GALLERY” will expand the template options to include “Personal,” “Work,” and “Education” templates. Expanded templates Once you click on “Blank” to create a blank presentation, you can begin to type in the dotted boxes to add text to your presentation. Adding text to Google Slides 73.0.3 Creating a Full Slideshow So far, this lesson has covered how to open a new slide show in Google Slides and how to enter text onto your first slide. However, to create a full slideshow, you’ll want to know how to: Add new slides Use different slide layouts Format text Add and edit images Change the theme and format of your slide show Rearrange slide order The remainder of this lesson will be dedicated to covering these topics. 73.0.3.1 Adding new slides To add a new slide to your slideshow, you’ll click the plus sign at the left on the toolbar at the top of your Google Slides presentation. New Slide A second slide will appear in the panel at the left. The blue highlighting around around the second slide in the panel at the left indicates that that slide is the slide being displayed on the right. New Slide highlighted in blue at left You may notice that this slide has a different layout than the first slide. The first slide had a large text box for the title. This slide, however, has a a text box for a title along the top and a large text box covering the rest of the slide. Google Slides is taking its best guess as to what type of slide you may want; however, you can always state which slide layout you want explicitly. 73.0.3.2 Changing layouts If the slide layout being displayed is not the slide layout you want, this can be easily changed by clicking on the “Layout” icon from the toolbar. Layout icon There are a number of different layouts from which you can choose, depending upon your needs. For example, you might only need a caption for a large image, so you could select the “Caption” layout choice. Click on the layout you’d like to use and the slide will change to the layout you’ve selected. You can always change the layout again if the one you’ve chosen doesn’t work well. As you gain experience making presentations, you will know which layouts work well for you! select layout menu 73.0.3.3 Formatting text As you start to add text to slides, you’ll quickly realize that the icons along the top and steps required are the same as they were in Google Docs. This is by design. Google has designed its suite of products so that once you master a skill in one of its products, you can use it in its other products. That said, you already know how to format text from the Google Docs lesson, so we won’t go over that here. Process for formatting text is the same across Google Products While the process for formatting text is the same, the one thing that differs between Google Docs and Google Slides is the fact that the positioning of the text box in Google Slides is important (and something you didn’t have to consider in Google Docs). To re-position a text box in Google slides, you’ll first click on the text box. Then, by hovering over the blue highlighted border around the text box, you will see a cross made of arrows appear. Once this arrow cross appears, you click and hold down. While still holding down, you can drag the text box to the the position on the slide where you want the box. Once the text is where you want it, you can release. The text box will now be wherever you’ve dropped the text box on the slide. 73.0.3.4 Adding and editing images To add images to a slideshow, the process is again the same as in Google Docs, and the process for re-positioning an image on the slide is the same as was just discussed. However, in Google Slides, you’ll likely be editing the size and appearance of images to a greater extent than you would need to in Google Docs. We’ll discuss how to work with images in Google Slides here. 73.0.3.4.1 Cropping an image Consider the following image in Google Slides. Notice that the blue box around the outside of the image is much larger than the actual image we’re interested in using. To crop out this extra white space, you’ll want to first click on the icons, you’ll select the crop icon. Crop icon The outline around the image will now have black bars around it. Remove whitespace from image These black bars can be dragged to the position where you’d like the image to be. Remove whitespace from image After the image is the size you want it to be, hit ‘Enter’ or click away from the image on your screen. The image has now been cropped, as indicated by the now-smaller blue outline on the image! cropped image 73.0.3.4.2 Aligning objects In addition to adding images, often you’ll want to be sure that images on your slides are aligned so that they look good when you go to present the information. Google Slides makes that pretty easy to do. To align images, you’ll drag your cursor over the objects to select all the objects you want to align. Once selected, you’ll right click on the selected objects to display a menu. You’ll then use the “Align horizontally” and “Align vertically” options to align the objects the way you want them. Aligning objects 73.0.3.5 Additional formatting options Additional formatting options can be explored by right-clicking on the image that will show a menu with additional options or by clicking on the additional image-editing icons on the toolbar. Additional formatting options 73.0.3.6 Formatting and editing themes In addition to formatting individual text boxes and images on your slides, you may want to format the design of your entire slide show. This can be accomplished, by clicking on the “Theme” icon along the toolbar Theme icon Note: If you do not see the “Theme” icon, click on a blank spot on your slide. This will make the toolbar option appear. After clicking on the “Theme” icon, a panel will appear on the right side of your screen with all the pre-loaded themes available. Themes available You can scroll through these themes. Clicking on one of these themes will apply the theme across every slide in your presentation. Themes applied across entire presentation 73.0.3.6.1 Changing background color In addition to pre-specified themes, you can also alter the background color of all your slides. To do so, you would right-click on a blank spot on your slide. On the menu that appears, you would select “Change background” Change background A menu will appear where you can select the color you want your slides to be from the drop-down menu and click “Done.” This will only change the background color of the slide on which you’re currently working. Change background color window 73.0.3.7 Rearranging slides Sometimes as you’re generating a slide show, you’ll realize you’d rather your slides be in a different order. This can be fixed easily by clicking on a slide in the left hand panel of your Google Slides window, and dragging and dropping your slide to the position in your slide show where you’d rather it be. Rearrange slides 73.0.4 Presenting Your Slideshow Once you have created all the slides you want in your slideshow, you can easily present them. On the right-hand corner of the window, click on Present and your slides will be in full-screen mode ready to be presented. View your presentation 73.0.5 Accessing, downloading, and sharing your slides All of the Google Slides you generate are saved automatically on your Google Slides account as well as to your Google Drive account. Like the other document editors you’ve learned about, Google Slides has an auto-save feature that means you don’t need to actively save your work. You can also download your presentations from Google Slides in different formats, such as Microsoft Powerpoint or Adobe PDF files. To do this, you would click on “File” in the top menu and then select “Download as.” You can then choose the format you want among the various options. Download your presentation For sharing documents you can follow the procedure we learned in the lesson on Google Drive. You can also share directly from the Slides itself by clicking “File” in the top menu and then choosing “Share” and entering email addresses of the people you want to share with or getting a shareable link. Sharing your presentation 73.0.6 More specifics on using Google Slides Here, we have covered the basics of Google Slides. If you want to learn how to further customize your templates, how to add transitions and animations, and details on working with images, check out the additional resources links below search for tutorials on YouTube.com. Google also has extensive information on getting started with Slides on their G Suite Learning Center, which can be found at the following web address: https://www.cloudskillsboost.google/course_templates/197 73.0.7 Additional Resources Google’s Slide Tutorial Adding images Transitions and animation Editing the Master Template How to Insert A YouTube Video into Google Slides (video) "],["how-to-give-a-presentation.html", "Chapter 74 How to Give a Presentation", " Chapter 74 How to Give a Presentation Data Scientists are often asked to present their work. This could be in a meeting with other co-workers or at a large conference amongst colleagues with similar interests. Regardless of where you’re presenting, there are general and important guidelines to consider when preparing the materials for your presentation. In this lesson we’ll cover what to consider when preparing a slide presentation, guidelines for creating good slides, and tips for presenting material well. This lesson is largely based off of our experiences and the tips from Zach Holman. Links to his work directly can be found at the end of this lesson. This lesson will be focused on how to give effective slide presentations, like what you could generate in Google Slides. These guidelines, however, will likely prove beneficial for any oral or visual presentation you may have to give as a data scientist. 74.0.1 Presentation Goals Generally, there are three goals of any presentation. A good presentation should inspire, educate, and entertain. Those listening to your presentation should leave the room feeling like they learned something, with the urge to try something new, and having enjoyed learning whatever it is you presented. If those three goals are accomplished, you have without a doubt delivered a stellar presentation. Presentation Goals 74.0.2 Know your Audience In order to inspire, educate, and entertain, the first thing you must consider is your audience. For presentations, people often say you must know your audience. If you have a room full of analytical people (mathematicians, accountants, scientists, etc.), you should consider presenting data and numbers. Conversely, if you are presenting to a room full of creatives (artists, photographers, writers, etc.), it’s likely best to ensure that the graphics you present are visually appealing. You should always keep your audience in mind while preparing a presentation. 74.0.3 Mind your Audience After considering your audience and designing your presentation to be most appropriate for those listening, you must always stay within your allotted time. Out of respect for the time of others in the room and for others who may also have information to present, this is essential. If you have 5 minutes to talk, stop at or before 5 minutes. If you have 20 minutes to speak, stop at or before 20 minutes. Your audience will tune out if you go over your time limit or you’ll be told to stop by someone else in the room. It’s best to avoid either of these situations and respect the time of those in your audience. Always stay within your allotted time. 74.0.4 Prepare Putting together a good presentation takes time. Be sure to leave yourself enough time to put a good story and beautiful slides together. 74.0.5 Tell a Story With those goals and your audience in mind, you’re ready to start considering how you’re going to go about presenting information to your audience. As mentioned in a previous lesson, when you’re giving a presentation, you’re telling a story. When presenting, you’re telling a story with and introduction, a middle, and an end. It’s best to outline your presentation before you start preparing slides. This includes jotting down or at the very least thinking about what you’ll convey to your audience and how you’ll make it into a story. You may consider a number of questions: What story do you want to tell? What do you want to convey in your introduction? What pieces are necessary to the story to be included in the middle? What do you want your listeners to take away from your presentation? How will you include this information in your conclusion? All of these are good questions to consider when outlining your talk. Outline your presentation Finally, as Zach Holman explained in his talk on talks, the best talks are stories and the best stories use repetition. It’s ok to repeat important information to your audience. Don’t be afraid of repeating yourself to make a point clear to your listeners. 74.0.6 Slide Design Now that you’ve outlined what you’ll talk about in your presentation, it’s time to decide how you’re going to put those ideas onto slides. There are many things to consider when designing visually-appealing slides. We’ll discuss the most important in this lesson. 74.0.6.1 Text A few guidelines when deciding the words you’ll include on slides: limit the number of ideas on a single slide limit words on slides encourage listening (limit reading) include references on the slides directly (and at the end!) Do not try to cram all your ideas onto a single slide. And, everything you say does not have to be up on the slides. You’re speaking for a reason. The slides should help guide you along in telling your story, but your audience should have to listen to what you’re saying to get the full picture of your story. 74.0.6.1.1 Fonts To convey textual information to your audience, choosing a good font is important. It’s generally safe to stick to Sans Serif font. Sans Serif fonts can be searched through at Google Fonts. And, when in doubt, Helvetica is a safe font choice. Further, use consistent fonts throughout your talk. More than one font can be used to draw viewers attention to something, but typically there should be a consistent font used throughout the presentation. Finally, as with most rules, there are exceptions, so take time deciding on what font looks best for each part of your presentation. Choose Good Fonts 74.0.6.1.2 Size Regardless of what font you choose for your text, your text should be very large. It’s often argued that you should design your slides for the back of the room. Use Large Text The same goes for images. There’s no need to leave empty space on your slides around images. Expand your images to take up as much space on the slide as possible. Expand Images 74.0.6.2 Colors Finally, with regards to slide design, colors matter. Choosing a dark background (such as dark gray) means that your text should be light (maybe white). If your background is light, your text should be dark. Contrast will make it easier on your audience to read. Choose Good Colors Additionally, use color to focus on text you want your audience to focus in on. If all your text is white, highlight a few words in a brighter color to draw your listener’s attention to what’s important. Highlighting Text The same goes for figures and plots. All images should have colors that are viewable and distinguishable by every person in your audience. Consider color blindness. Consider colors that tend not to project well (such as yellows or light pastels). Then, design your slides using colors that will work for your audience. 74.0.6.3 Alignment Visually, things should be aligned. Center things in the middle of your slide. Make sure text boxes that should line up do line up. It may seem trivial, but it’s important to line things up so viewers aren’t distracted by something like alignment and can instead focus on your talk. Prioritize alignment 74.0.7 Presenting When it comes time to give the presentation, there are a few things to keep in mind. First and foremost, speak clearly. Whether you are a loud-talker, a soft-speaker, someone who speaks slowly, or a fast-talker, the most important thing is to make sure that the words you say are clear to your audience. Second, you should be excited. If you’re not excited or passionate about what you’re talking about, why would your audience be? If you’re not excited, it may be best to reconsider your outline and re-work your story until you are. To avoid being boring during your talk, you can vary the speed and volume of your voice. You can move around so you’re not always in one place. And, you can use pauses effectively. It’s ok to not speak every second that you’re up there. You can pause and give your audience some time to look at a graph before you talk about it. Silence for short periods of time is allowed! Finally, be yourself. If you naturally like to tell jokes, tell jokes during your presentation. If stories are more your style, work a story or two in. Ultimately, if you’re not comfortable in front of your audience, check to make sure you’re being true to who you are as an individual. Presentation Tips Finally, when presenting it’s important to remember that you’ve seen the data and figures you’re presenting many times, but your audience has not. They’ll need time to get up to speed! Thus, it’s important to always introduce your axes (tell your audience what is on the x-axis and what is on the y-axis), explain the colors on your plot, and walk the audience through the figure you’re presenting. Explain Your Figures 74.0.8 Practice Most people are hesitant when it comes to speaking. They’re nervous about getting up in front of a crowd. If you find yourself being nervous to make a data science presentation, know that you’re not alone. For those who are nervous to speak publicly, nerves may never go away completely, but there are ways to limit your nerves. Practicing your presentation is one of the best ways to limit nerves during your presentation. You should present out loud to yourself. Recording yourself and listening can help too (even if it is sometimes painful)! In addition to limiting your nerves, practicing will help you figure out the best way to explain the details included in your presentation and will ensure that your presentation does not take more time than you have been allotted. The further along you are in your career, the less you will likely have to practice before presentations. However, as you get started, practicing is the best preparation for giving a talk! Practice. Practice. Practice. 74.0.8.1 Be Prepared The day of your presentation, things may go wrong. Power goes out, projectors don’t always present your material exactly the same way it looks on your computer screen. Technology doesn’t always behave as we expect it too. In these cases, you just have to roll with the punches. To avoid any big issues, it’s best to bring a copy of your talk on a USB drive, carry any adapters you may need should you have to (or choose to) present from your own computer. Don’t expect the location to have the adapter you need. And, be sure your phone is on silent. After that, just roll with anything that goes wrong. The audience will be forgiving, as they all know that technology can sometimes act unexpectedly. Prepare &amp; then roll with the punches 74.0.9 Handling Questions After your presentation, there may be time allotted for the audience to ask questions. When answering questions, always take a little bit of time to pause and think about the question before answering. In fact, it’s often helpful to repeat the question asked of you back to the audience. This ensures that the entire room has heard the question and gives you some time to think. If you need more time, remember, silence is ok! Finally, if you don’t know an answer, it’s ok to say “I don’t know.” You should never make up an answer because you’re afraid of saying “I don’t know.” Q&amp;A Tips 74.0.10 Example Presentation: Analysis Often, as a data scientist, you’ll be tasked with presenting an analysis you’ve done. To do so, you’ll want to use the same guidelines previously discussed in this lesson, but with a focus on presenting the story of your data analysis. This includes the plots, analytical approach, and findings of your project. That said, your slides should still be well-designed, you should still tell a story, and your text and colors should all be readable when projected; however, in this case, there are a few additional caveats to consider. To discuss these, we’ll use an example presentation from Julia Silge, who is a data scientist at Stack Overflow and a wonderful presenter. In 2017 at the rstudio::conf, Julia Silge presented “Text Mining the tidy Way”. The video and visuals from her presentation are available online. This talk discusses the tidytext package (which she and David Robinson developed) and example analyses using this awesome R package. We’ve discussed this package previously, but if you want a refresher on tidytext, how to work with textual data, and sentiment analysis, this is a great resource! As above, we’ll highlight a few slides from the talk and discuss considerations to make when presenting a data analysis. 74.0.10.1 Talk Outline As in Lucy’s talk, Julia Silge, in addition to an introduction and a conclusion, had three main parts to her talk: tidytext package Jane Austen example NASA example In data analysis talks, it’s often best to start with background information. Julia covers this information when discussing what the tidytext package is. Then, she delves into discussing a few examples of the types of analyses you can do with this package. 74.0.10.2 Slide Design As discussed above with Lucy’s presentation, in Julia’s slides, she limits the amount of information she puts on any one slide, thus allowing the audience to listen to the words she’s saying. And, she has text that’s large enough and legible when projected. However, what’s different about this presentation is how Julia presents her analyses. When presenting an analysis, it’s generally best to first explain the approach and then present the results. For example, Julia uses TF-IDF to analyze text in Jane Austen’s novels. She first explains what TF-IDF is. She includes the bare minimum amount of information on her slide, so that her audience is only seeing what’s necessary. Then, she verbally describes what TF-IDF is. TF-IDF slide On subsequent slides she includes the code she used to run the analysis, discussing what is going on with the code on each slide and describing the output. code to run TF-IDF Finally, for each part of her analysis, she includes a visualization of the results. These figures have text large enough to be seen by all in the audience, are well-labeled, and include an informative title. figures from analysis Note that Julia does not go into every single detail of every analysis, nor just she talk about all the things she tried in a single presentation. She includes enough information so that her audience learns and can go try this type of analysis on their own, if they’re interested! 74.0.10.3 Presentation Watching the 18 minute video will give you the best sense of how Julia presented the information in her tidytext presentation; however, we’ll highlight a few things here. video for Julia’s presentation Julia spoke at a reasonable pace and clearly, keeping her audience interested. Additionally, she presented true to her personality. Julia and Lucy are two different people and that is clear in their presentation styles. Differences are wonderful and help keep presentations interesting! It would be boring if everyone were the same. Additionally, Julia knew her audience. She knew that there would be many programmers in her audience, and that most of them would be familiar with the R programming language. Thus, she didn’t need to explain what the R syntax was. She could put R code on her slides, and her audience would successfully follow along. But, she knew that not everyone has worked with the tidytext package. Thus, she focused her time here, discussing the purpose of and basics on how to use the tidytext package and provided clear examples of analyses using this package. When outlining and preparing presentations, always consider your audience. If your audience is more technical, include more details about how you did the analysis. If they’re less technical, be sure to explain what you did at an appropriate level. And, of course, always make sure your figures are clear, regardless of your audience. Additionally, we’ll note that at around 17:45 in the presentation, Julia went to show an image but it didn’t immediately show up how she had intended. This is a great example of rolling with the punches. She didn’t panic. She simply acknowledged that it didn’t work the first time and tried again. The audience, of course, didn’t mind, but when you’re presenting, these small occurrences can feel like a big deal. When it happens to you in the future, know that things don’t always work as anticipated and that this happens to everyone. If this happens to you, take a play out of Julia’s book and simply acknowledge what happened, try again, and continue on! Julia keeps her cool 74.0.11 Summary In this lesson we’ve discussed the important pieces to designing and presenting your data analysis project. Always be mindful of your audience, create good slides, and present your ideas clearly. Practicing your presentations will help, especially when you’re first giving talks. 74.0.12 Additional Resources 74.0.12.1 Presentation Guidelines The Talk on Talks, by Zach Holman speaking.io, Thoughts on Public Speaking from Zach Holman VIDEO: Your (first) talk, by Steph Locke How Typography Determines Readability: Serif vs. Sans Serif, and How To Combine Fonts, by Harshita Arora Google Fonts 74.0.12.2 Presentation Examples Text Mining the tidy Way video visual, from Julia Silge "],["projecting-from-a-chromebook.html", "Chapter 75 Projecting from a Chromebook", " Chapter 75 Projecting from a Chromebook As a data scientist you will often be asked to give presentations on your findings to your employers, to your colleagues, or at workshops or conferences. So you will need to learn to connect your Chromebook to a projector. 75.0.1 Finding the right dongle You won’t be able to plug your Chromebook in directly to a projector. You will need an adapter - often informally called a “dongle” to connect from your computer to the input cord of the projector. To figure out what kind of dongle you need, you need to know both the type of projector input you will be using and what type of ports you have on your computer to plug your dongle into. The two main types of inputs for projectors are VGA and HDMI. You can figure it out by looking at the cable coming from the projector. Find out what kind of projector cable you will be using You will then need to figure out what ports you have available on your computer. For example, the type of Chromebook I’m writing this on is a Chromebook Plus from Samsung. My computer has a USB-C port for connecting external devices like printers. My Chromebook has a USB-C connector, but yours might be different. To find out what kind of ports you have on your Chromebook you can go to the website https://www.google.com/chromebook . Then click on “Find Yours”. Go to https://www.google.com/chromebook and click Find Yours You will need to search or scroll down to find the version of Chromebook that you have. Once you have found your style of Chromebook click on it. Find your style of Chromebook and click on it. This should bring you to a screen with different options for purchasing a Chromebook. Any of those websites will have information about your Chromebook, but we have found that BestBuy often has the most information about ports, so click on the BestBuy logo on the left hand side to see more information. Click on the BestBuy logo to see more information. On the BestBuy page you can scroll down until you see the section called “Features”. Look for the word “Port” to find the type of port you have. In this case it is a USB-C port. Find the type of port you have under Features. Once you know the type of port you have on your Chromebook and the type of connection the projector cable has you can get the right dongle. Sometimes the place you will be presenting has a dongle available, but it is better to have two dongles. For each dongle you want one end to plug into the appropriate port on your computer (in my case USB-C). One of the dongles should have a VGA connection on the other end and the other dongle should have an HDMI connection. Then you will be prepared to connect your Chromebook to the vast majority of projectors you will use. Get a projector with the port for your computer and a VGA and HDMI connection on the other end. 75.0.2 Projecting from your Chromebook If you have the appropriate dongle you are now ready to project from your Chromebook. First, connect the dongle to the projector cord. Then connect the other end of the dongle to the port on your computer. Connect the dongle to the projector cord and to the port on your Chromebook. When you do this you will see that a window appears that says “Extending screen to” and then the name of the display. You should click on this box to open the display settings. Click on the extend displays box to open your Displays settings. There are two ways that the projector and your laptop can be connected. Extended display means that the projected screen is like a second Desktop connected to the Desktop on your computer. So you can drag files from the screen on your laptop to the projected screen. Mirrored display means that exactly what is on your Desktop is displayed on the projector. Usually when you first connect a Chromebook to a projector it will extend the display. This means that if you open the Displays window by clicking on the extending displays box, it may appear on the projected screen but not on your Desktop! This is because you have extended your screen so you actually have two screens you are working with. Extended display gives you two screens so what appears on your Desktop may differ from what appears on the projector screen. Usually when presenting you want the same thing to appear on your Desktop screen and the projector screen. You can do this by changing the setting from “Extending Display” to “Mirrored Display”. First you need to get the Displays Window to your Desktop. You can move your cursor all the way off the left or right hand side of the screen while watching the projector screen. You will see the cursor move up to the projector screen and you can click on the Displays window, hold down, and drag the display back to your Desktop. Drag the displays window to your Desktop and click mirror displays. Then if you click mirror displays you will see that your Desktop and the projector show the same thing. Click the Mirror Displays box to mirror the displays. You are now corrected and ready to present! Mirrored displays show the same thing on your laptop as on the projector. Once displays are mirrored you see the same thing on the laptop and the remote version. In a future class we will go over the software for making and delivering data science presentations. "],["how-to-present-to-a-general-audience.html", "Chapter 76 How to Present to a General Audience", " Chapter 76 How to Present to a General Audience In the last lesson we discussed general guidelines for presenting data science projects and discussed the fact that considering who is in your audience is essential. In this lesson we’ll break that down a little further to discuss how to approach presenting a data science project to a general audience and what considerations to make when preparing such a presentation. (In the next lesson we’ll dive into presenting to a technical audience.) 76.0.1 The Audience When considering presenting to a general audience, this would be an audience that doesn’t know a lot about data science or quantitative analysis. This would be an audience that generally does not code or analyze data on a regular basis. This could be individuals at your company who are more focused on day-to-day operations or an audience of high schoolers getting their first introduction to data science. 76.0.2 The Goal Regardless of exactly to whom you’re presenting, the goal of this type of presentation is to teach at a high level what you did. Your presentation should still inspire, educate, and entertain, but it should leave out some of the nitty gritty details and keep in more background and general explanation of your approach. 76.0.3 The Presentation When it comes to presenting this information, the slide design guidelines discussed in the last lesson are still pertinent; however, there are considerations to be made when deciding what to talk about, how much detail to include, and what to emphasize. 76.0.3.1 Emphasize When presenting to a general audience you want to emphasize a number of different things. Your presentation should certainly still tell a story, but you may want to spend more time on background and introduction than you would to a more technical audience. In your introduction you want to be sure to include: why you did the analysis why your audience should care what specific question you set out to answer A large focus of your presentation will likely be in this section of your talk. This will provide your audience with the necessary information to understand the rest of your presentation. It’s ok to spend more time on the beginning of your talk when addressing a general audience. In the body (middle) of your presentation, you should always: describe the data explain what you did provide results explain limitations or caveats With regards to a general audience, the details of this part of the presentation should not be the focus. You never want to lose your audience because you’ve left out important explanations or assumed they know more than they do. So, be sure to be clear in your explanations for this part and refrain from including every single detail of what you did. At the end of your presentation, it can be helpful for a general audience to: include a conclusion slide tell them exactly what they should take away from your presentation 76.0.3.2 De-emphasize In a presentation to a general audience, you should de-emphasize: details of your approach issues you ran into in your analysis IF we were to summarize this graphically, where the size of each box is relative to how much you should emphasize it in your presentation, the introduction and background will likely be the bulk of your talk, whereas the details you include about your analysis may be less. Your conclusion will be succinct and wrap everything up for your audience. Summary of presentation parts Remember, your audience will be hearing a lot of this information for the first time. There is only so much any individual can learn at once! Because of this, be sure to focus on the introduction, including the question you asked and the necessary background information. While you will certainly provide your audience with what you did and what you found, you will keep the details here to a minimum. Avoid including details about things that confused you along the way. Then, in your conclusion, remind your audience of your conclusions and provide them with the most important message they should take away from your presentation. 76.0.4 General Presentation Example To discuss giving presentations to a general audience using a specific example, we’re going to walk through a presentation that was given to a general audience. The audience for this presentation was college students with a year of calculus but no particular background in statistics. This means that any statistical concepts would have to be explained, but it could be assumed that the audience was fairly analytical. The slides for this talk can be viewed here. The title of this talk was “Upcycling genomics data: From publicly-available ‘junk’ to priceless ‘treasure’.” The outline of this presentation was as follows: Introduction Part 1: recount Part 2: phenopredict Part 3: application Conclusion Click on this link and scroll through the slides to get an idea of what the presentation looked like (Additionally, a quiz question for this lesson will require you to open this link, so it’s best to just take a look now!). The content of the talk is not what we’ll be focusing on, so it doesn’t actually matter if you look at these slides and understand what they’re talking about. Rather, in this lesson, we’re going to walk through how this presentation was organized and designed to focus on a general audience. In the next lesson, we’ll walk through the slides for a talk on the exact same topic, that that was designed and presented to a technical audience. To walk through how this talk was organized, we’re going to use the following figure: Talk breakdown Each slide in the presentation was assigned to one of 7 categories. Each category is a different color on the figure: Question - the motivating question used to tie the story together Outline - to remind the audience what we’ve already discussed and what we’re about to discuss Background - Information the audience is required to know in order to understand the project/analyses Approach - How the data were analyzed Teaching - A quick lesson about a specific topic to ensure the results make sense to the audience Conclusion - The end of the slides, wrapping up the presentation Categories The bar in the middle shows how the categories break down throughout the presentation. At the left we have the title slide. All the way at the right we have the conclusion slides. In between, the width of each section is proportional to the number of slides included in the presentation. The wider the box, the more slides. The thinner the box (or line), the fewer slides in the presentation were dedicated to that category at that point in the talk. How to interpret the graphic The last bit about how to read this image is that above the colored boxes, you can see the percentage of the slides dedicated to each part of the talk. For example, most of the slides (51%) were used in Part 2 of the talk. This makes sense, as the focus of the talk was on “phenopredict.” Percentage of slides for each part of the talk Now that we know how to read this figure, let’s discuss a bit of what we see. First of all, there is a heavy focus in the first part of the talk on discussing necessary background information. As this is a general audience, that makes sense! The first part of the talk is background-heavy In the middle of the talk, a lot of time is dedicated to teaching the audience the necessary concepts and information and explaining (at a high level) what approaches were used in the analyses. There is a focus on teaching and approach in the middle Note that while results are presented, it’s not in great detail. The audience is taught what they need to know before results are presented. This ensures that nobody gets lost. Results are presented, but nitty-gritty details are omitted Additionally, in this talk, a motivating question was presented to the audience to fully explain why this work was necessary. By motivating the presentation with this question, coming back to it throughout the presentation, and then using the data and analysis presented in this talk to answer the motivating question at the end, the presentation becomes a story. A motivating question can help tie your presentation into a full story Lastly, an outline was used throughout the presentation to remind the audience what has been discussed and what was yet to come. Outlines can help the audience When all was said and done, if we were to calculate the percent of slides dedicated to each of the categories, we would see that the category with the largest percent of time spent is background information, with 30% of the total slides being spend on it. Categorical breakdown However, if we consider background, teaching, and explaining the approach, we see that about 65% of the time was spent providing information to the audience that had nothing to do with the specific new information being presented about this analysis. Considering background, teaching, and approach together This makes sense, because, as you’ll remember, the goal of a general audience presentation is to teach your audience what you did. It’s not about the nitty gritty details. Instead, you want to present information at a high level. Here, we’ve presented an example of one presentation. This is to describe how presenting to a general audience can be done. This does not mean that every time you present to a general audience, exactly 30% of your time should be spent on background. Rather, this is to give you an idea of how to approach a general audience presentation. 76.0.5 Summary In this lesson we discussed the approach to and specifics of presenting to a general audience. We walked through an example of a presentation used to present to a general audience and broke down what parts of that example presentation were spent on what aspect of presentation. We highlighted the need to present background information and the need to teach the audience necessary concepts (at a high level!) so that the results you present will be understood. "],["how-to-present-to-a-technical-audience.html", "Chapter 77 How to Present to a Technical Audience", " Chapter 77 How to Present to a Technical Audience In the last lesson we covered what to consider when presenting to a general audience and walked through an example of an actual general-audience presentation. Here, we’ll approach presenting to a technical audience using a similar approach! 77.0.1 The Audience When presenting to a technical audience, you’re presenting to an audience of your peers. These are individuals who regularly analyze data and/or are familiar with your work and the way you approach data analysis. This could be other members of your team or other individuals at a conference in your specific field of expertise. As a result, you can often limit the amount of information you provide as introductory material in this type of presentation and really focus on the details of your analysis. 77.0.2 The Goal Unlike a general audience presentation where the goal is to teach at a high level what you did, the goal of this type of presentation is to really explain details of what you did. The details and caveats of your approach should be the focus here. 77.0.3 The Presentation For a technical audience presentation, you’re still telling a story, but the organization of your talk and the material you emphasize will be different. It should still be cohesive with a beginning, middle, and end, but you’ll really focus a lot of attention on the middle, where you explain the details of your analysis. 77.0.4 Emphasize While you will certainly still provide your audience with the question you’re asking and important background information, the emphasis of your technical talk will be on the middle portion, where you’ll describe in detail what you did and what your findings are. Thus, you’ll want to provide your audience with: a detailed summary of the data (descriptive + exploratory analysis) what you did (with concrete details) what you found (with multiple figures discussing results) discussion of the analysis conclusion In a presentation to a technical audience, you’ll want to be sure the audience knows exactly what data you’re working with. In a general audience presentation, you will likely inform the audience of how many observations were included in your analysis; however, here, you’ll likely discuss that in addition to presenting some exploratory plots that provide your audience insight into the data included in your dataset. In addition to providing your audience with details about the data you used, you’ll want to provide details about what you did. This includes what software and packages you used (including what versions were used!) as well as details of how you approached and carried out your analysis. Finally, while in a general audience presentation you’ll want to limit the results you present to one or two figures, here it is ok to include more results, as long as they’re important to your story. In addition to presenting detailed results, it’s also appropriate to discuss where you ran into trouble during this analysis, what you thought about the tools you chose to use, and a discussion about the tools themselves. A technical audience will likely consider whether or not your approach will be helpful in their own work, so it’s helpful to include this type of discussion. Finally, you’ll again want to wrap up your presentation with a conclusion slide. In a technical talk, this can include conclusions about both your analytical approach and the results from your analysis. 77.0.5 De-emphasize In a technical talk, it’s ok to de-emphasize discussing background information that is shared by everyone in the room. For example, if you’re meeting with other team members who are all working on a similar project, you can likely just refer to the project you’ll be discussing to get everyone on the same page and can leave out the rest of the background information about the project, since everyone in the room is already familiar with that information. All that said, while it’s still certainly important to provide your audience with some Introduction and Background information, in a presentation to a technical audience, the bulk of your time and focus should be spent discussing the details of the analysis and its results. Technical Talk Breakdown 77.0.6 Technical Presentation Example Like in the last lesson, we’re going to walk through a technical talk. The title of the technical talk we’ll be discussing is “Improving the value of public data with recount2 and phenotype prediction” and the slides are viewable here. Go to these slides now and scroll through them. Note the mathematical notation on slides 70 through 87 – this level of detail was not included for the general audience (slides 96-110 [here for the general audience comparison(bit.ly/general_audience)]. There will again be a quiz question about these slides, so it’s best to open it up and take a peak now. While the title and presentation details differ from the general audience presentation, the outline of the talk remains the same: Introduction Part 1: recount Part 2: phenopredict Part 3: application Conclusion We’ll break down this talk in the same way that we did the general audience talk in the last lesson. Visual representation of technical presentation As a reminder, each slide was assigned one of 7 categories: question, outline, background, approach, results, teaching, or conclusion. Each category was assigned a different color. (The colors are the same as in the previous lesson). All the way to the left represents the first slide in the presentation. All the way to the right represents the last slide. You’ll first note that this presentation (like the general audience presentation) is motivated by a question, helping tie the story together. The presentation again begins with background. Presentation still begins with a motivating question and background information However, what you’ll notice is that this technical presentation is results and approach heavy. A fair amount of the presentation is dedicated to results. Much of the presentation is results or approach Further, unlike the general presentation slides, the approach here includes a greater amount of technical detail. Thus, while the analytical approach is discussed in a general presentation, it’s done at a high level. Here, in a technical presentation, equations and specific details of the analysis are included. To summarize this presentation, we can see that the bulk of the presentation is spent on background, results, and approach. There is not much teaching in this technical presentation. Bar plot summary of technical talk 77.0.7 General vs Technical In the last lesson in this course we visualized a general talk and here we’ve done the same for a technical talk. Each presentation was on the same topic. Thus, it would be beneficial to compare the two directly here. Here we see the general presentation visualized at the top and technical beneath it. The focus on teaching and approach in a general talk and the focus on approach and results in a technical talk becomes evident when compared on the same image. General vs. Technical To summarize this graphically, using the same bar plots, here we have the same categories we’ve looked at previously, but we see the results for each type of presentation. General talk is in green. Technical talk is in orange. When we focus on the results and teaching bars, we can see this general talk did a lot more teaching, while the technical talk focused more time presenting results. Technical focused on results; General did more teaching As discussed previously, both have a similar amount of background and approach; however, the approach slides for a technical talk are much more specific than they were for the general talk. Background and approach similar in both; more detail in technical Finally, we’ve also broken down the presentation by percent of slides dedicated to each part of the presentation. We can see that there aren’t huge differences between the two types of presentations. In both cases, the bulk of the time was dedicated to discussing part two, as this was the focus of the presentation. Thus, while the way the information was presented and the level of detail differed between the two presentations, the breakdown of the topics discussed in each presentation were pretty similar. Time spent on each part of the presentation was similar 77.0.8 Summary In this lesson we discussed the approach to and specifics of presenting to a technical audience. We walked through an example of a presentation used to present to a technical audience and broke down what parts of that example presentation were spent on what aspect of presentation. We highlighted the need to present the details of your analysis and to discuss your thoughts about the analysis and tools used in your presentation. Additionally, we’ve compared the general and technical presentations visually to really drive home the shift in focus from a high-level general audience presentation to a detailed, include the nitty-gritty technical audience presentation. "],["how-to-write-a-blog-post.html", "Chapter 78 How to Write a Blog Post", " Chapter 78 How to Write a Blog Post Blog posts are pieces of writing that are posted to and shared on the Internet. For data scientists, writing blog posts can help to share ideas, projects, and information with others while reading blog posts can help you learn how others analyze data, figure out what new tools are out there, and obtain new skills and tools that will be helpful to you as a data scientist. Thus, learning where to find data science blogs and how to write a good blog post is an important skill. First and foremost, blogs do not have to be the same quality as a published article or academic paper. They do not have to be as polished as a published and peer-reviewed paper would have to be. Instead, blog posts can be of different levels of sophistication and varying lengths. They can be short and just provide a single, simple example. Or, they can be longer, more in-depth, and more polished. 78.0.1 General Outline While there are a number of different types of blog posts (we’ll get to that in a minute!), there is a general outline that can be followed whenever you’re writing a blog post. As with all forms of effective data science communication, a blog post should tell a story. The best blog posts are interesting, clearly-written, and informative. The beginning of your blog post should include some introduction as to why you’ve decided to write this topic and any necessary background information. This means that the blog post should always explain in the beginning why the reader should care and/or what problem is being tackled in the post before diving into the rest of the blog post. After that, any necessary background information should be included. References/citations to others’ work should be included when applicable throughout the post. After you introduce your topic, the body (or middle) of your blog post should be broken down into sections. Sections of the blog post should be separated using section headers to help organize the post and guide readers along. Generally, shorter sections are better than longer (our attention spans as humans are pretty short after all!). These sections should be ordered in a way that logically tells the story you’re trying to tell. If it’s an analysis you’re writing about, you’d start with a section on the data you used, then talk about your approach, and finally share your results (with good figures!). Finally, there should be a conclusion section. This should concisely explain the take home message or messages that you want readers to get from your post. General Outline 78.0.2 Types of Blog Posts While there are many different types of blog posts out there, the four most common are: Announcement How-To Analysis Workflow We’ll describe what each of these is in this lesson and walk through an example or two of each type of post, highlighting the important parts of each post. 78.0.2.1 Announcements Announcement posts most importantly provide information about a tool or resource that may be helpful to the community. They can optionally include an example of how to use that tool or examples of why this information is important to the community. However, most importantly, their purpose is to make the community aware of something. For example, Julia Silge contributed an announcement post to the Stack Overflow blog titled: Public Data Release of Stack Overflow’s 2018 Developer Survey. The main goal of this post is right there in the first sentence. The post was written to inform readers that they can now “access the public data release for Stack Overflow’s 2018 Developer Survey.” Announcement Post The post goes on to describe what the Stack Overflow 2018 Developer Survey is with hyperlinks to information that may be helpful to readers. Readers could go to that link to read all the details of this important annual survey, which asks more than 100,000 developers about the technologies they use, careers they have, and how they learn; however, that’s not required. This is because in addition to making this announcement Julia also includes what types of questions these data can be used to answer with figures and short explanations of each example right in her blog post. Each example in her blog post is separated by a bold header that asks a question. Julia then provides a figure answering that question with a short bit of text after the figure explaining what readers should take away from the figure. Examples using the announcement Specifically, if we look at the section “Who considers themselves part of the Stack Overflow community?” in the post, we see a clear figure with “years of coding experience” on the x-axis and “% who consider themselves part of the Stack Overflow community” on the y-axis. The lines on the graph are colored by gender, with a key for which line is which gender off to the right. Below the figure is a short paragraph explaining in words how readers could interpret the information on this plot. This blog post is a great example of an announcement blog post as it: Makes the announcement Explains why this information is helpful Includes a few clear examples The post uses clear language and examples and does not go into unnecessary detail, which is perfect for an announcement blog post. Generally, announcement blog posts are helpful for announcing new software, new datasets, and/or new resources. 78.0.2.2 How-To Another very common and helpful type of blog post is a How-To blog post. This type of post is also often referred to as a tutorial. These blog posts explain and teach how to do something and include all the necessary information for others to do the same. How-To blog posts on introductory data science topics, such as data wrangling, can be incredibly helpful to those just getting started in. For example, in an earlier chapter when we were discussing the dplyr package, we referenced Suzan Baert’s four-part data wrangling series. These posts are an example of wonderful How-To posts. Through explanatory text and helpful examples, this set of posts teaches users new to programming or programmers new to dplyr how to wrangle data in R. How-To blog posts should introduce what it is the post will teach and then provide all the information needed for the reader to learn the topic being covered. In Suzan’s dplyr posts specifically, the title first describes what readers will learn by reading the blog post. dplyr tutorial She also helpfully includes a table of contents for readers, so that skipping to the section of most interest to you is simple! table of contents Then, for each section, Suzan includes a section title, text to describe what the section will teach, the necessary code, and the results of the code. each section of the post By using this structure, each step is organized into a digestible amount of information. Suzan doesn’t introduce all the topics at once. Instead, she incrementally walks readers through each required step. This is an important part of writing good how-to blog posts! For another example, Hilary Parker wrote a blog post called “Writing an R package from Scratch” on her blog Not So Standard Deviations. While we haven’t discussed writing R packages, we have worked with many different R packages! Each package has a similar structure. Hilary Parker explains how to write an R package in her very helpful how-to blog post! Writing an R package from scratch In this post, she includes a very clear title and begins her post with a story about why she’s writing this post. This helps draw the reader in. Then, at the end of the introduction she informs the reader what they will get out of the tutorial. As her blog post continues, she includes links to where readers can find more in-depth information about the topic as well as clearly defined steps so that a reader could make a package of their own.Steps are separated into sections and code is clearly separated from explanatory text. Information is presented clearly In both of these examples, the How-To Post: Introduced the topic/problem being tackled Included text and code Explained all necessary steps for reader to follow along Writing a good How-To post involves identifying a problem that others may run into and writing a helpful post about how to step-by-step solve that problem. 78.0.2.3 Analysis In addition to How-To posts, data scientists often blog about interesting analyses they have done. One that we have already discussed in previous lessons is Predicting movie ratings with IMDb data and R by Dimiter Toshkov. He includes the main point of his analysis in his title and then introduces where he got the idea to do this analysis. Blog post begins with background information After introducing the question, David Robinson introduces his readers to the dataset he’s used for the analysis, including code for others to also carry out the analysis Dataset introduction The post then walks readers through all the code necessary to reproduce his analysis, text to explain each step in his analysis, and figures throughout the post to display the results. For example, in the “Comparison of words” section of his post, he shows readers the code used to wrangle the data used in his figure. Then, the displays the figure with text beneath explaining what the figure shows. Presenting &amp; Explaining Figures Note that all the figures in this post are clear; however, they’re not always as polished as what you may see in a published article. This is OK! The point is to convey the results of your analysis, not to have the prettiest figures. Thus, time should be spent ensuring that your post is clear and your analysis correct, but it’s not necessary on a blog post to ensure that everything is of publication quality! Of course, you don’t want to share incorrect information. So, it’s best to spend time cleaning your data, ensuring that your analyses are correct, and considering the ethical implications of your work before sharing it online. 78.0.2.4 Workflow Finally, helpful blog posts can also provide readers with a description of the authors’ workflow. These types of posts are helpful to understand how others work on their computers and analyze data. They provide readers with ways to speed up their current data analysis process, introduce readers to new technology, and provide an alternate approach that may be different than your current approach. One popular workflow blog post is from Jenny Bryan titled “Project-oriented workflow”. In this post, Jenny Bryan explains how she sets up her working environment in RStudio using a project-oriented workflow. She begins the post by explaining what prompted her to write this post. She follows up the introduction by defining some important terms (necessary background information!) that will help readers understand the rest of her post. Introduction Then, Jenny Bryan introduces the here package and using RStudio Projects in your workflow. She includes section headers to organize the post and all necessary code to get started using this workflow workflow details It’s a brief but helpful post about how to approach data analysis using projects and what the potential downfalls are of opting not to use this type of workflow. 78.0.3 Why Blog? Now that we’ve covered the four most common types of blog posts, why is this an effective form of communication for a data scientist and why dedicate the time to blogging? First and foremost, writing blog posts can help you clarify and organize your thoughts by writing them down in a clear and concise blog post. This can be helpful to you and others who may read your blog post and learn from it. Second, your potential audience is huge. With a blog post, you have the ability to reach anyone with access to the Internet. Helpful blog posts can help get your name out there, which is important in a field where there are lots of jobs but also lots of individuals interested in those jobs! 78.0.4 Blog Etiquette When looking at others’ work, it’s often easier to be critical than supportive. Being openly critical of others’ work online can also seem easy since there’s a computer screen between you and the person/people whose work you are criticizing. Thus, while being critical of others’ work is OK, it is not ok to be mean or rude when doing so. It’s ok to disagree with others online, however, the comments about others’ work should always be limited to the work itself (no personal attacks!) and the tone should be as kind as possible. There’s no need to make enemies, and you’ll look like a jerk if you act like a jerk online. It’s much better to be supportive of others in blog posts than to be rude. People will be more likely to help you when you have questions in the future and to keep up with your work if you approach blogging in a supportive manner. Keep this is in mind whenever you generate content online, be that a blog post or a comment on someone else’s work. 78.0.5 Summary Now that we’ve walked through the main types of blog posts and why you would want to write a blog post, we’ll leave you with a few of the dos and don’ts of writing blog posts: 78.0.5.1 Do Have an informative title Include hyperlinks and references to others’ work Separate sections with headers Include good figures Use clear, concise language Proofread Blog about things that are interesting to you! 78.0.5.2 Don’t Be a jerk Use a click-baity title Spend all your time making sure it’s absolutely perfect Leave out important details Disparage others’ work Be afraid to put your work out there! 78.0.6 Additional Resources For access to blog posts written by R users, check out the following two resources: R-bloggers R Weekly "],["participating-in-meetings.html", "Chapter 79 Participating in Meetings", " Chapter 79 Participating in Meetings Most data scientists spend a fair amount of their time in meetings, either running them or as a participant. In this lesson we’ll cover when to have a meeting, how to run a meeting, and how to participate in a meeting as a data scientist. 79.0.1 Meeting Etiquette When it comes to meetings, whether you’re running the meeting or in the room as a participant, there are a number of general guidelines when it comes to meeting etiquette. Some of these may be obvious, but we’re including them for completeness. Those who attend many meetings will already be familiar with this topic; however, for those who have not yet attended many meetings, this information is incredibly important. It will help your meetings be more organized and help so others don’t like meetings you host or those at which you present. When it comes to meetings: be on time - show up on time, start the meeting on time, and end the meeting on time be concise - avoid rambling on. People will stop listening to you, and you don’t want that be positive - you can disagree with an aspect of someone’s work while still expressing appreciation on what they’ve done. It’s okay to “What you’ve done here is great and a lot of work. I have one suggestion though…” don’t talk over others - We all get excited about an idea from time to time, but that’s no excuse to talk over others. If someone else is speaking, you shouldn’t be don’t interrupt - Let others finish their ideas. Even if you know the rest of the sentence you’re going to say, everyone else in the room may not and it’s just rude to interrupt others. Let the person speaking finish their thought. avoid being hyper-critical - Simply, being hypercritical is not a good look. It makes others not want to work with you and makes you look generally like a jerk. Find a way to make your thoughts clear without being unnecessarily critical if you point out a problem, try to think of and offer a solution - Providing your thoughts and feedback within a meeting may be incredibly important; however, if you’re identifying problems, it’s best to also offer possible solutions or suggestions While some of these may seem obvious, it can be hard to be a good citizen within a meeting. If you’re less experienced, speaking up may be intimidating so you may be more likely to ramble on when called upon. If you’re more experienced, you may be quick to notice errors others have made because of your experience. In each case, it’s important to remember these guidelines so that meetings do not become things that neither you nor your team members want to attend. Meeting Dos and Don’ts 79.0.2 How to Host A Meeting Sometimes you’ll just be a participant in a meeting while other times you may be the one running the meeting. If you find yourself in charge of hosting and running the meeting, there are a number of considerations to take into account while planning the meeting, during the meeting, and after the meeting. A lot of this has been discussed elsewhere previously and we’ll use this twitter thread to guide this lesson, but it’s important information to cover here as well. 79.0.2.1 Before the Meeting Before the actual meeting it’s important to: Decide If you Need to Have a Meeting Write &amp; Distribute an Agenda Decide When &amp; Where to Have the Meeting Determine Who Should Be There 79.0.2.1.1 Decide If You Need to Have a Meeting When you first decide to have a meeting, the first thing you want to be sure is that there actually has to be a meeting. Meetings are necessary for sharing and discussing ideas among participants or for interactive presentations. If the purpose of the meeting is simply to share information, this should be an email and not a meeting. 79.0.2.1.2 Write &amp; Distribute an Agenda Once you’ve determined that a meeting is truly necessary, you should draft an agenda. This agenda should have times on it for each part of the meeting to keep everyone on time during the actual meeting. When deciding on an order for the meeting, the most pressing and important issues should be covered first. Prioritize those items that will have the largest impact, but take the least amount of work/time to accomplish. Additionally and importantly, this agenda should be distributed to all meeting participants before the meeting 79.0.2.1.3 Decide When &amp; Where to Have the Meeting With a plan in hand for the meeting, it’s up to the host to find the date &amp; time that will work best for the people who have to be in the room and to ensure that the room/space where the meeting will be held is available and has been reserved. 79.0.2.1.4 Determine Who Should Be There When planning and hosting a meeting, ensure that everyone who should be there is invited with enough notice. Similarly, be sure that people who do not need to be at the meeting are not brought to the meeting. There’s no reason to waste others’ time with a meeting full of information that does not pertain to them. 79.0.2.1.5 Send out The Meeting Information There are may different ways (email, apps, etc.) in which people invite others to meetings, so it’s best to use whatever system is customary at your job. Regardless of how you send out the information about the meeting, be sure to send out the date, time, and location for the meeting to all participants. This should be done as early as possible, as peoples schedules fill up. Then, a day or two before the meeting, send out a reminder to all who will be there. 79.0.2.2 During the Meeting Once the time, date, and location are set, participants have been made aware, and an agenda has been sent out, you’re close to ready for the actual meeting. 79.0.2.2.1 Ensure Someone is In Charge First, someone should be running the meeting. This does not mean that this person has to speak the most. In fact, it’s best if this person is not the person leading most of the discussion, as the person engaged in a lot of discussion is less likely to keep everyone on task and on time. Alternatively, the person running the meeting should make sure that the schedule is adhered to and that everyone stays on topic. If discussions begin to veer off topic, it’s the person who’s in charge’s responsibility to cut it off before everyone’s time is wasted. The person running the meeting can always encourage those involved in the tangential discussion to chat outside of the meeting. Additionally, if you’re concerned about stopping the conversation and not wanting to be rude to those in the off-topic conversation, just consider the time of the others in the meeting whose time is being wasted by off-topic discussions. By cutting these types of discussions short, you’re doing everyone else in the room a favor and doing your job as the person in charge of keeping the meeting running smoothly. 79.0.2.2.2 Keep Minutes Someone should be responsible of keeping notes during the meeting. All important information, decisions that were made, and tasks that were assigned should be recorded. 79.0.2.2.3 Meeting Expectations Additionally, at the beginning of the meeting, expectations should be clearly set. If this is a meeting where information will be presented with time for questions at the end, ensure that the audience knows that. If it’s a meeting involving a lot of brainstorming, make sure that is stated explicitly. Additionally, be sure that if many people are given the chance to contribute, that everyone with ideas is getting to contribute. It’s the person in charge of the meeting’s responsibility to ensure that a few loud voices are not dominating the conversation when others have ideas to contribute. 79.0.2.2.4 End Early If the agenda is adhered to and individuals stay on-topic, this should be no problem. If a meeting is to end at 2PM, be sure that it ends at 1:55PM at the latest. This will allow everyone at this meeting to get to their 2PM appointments. If the agenda is not adhered to and topics still aren’t covered by the time 1:55PM rolls around for your meeting that was scheduled to end at 2PM, too bad. The meeting still must end by 1:55PM. It is not ok to take up others’ time due to the meeting agenda being poorly-executed or managed. Always end early. 79.0.2.3 After the Meeting Right after the meeting, be sure to send a meeting summary including all the information in the minutes to all meeting participants as well as those who weren’t able to make the meeting but would have participated had they been free. These meeting minutes can also be referenced in the future to hold people accountable. If someone promises to do something by the next meeting, that will be recorded in the minutes and can easily be followed-up on at the next meeting. 79.0.3 How to Participate in a Meeting If you’re not in charge of running the meeting but are rather a participant in a meeting run by someone else, you have a number of responsibilities as a meeting contributor. If you’re presenting at the meeting, you have additional responsibilities. 79.0.3.1 Your Responsibilities When you’re in a meeting, it’s your responsibility to pay attention to what’s going on. This may mean putting your laptop and cell phone away. Often, it requires at least jotting down notes. As you jot down notes, it’s good practice to write down questions you have. When it’s your turn to speak, you can use what you’ve written down to ask a great question without rambling on. Additionally, as a meeting participant, it is your responsibility to limit the length of your commentary. Avoid rambling on without saying anything important. 79.0.3.2 When Presenting In addition to these responsibilities as a meeting participant, if you are presenting at the meeting, you have additional responsibilities. 79.0.3.2.1 Be Mindful of Time First and foremost, be sure to check on the agenda to see how long your topic has for presentation and discussion. As discussed in the presentation lesson earlier in this course, always stay within your allotted time. Any time that you go over what you’ve been allotted takes away from someone else’s presentation time, and that’s not fair. 79.0.3.2.2 Be Prepared When presenting at a meeting, you should always be prepared with a high level overview of what you’re presenting. This is often referred to as an executive summary. As discussed in the brief reports lesson earlier in this course, this will include only the necessary and minimal details about your work. While you’ll often only present the high level and general summary, you should still know the details of the work to answer questions from others in the room. Additionally, if you’re presenting high-level slides, it’s a good idea to have “just in case” slides at the ready. These are slides that you don’t intend to present but could be used if someone were to ask a particularly detailed question about your project. 79.0.3.2.3 Guide The Discussion This can be tricky, but if someone starts down a path that is tangential or unrelated to the work you’re presenting, it’s ok to cut that line of discussion off. It’s often best to say something like “That’s an interesting idea, but I’m not sure we have time to discuss that now. You and I should talk after the meeting about that.” It can be difficult to do this, especially if the people asking the unrelated or off-topic questions are your bosses, but it’s best for the meeting to do so, so just keep that in mind! 79.0.4 Summary In this lesson we’ve covered the basics of participating in and running meetings. At their core, meetings take up time, thus it’s best to ensure that this is time well spent. To do this, be organized, start on time, keep to the agenda, stay on task, and end on time. If you do this, your meetings will be productive and run smoothly! 79.0.5 Additional Resources Twitter Thread on Meetings, by Greg Wilson How to Run a Meting, by Antony Jay "],["how-to-have-a-one-on-one-meeting.html", "Chapter 80 How to Have a One-on-One Meeting", " Chapter 80 How to Have a One-on-One Meeting While meetings often involve a number of individuals, one-on-one meetings are also common to data scientists. The goal of larger meetings are often to share updates on projects, to brainstorm ideas, or to present recent findings from data analyses. However, for one-on-one meetings, the goal is often to consult. In these cases, individuals with less data science experience will often bring you (the data scientist) a dataset they don’t fully understand. It’s your job to figure out what to do. It’s these types of one-on-one meetings that we’ll discuss in this lesson. We’ll discuss how to navigate these conversations, how to guide the meeting, and what to do after the meeting. 80.0.1 Consulting One-on-one meetings where a project or dataset are being discussed between two people – one interested in answering the question and one the individual with the data science experience – are often referred to as consulting. If, in your job, you’re expected to consult with others at the company regarding their data problems, it’s important to know how to navigate these discussions. Often, these meetings will be scheduled by email (or some other direct messaging system, such as Slack). Meetings of this nature can take just a few minutes (10-30 minutes) if the question requires very little; however, more often than not, these types of meetings take at least an hour to really hone the question that can be answered given the data at hand and to devise a plan to answer the question of interest. That said, in the time between when the meeting is scheduled and the meeting is to take place, it’s ok to try to get some information to best prepare for the meeting to come. Depending upon what the individual seeking your help is looking for, this may be a time to request access to the data you’ll be discussing so that you can familiarize yourself with the dataset or for background information for you to read beforehand. Having this information beforehand can be helpful but is not necessary. It’s not uncommon for the individual you’ll be meeting with to just want to chat at that first meeting, and that’s ok! 80.0.2 What to Expect Once the day of the meeting, all the etiquette rules discussed in the last lesson still apply: be on time, be considerate, don’t interrupt, etc. These rules are often easier to follow when there are just two people having a discussion in a room, but they’re incredibly important. Be sure that you’re respectful of each other in these meetings. Be aware that there may be concepts that are familiar to you and unfamiliar to them. And, know that the opposite is true: things familiar to them may be unfamiliar to you. This is what makes meetings great – you can both learn from one another. However, for this to be true, you must respect the other person and never put someone down for what they don’t know. 80.0.2.1 The Investigation Once the meeting begins, the start to the talk is a lot like an investigation. This initial discussion involves the person seeking consultation explaining the problem they have, the data to which they have access, and the question or questions they want to answer. Your job at this point is to: figure out what kind of question they want to ask determine what they already know about this question identify what data they have in hand figure out what data they think they’ll have in the future determine if they can answer the question they want identify any constraints on the data 80.0.2.1.1 The Question Often, people will come to you with a vague question of interest. It’s your job to help form a data science question using the skills learned in Data Analysis course. Ensuring that this question can be answered and is specific enough should be your goal. By having a discussion in this meeting and explaining why the question must be specific will help you both reach this goal. 80.0.2.1.2 Prior Knowledge It’s important to understand what the larger community already knows about this topic and what the individual you’re meeting with has already learned from the work they’ve done. This is where you should do a lot of listening and learning. Eventually, the person you’re meeting with should learn some analytical skills from you, but at this point you’re learning from the person with whom you’re meeting 80.0.2.1.3 The Data Often, people meet with you after they’ve already collected data. Ideally, it’s best to meet before data are collected to ensure that the best possible data (with the right variables and for the necessary number of people) are collected; however, in reality, that’s not always the case. Often, you’ll be contacted after data has been collected. In these cases it’s important to identify what data the individual has already. For datasets that are not tidy, it’s best to discuss the principles of tidy data and to determine what their data would look like if it were in a tidy format. You can decide whether or not you’ll do the tidying with the data they have or whether they’ll tidy the data; however, moving forward, regardless of who does it, you’ll need to work with a tidied dataset. Also, by discussing what form the data should be in for easy analysis, you can determine how to best collect data going forward. Often, the individual will have a plan to collect more data or will be aware of other datasets that they’ll be receiving soon. In this meeting, it can be helpful to discuss how to format the data they’ll be collecting in the future. Additionally, by discussing what data they’re expecting to receive, you’ll be able to devise a better plan for analysis. 80.0.2.1.4 The Analysis After discussing the type of data and the question of interest, this is where you, as the data scientist, will determine what type of analysis is most appropriate. Given the information at your disposal, you’ll determine what approach you’ll take to answer the question of interest. You’ll have to determine what variables will be used to answer the question of interest, how you’ll deal with missing data, what your plan is for outliers, and determine whether there possible confounders in the analysis? This is where you’ll be doing the teaching and the person you’re meeting with will get to learn. Be sure that everything you say is clear to the other person in the room. Always give them the chance to ask questions. They may be much less familiar with the topics you’re discussing, and it’s your responsibility to ensure that you’re clearly explaining everything. That said, it’s their responsibility to ask questions if anything is unclear. Together, with the best understanding possible on both sides, the project will be able to move forward. 80.0.2.1.5 Limitations It’s always important to consider what limitations there are to your analysis. To do this, again consider what the perfect dataset would look like to answer this question. Then, think about how the data you have differs from this optimal dataset. Those differences are the limitations to your data. A similar mental exercise can be done for the analysis. If you’d like to establish a causal relationship between two variables but only have observational data, you may like to have a randomized trial dataset. But, you don’t. This is a limitation. Thus, you’d report that you are only reporting an association and are not assigning causality. It’s important to determine before the analysis what your interpretation of the results would be however they turn out after the analysis. This will help you avoid over-interpreting your findings after the fact or trying to conclude something beyond what the results of your analysis suggest. All projects have limitations. And, that’s ok! However, they must all be discussed and reported. 80.0.2.2 The Checklist To ensure that you’ve not missed anything in this meeting, using this checklist to guide your meeting may be helpful. By familiarizing yourself with the questions here and reviewing it in your meeting, you’ll be more likely to have a productive consulting meeting. Have you…: Determined a specific question? Discussed what information is required to answer question? Decided what variables you’ll use in the analysis? Discussed what data are currently available? Determined if available data in a tidy format? Discussed data that will be available in future? Discussed limitations to the dataset? Determined approach that will be used to answer question? Discussed why this approach is the best approach? Discussed limitations to this analysis? Asked if there are any questions? Asked if anything is unclear? Consulting Checklist 80.0.2.3 Assign Responsibility Once the question has been honed, available data have been discussed, and an analysis plan has been decided, it’s time to decide who will be doing the analysis. This often depends on how the company is structured. Sometimes, it’s your responsibility to do the analysis. Other times, they will do the analysis and discuss results with you at a future meeting. It’s important at this point to determine who is responsible for doing what and to set up a future meeting if necessary. It’s also best to verbally summarize the meeting. By discussing the question of interest, the data, and the analysis, you’ll both be on the same page. It’s also best to write notes down at this point. Every data scientist has thought they’ll remember something only to look back in the future and not be able to remember some important detail. Keeping these notes in a lab notebook is a great idea. We’ll discuss exactly how to do that in a future lesson in this course. 80.0.3 Follow-up Meetings After the first meeting, you’ll each carry out what you agreed to in the first meeting. Then in follow-up meetings, you’ll: report on what you’ve done discuss limitations determine their understanding ask questions answer questions In this meeting, you’ll each report on what you’ve done. It’s your job to explain what you did and explain why you did what you did. You’ll want to explain how you did the analysis and what your interpretation of the results are. You should also discuss any limitations of the analysis at this point. Did you not have the data you needed? Was the sample size not big enough? All of this should be discussed. After discussing the results and limitations, it’s best to determine how well you each understand the project. It’s possible there’s some piece of background information you’ve not understood perfectly or some part of the analysis they don’t understand. It’s best to determine whether or not you both understand what’s necessary for understanding of the project and the analysis. Finally, there should always be a portion of this discussion where you both get to ask and answer questions. You may need to further clarify or explain the analysis. They may need to further clarify what they were looking for. This is all fair game and should be discussed in this meeting! The second meeting may be the last; however, projects often involve more than two meetings. In between each meeting, emails or Slack discussions can help move the project forward. Eventually, once all required analyses have been completed and the question has been answered, these meeting may not be necessary any longer; however, it is ok to need to meet multiple times on a single project. 80.0.4 Common Pitfalls Working with others is one of the best parts of being a data scientist. You get to learn from and teach others regularly as part of the job! However, any time individuals with different educational backgrounds, different skill sets, and different personalities work together, there is a chance that things won’t go perfectly. Here, we’ll discuss some of the common pitfalls of one-on-one meetings and offer approaches to avoiding these potential obstacles. 80.0.4.1 The Initial Meeting At the initial meeting, the person you’re meeting with: may miscommunicate could struggle to be specific enough won’t know what they want This miscommunication may be unintentional (they don’t understand the data they have yet) or intentional (they are afraid or don’t want to be honest with you about the data). In these cases, when you go to analyze the data, you may realize that a miscommunication has happened. In these cases, it’s best to go back to your collaborator and discuss your reservations and what miscommunication has happened to correct the record and more forward. If their miscommunication was meant to deceive and they still won’t be honest with you, it’s best to consider whether or not you should continue to work on this project. Here, discussions with your boss or human resources may be necessary. Always try to discuss and work with your collaborator first; however, if people are acting unethically and dishonestly, it is not your responsibility to continue to work on the project. If your collaborator is struggling to be specific enough, it’s often because they haven’t thought fully enough about their project yet. In these cases, it’s best to explain why being specific in asking their question is important using what you learned in the Data Analysis course earlier in this course set. Additionally, by looking at the data they have available and doing some exploratory analyses and discussing the results, you may be able to help them toward being specific enough going forward. Finally, if the person you’re meeting with doesn’t know exactly what they want, that’s ok! Be prepared to have ideas and suggestions to try to get them on the right track. Providing solutions in this case is the best way to get everyone back on track! 80.0.4.2 Follow-up meetings At follow-up meetings, there are further common pitfalls to be aware of. At these meetings: the data may not show the desired results information could be missing your collaborator may not understand or may misinterpret the findings Projects often go differently than planned. The results may not show what you were hoping for. Projects go in different directions than what was initially intended. All of this is more than okay. As a data scientist, your job is to answer interesting questions using data. If the data show something different than what you were expecting, check to make sure that your analysis was correct. If it was, then the results are what results are. It is never ok to change your analysis because you or your collaborator don’t like the results. If a collaborator ever tries to convince you to do something unethical or to obtain results that they want to see rather than accept the results for what they are, it is your responsibility to push back. Stick to your code of ethics and always explain why you are unwilling to do so to best explain where you’re coming from. If you have not received the data you were anticipating or if necessary information you thought were in the dataset are not, this could cause issues at follow-up meetings. In this case, explain the limitations to the analysis should the data or information not be provided. If your collaborator now states that those data will not become available, consider re-working the question or approaching the analysis differently given this new information. Finally, given differences in backgrounds, a collaborator may misinterpret or not understand the results of your analysis. In the meeting, it’s always best to ensure that you’re on the same page. But, what if you think you’ve done that and then see in an email from your collaborator to your collaborator’s boss reports of information that is not what your analysis concluded. In these cases, it’s best to follow-up directly with your collaborator, explaining (kindly) where they’ve gone wrong. Often your collaborator will realize their misinterpretation and correct their mistake in a follow-up email to everyone. If they refuse to correct the record; however, it’s then your responsibility to do so. This won’t always be comfortable, but it’s most important that your analysis not be misinterpreted. Generally, it’s best to give your collaborator the benefit of the doubt first and to discuss misunderstandings with them directly first. If they are resistant, however, it’s your responsibility to make sure that your analysis is not being used incorrectly. 80.0.5 Data Science Ethics We’ve discussed ethics in previous courses and briefly above in this lesson, but we want to be explicitly clear: as a data scientist it’s the process is what matters not the result. Your job is to do data science, not to produce results that make your bosses happy. If the analysis makes your boss happy, awesome! But, you should never be pressured to change an analysis just to please a co-worker. This can be a challenge, especially if pressure is being applied to you. In these cases remember, ethics is more important than results. Don’t succumb to the pressure - it’s ok to push back, even if it feels like something difficult to do. 80.0.6 Summary In this lesson we’ve discussed the basics of one-on-one meetings, how to approach the meetings, what to cover in the meeting, and what your responsibilities are as a data scientist in one of these consultation-type meetings. Additionally, we’ve covered a number of common pitfalls that can happen in these types of meetings, and how to handle it should one of these pitfalls happen to you. Finally, we discussed the need to always be true to your ethical code and never change your analysis or results simply because someone wants a different outcome. 80.0.7 Slides and Video How To Have a One-on-One Meeting Slides "],["github-and-final-data-project.html", "Chapter 81 GitHub and Final Data Project", " Chapter 81 GitHub and Final Data Project In this project, we are going to have you form your own question and find data to explore it. But, we will also want to practice using version control and getting our projects to be open source and available on GitHub. 81.0.1 Starting up this project Follow the Creating A Repository steps and subsequent chapters to create a new GitHub repository for this project. Follow the instructions in Cloning A Repository chapter to also clone this project to your RStudio workspace Work on formulating your data science question – try to pick a topic you are interested in! You can gain inspiration by browsing some of the open source dataset websites we have discussed in the Finding Data chapter. When you find a dataset and question combo you are interested in, you can feel free to borrow this R Markdown template to get you started. You can find this same file in your DataTrail_Projects RStudio project. 81.0.2 Your objectives! To complete this project there are a few requirements you will need to fulfill. Remember that you are not on your own for this project! Data science is done best as a community, so please ask others (and instructors) questions you have when you get stuck! Clearly state the data science question and goal for the analysis you are embarking on. This project should be completely uploaded and up to date on GitHub. Follow the steps in Pushing and Pulling Changes chapter for how to git add, commit, and push the changes you have done. Follow good organization principles – you should at least have 2 folders: a results folder and a data folder. 4. 4. You should also have a README Make a resulting plot that you save to a file. Write up your final observations in regards to your original question. Note that some data science projects end with “This isn’t what I thought it would be” or “that’s strange” or “I think this is leading into another question I would need to investigate”. Whatever your observations may be, write them up in your main R Markdown. When you feel your analysis is ready for review, send your instructor the GitHub link to your project so they can review it. Pat yourself on the back for all this work! You are a data scientist! "],["building-a-resume.html", "Chapter 82 Building a Resume 82.1 Learning Objectives 82.2 What You Need to Find a Data Science Job", " Chapter 82 Building a Resume 82.1 Learning Objectives Through the completion of this section our goal is that you will be able to: Build a resume and understand what features are a part of an effective resume Make your own professional website Show off the great work you’ve done! Use Twitter to build connections with the data science community Find data science job openings Communicate effectively in an interview and conduct a presentation Meet up with data science individuals online or in person 82.2 What You Need to Find a Data Science Job Whenever you’re looking for a job, in data science or otherwise, there are a number of things that you’re required to submit to officially apply for a job, such as a cover letter and a resume. However, for data science jobs, there are a number of other steps you should take outside of what you submit to each company to give yourself the best chance at getting the job you’re interested in! These include ensuring you have a personal website and that it is up to date and ensuring that all your professional social media accounts are up-to-date! In this lesson, in particular, we’ll cover an overview of what you need to find a data science job. And, then, in the lessons that follow, we’ll discuss the ins- and outs- of those official documents (resume, cover letter, etc) and those less official steps to take (updated website, Twitter presence, etc.), helping to ensure you get each of these set up to move forward. Note that there will be a lot to do throughout this course, but that’s what it takes to get a job! So, read carefully, follow along, and make all the updates and changes outlined throughout this course to start preparing yourself for looking for a position! 82.2.1 Job Applications When applying to jobs, they minimally will require you to submit a cover letter and a resume. resume &amp; cover letter 82.2.1.1 Resumes Resumes are (typically) one page documents that include how to contact you along with information about your education, skills, and experience. They are meant to be read at a glance, so they should be brief, clear, well-formatted, and include only the essential information. 82.2.1.2 Cover Letters In addition to a resume which will not change much from one job application to the next, you will also submit a cover letter. A cover letter is supposed to convey both why you’re interested in the job to which you’re applying and why you’d be a great fit for that job. These should also not exceed one page. 82.2.2 Online Presence In addition to the official job application materials, the individuals responsible for hiring decisions at the company where you’re submitting your application will have access to the Internet. This means that if you get through their initial screening, where they look at your cover letter and resume, they’ll likely search for you on the Internet. This means that your online presence should always be up to date when you’re applying for jobs. Online Presence 82.2.2.1 Personal Website You will want to have a personal website. With GitHub this is not too difficult (and its free). We’ll guide you on how to set this up! 82.2.2.2 Social Media In addition to a personal website it will be important to have your GitHub, LinkedIn and Twitter profiles up-to-date. Briefly here we’ll discuss why this is important and in the lessons in this course, we’ll walk through the steps necessary to take to get everything ready for applying to jobs. GitHub - hiring managers will look here to see work you’ve done and how much you’ve worked with others LinkedIn - is a great place to get more details beyond what is included on your resume Twitter - is where you’ll want to share your work and support the work of others It’s important that each of these profiles are up-to-date, contain all the information hiring managers would want to see, and are easy to find. We’ll make sure you’re all set with this throughout this course! 82.2.3 Additional Resources How to be a modern scientist, by Jeff Leek "],["resumes-1.html", "Chapter 83 Resumes", " Chapter 83 Resumes When applying to jobs, you’ll almost certainly be asked to submit a resume. Anyone can write a resume, but it’s critical that you submit a good resume when applying to jobs. We’ll discuss how to generate a good data science resume in this lesson. 83.0.1 What is a Resume? First and foremost a resume is a short document that describes one’s qualifications for a job. More specifically, this document will include your contact information, a brief summary of your qualifications, your experience, and education. And, importantly, this document will be short (usually, no more than one page) and easy to read. This means that the document must be organized, well-formatted, and clearly written. 83.0.2 General Features Before we jump into each of the sections on a resume, let’s review a few general features of resumes. First, resumes are brief. Resumes should almost certainly not exceed one page. If you feel like you have too much to say in a single page, consider the organization and formatting of your current resume. Do not consider going to a second page. Those reviewing your application should be able to glance at your resume quickly and garner the information they need. Do not count on them turning to a second page. Instead, put all the pertinent information on that first page. Second, bullet points are ok. Resumes are intended to be read quickly, so full sentences are not necessarily required. Bullet points are easier to read quickly than paragraphs. Thus, use bullet points on resumes. Third, use action words. Action words that describe what you did for specific projects or at a previous job should be used on your resume. And, the words you use should vary. If you led a project, you could use “Chaired,” “organized,” or “oversaw.” If you carried out the work on a project you may use “developed,” “designed,” “implemented,” or “devised.” If you saved your company money with the results of a previous analysis, you’d use words like “conserved,” “reduced,” or “decreased.” The purpose of these examples here is not to state the best action words to use on your resume, but rather it’s to demonstrate that there are a lot of action words out there. Use them. And vary the ones you use. Don’t simply use “Developed” over and over again throughout your resume. Fourth, your resume should be organized and well-formatted. The goal of your resume is for a hiring manager to look at your resume, learn about you, and want to hire you at a glance. Thus, be sure that the hiring manager is not focused on the poor spacing, the small font, the hard-to-read color, or the disorganization of your resume. Be sure that the most important information jumps off the page and that nothing is hard to read on your resume. Fifth, when organizing your resume, put information in reverse chronological order. This means the most recent things should be first within a section and the oldest things should be last. Finally, everything on your resume must be truthful. Do not falsify any information on your resume. Do not include things you plan to do but have not yet done. Lying on your resume is never acceptable, no matter the circumstance. 83.0.3 Formatting To ensure that your resume is formatted appropriately, we’ll discuss a few guidelines here. At the end of this lesson, we’ll include a few sample resumes though and you’ll see just how vastly resumes can differ visually and still be good resumes. 83.0.3.1 Fonts The font you use on your resume should be easy too read. Avoid script fonts or ones that look like something a small child would write. Stick to fonts that can be read with ease. Times New Roman has been used historically; however, Helvetica, Arial, Verdana, and Calibri are also easy to read. 83.0.3.2 Font Size Your name will likely be the largest piece of information on your resume. However, nothing on the page should be hard to read. Generally, for most fonts, the smallest font size you should use is 10 point. Section headers should be larger than the smallest font on the page but smaller than your name. 83.0.3.3 Colors Sometimes, resumes will use color. This could be used to make your name and section headers stand out. Stick to colors that will show up regardless of whether the document is printed in color or black and white. This means that darker colors are safer. Navy blue or deep purple are better than sky blue or pastel purple, for example. 83.0.3.4 Be Consistent Don’t use too many different colors. If you use a color for one section header, that exact same color should be used for all the section headers. The same goes for font and font size. Do not distract readers by using lots of different fonts. And, be sure that each element of your resume uses a consistent font size. Text in one section should be the same size as text in another section. Section headers should all be the same size. Spacing between the sections should be consistent. Consistency is key. formatting matters 83.0.4 What to Include Now that we’ve covered some resume basics, we’ll step through what you should include on your resume. 83.0.4.1 Name &amp; Contact Information First and foremost, your name and contact information should be included and should be at the top of your resume. This will include your mailing address, but should also include an email address and phone number. Further, for data science jobs, you should also consider your GitHub username and personal website. This will be a recurring theme in the lessons throughout this course. From each location, be it your resume, GitHub, or personal website, viewers should be able to easily access all other platforms where your information is stored. It should be easy to navigate to information about you between all platforms. That said, it’s not necessary to put your LinkedIn or Twitter handle on your resume, unless the employer requests it. 83.0.4.2 Brief summary While the general rule for resumes is that bullet points are ok, the summary is the one exception to that rule. This should be a short (2-3 sentences) paragraph that summarizes your qualifications for the job to which you’re applying and what you’re looking forward to doing at that position. While most of the information on your resume does not change as you apply to different jobs, this section should change and be specific to the job to which you’re applying. 83.0.4.3 Skills In addition to stating who you are, how to get in contact with you, and where to learn more about you, it’s imperative that your resume highlight the pertinent skills you have for the job to which you’re applying. For data science positions, this section should specifically explain your programming and data analysis skills. If you’ve completed the courses in this course set, you’d be sure to include that you are comfortable in R and are skilled at data wrangling, data visualization, and basic data analysis. However, for any skill you state you possess, you should demonstrate examples of actually having and applying these skills in your experience section (discussed below). 83.0.4.4 Education Your educational history should be included on your resume. Each entry should include the institution, what degree or diploma you earned, the years in which you attended the institution, and the city and state where that institution is located. This section should also include online programs (such as this one, once you’ve fully completed it!) and any pertinent other job training, such as workshops attended or other pertinent certificates earned. Note that you should not include certificates you’ve earned that are not related to the job to which you’re applying. Once completed, this Course Set should be included in this section. However, you should not include this on your resume until you’ve completed the entire Course Set. 83.0.4.5 Experience Finally, your previous job experience should be included here. If you have years of data science experience, then it’s not necessary to include other, unrelated jobs. However, if you have years of job experience demonstrating your commitment to working for a company for an extended period of time but don’t yet have data science experience, put this job down. Employers want to see that you have worked at a job for a period of time. After stating the employer, your title, and the location of each position, it’s customary to include bullet points of your responsibilities and projects worked on while employed. This is where it’s crucial to use bullet points, action words, and to be clear and concise. This section demonstrates to the employer your experience and what role you played at your former positions! If you don’t yet have pertinent job experience, this section should include information about projects you’ve worked on. 83.0.4.6 Projects This section is not generally required, as the bullet points explaining what you’ve done at previous jobs should explain the projects you’ve worked on and the role you played. However, if you don’t have formal data science experience working with a company, it’s incredibly important to demonstrates what projects you’ve worked on. These could be projects you’ve worked on throughout your coursework in this course set. Or, these could be projects you’ve worked on on your own. We’ll talk about your project gallery and what it should include in a later lesson; however, it’s important to show to employers that you’re interested in data science work and have worked on projects on your own time. These projects should certainly be included and described on your resume, especially if you do not yet have official data science experience. 83.0.5 Example Resumes To demonstrate how to format a data science resume, we’ll walk through a few examples. Note, none of these examples are actual data science resumes for real people. Rather, we’re using these as templates to highlight how to organize a data science resume. 83.0.5.1 Classic Historically, resumes have not been visually stunning. They’ve been clear and well-organized with a focus on formatting; however, they have been visually standard with one section after another. Bold and underlined text have been used to highlight text with section headers sometimes being a different color and generally being a larger font size. Here, we see an example of one such resume: classic resume We’ll highlight a number of features of this type of resume to draw your attention to important aspects of a data science resume. First, notice that the applicants name and contact information are right at the top of the resume, and the name in particular stands out. Name and Contact Information are prominent Second, note that the font size, spacing between lines and sections, and chosen fonts are consistent throughout the resume. consistency is key Additionally, the expected sections are present and separated visually from one another. Further, within each section, bullet points are used and the text are aligned consistently from one point within the section to the next expected sections are present visually separated Note that no resume is perfect. In fact, there are many things that could be improved about this resume. For example, white space is to be avoided whenever possible on a resume. So blocks of white, empty spaces are opportunities to explain what you’ve done. This does not mean that you need to add more text to fill space. Rather, maybe font size could be increased or a different layout would have been able to display the important information more effectively. Additionally, the Projects section could use work. There are too many bullet points for Projects. Three to four bullet points for any one topic is usually plenty. We would want to see if we could remove one of these bullet points. Also, the bullet points are smushed up against the underlined project name. It would be best to add a small space after the project name and the bullet points. Blank white spaces should be avoided &amp; spacing and bullet points could be improved After noticing these, we would want to go back to the resume and make these improvements before applying! Your resume will be one of the first things seen by your possible future employer. You want to be sure to put your best foot forward! 83.0.5.2 Creative Recently, with the frequency of data scientist positions within smaller startups, there has been some room for flexibility and creativity in data science resume design. Going with the classic design is a safer route; however, for positions where creativity is something the position is looking for, it may be ok to go with a somewhat different resume design. Here we see an example of a less traditional resume. Information very similar to what was seen in the last example is displayed; however, the formatting is very different. We’ll similarly walk through this resume design highlighting what it does well and where it could be improved. creative resume As we saw in the last example, the applicant’s name and contact information are clearly at the top of the resume. Here, the addition of small icons makes this information slightly more appealing. name and contact information are visible and visually-appealing However, unlike in the classic view, sections are not spaced one on top of the other. While all the same sections are still there, the page is separated using one gray block in the middle to highlight projects the applicant has used. And, sections are separated using either blue text, or white text within blue shapes. These do not all start at the left end of the page. Instead, they can be either on the left or in a second column on the page. Spacing, coloring, and layout differ in this resume As mentioned above, no resume is perfect. Maybe the gray is a little too dark. Maybe you wish you had more room to talk about your skills, as you did in the traditional resume view, but that you don’t have with this layout. Or, maybe the hiring manager won’t like this type of resume, as it’s not what they’re used to looking at. By choosing to use a less-traditional resume type, you’re taking a risk. It may make your resume stand out, or it could confuse be something the company doesn’t like. Know that there are risks behind using a less-traditional resume. It’s important to consider the limitations of your resume and the changes you should make to improve it before submitting it with your application! Creative resumes layout limitations 83.0.6 Sharing your resume Last, once your resume has been written and formatted, you’re all ready to send it to employers. This should always be in the PDF format! After saving your resume as a PDF, always open it up, take a look at it, and make sure the formatting looks perfect in this format. If there are things you want to change, go back and edit your resume. Then, re-save as a PDF and look at the formatting. Do this until your resume is perfect. You wouldn’t want a hiring manager to skip over you because your resume can’t be read easily or is missing information. It’s certainly worth it to spend a great deal of time on your resume. 83.0.7 Summary In this lesson we’ve reviewed the general guidelines for resume writing, the important features of any resume, and a few examples of data science resumes. It’s best at this point to get working on your resume. You’ll be asked to submit a link to your resume in a lesson later in this course as a quiz response. And, while you’ll always have to update this document and there will always be ways to improve its appearance, the hardest part is getting a first draft, so get working on that now! 83.0.8 Additional Resources Data Science Resume Examples Advice for Applying to Data Science Jobs Example Resume Templates - Classic ; Creative "],["cover-letters-1.html", "Chapter 84 Cover Letters", " Chapter 84 Cover Letters Cover letters are a supplement to your resume with a different goal. While the goal of your resume is to provide hiring managers with information about your qualifications for the position, the goal of a cover letter is meant to demonstrate that you are the right person for the job and convey your personality while still being professional. As such, the cover letter you write should be tailored specifically for each and every job application you submit. Since the company you apply to will have both your cover letter and your resume, your cover letter should not discuss every aspect of your career and educational history. In fact, that’s in your resume, so don’t make people read the same information twice. Instead, your cover letter should should be memorable. This can be accomplished by telling a story or two about your background as it pertains to this job in particular. Or, you can talk about your personality and skills and give examples of things you’ve accomplished using these that will help you succeed in this position. Finally, cover letters are a place to express why you want to work at the organization or company to which you’re applying. The cover letter is a great place to express all of these sentiments. As with your resume and every aspect of any job application, all information contained within your cover letter must be true. If a job is in London and you have no desire to move to London, then don’t state that you have been looking forward to a London move for years in your cover letter. Similarly, if you tend to be a serious and not bubbly person, don’t give off the impression that you’re a bubbly individual through the stories and language used in your cover letter. To note, being a serious or bubbly person are both fine personality traits to possess! The important thing however is that you only ever convey information about yourself that is true. Finally, always proofread your cover letter (and all your application materials!) before submitting your application. Typos and grammatical errors reflect poorly upon you. It’s worth the extra effort to ensure that you do not have errors in your cover letter! 84.0.1 Format Unlike a resume where there are sections that hiring managers expect to see and will be looking for, cover letters are a lot more flexible in their contents and format. The cover letter should in fact be a letter, so it should follow the following general format; however, what is contained in each of the paragraphs can vary a lot from one letter to the next: Cover Letter You’ll note here that your contact information should be at the top of your letter. Then, in the letter itself, you should include the date and then address the letter to the hiring manager or recruiter specifically. The hiring manager’s work address should then be included. Contact Information Note that it’s best to figure out the name of this individual rather than addressing the letter “To whom it may concern.” Make the effort to figure out who will be reading this letter by searching on the Internet or talking to individuals at the company. Use the hiring manager’s name After including your contact information and the company’s information, you’ll address the hiring manager specifically and then begin your letter. Cover letters need only be a few paragraphs and should not exceed a single page. We’ll discuss what should be in those paragraphs in the next section of this lesson. Content of letter Finally, close your letter and include your name on the line after your closing (i.e. “Sincerely,”). Close your letter and include your name This is the basic format of a cover letter. Here, we’ve shown a basic template with minimal formatting. However, you can change the colors or layout to best fit your personality and the type of company to which you’re applying. New startups will likely have fewer expectations as to the format of your cover letter, where you have some more flexibility in changing up the letter’s appearance. Corporations that have been around for a long time may have a greater expectation that your cover letter not stray too far from what you see here. Be sure to adjust the format of the letter to best fit your personality while still matching with the hiring company’s expectations. Regardless though of where you’re applying, avoid fonts that are hard to read, be sure that the layout and formatting does not distract the reader from the content, and save your cover letter as a PDF to ensure the formatting of your letter appears to the hiring manager as it does to you. 84.0.2 Content Now that we’ve discussed the general format of the cover letter, let’s discuss what is expected to be in each of these paragraphs within the body of your cover letter. 84.0.2.1 Intro The goal of your introductory paragraph is to draw your reader’s attention in. Open with a unique line and be sure to express in this paragraph that you have a clear understanding of what the company does and what they care about. Additionally, mention a few roles you’ve had, projects you’ve worked on, traits you possess, or passions you have that make you an ideal candidate for the position. Intro 84.0.2.2 Body The body of the paragraph should include specific examples of experiences you’ve had or things you’ve accomplished that align with the keywords stated in the job description. If the job description states they are looking for someone “passionate about data,” it is not sufficient to say “I’m passionate about data” in your letter. Rather, give an example of your passion. Was there a time when you came across an article in the news and just had to find the data and analyze it yourself? Tell that story! Tell what you found. Or, was there a time that you generated a dataset to answer a question in your own life that had been in the back of your mind for a while. Tell that story! Be sure to use a specific example or two and at least a few of the keywords stated in the job description in this section. Talk about your work, accomplishments, passions, and interests in this section and use specific examples. Body 84.0.2.3 Closing There is a balance to be struck in the closing. You want to summarize the interests, qualifications, and passion you’ve already stated in the letter, but you do not want to simply repeat what has already been said. Be sure that you’re summarizing without simply repeating. You never want the hiring manager to think “didn’t they already say this?” while reading your letter. Be sure to include how you would specifically contribute to the company given your attributes and skillset. Let your personality shine through in this paragraph. Closing 84.0.2.4 Call-To-Action Give the reader a reason to contact you. This does not mean you should include things like “I’ll call to schedule an interview.” That is off-putting, overly-aggressive, and not your place. It is their job to contact you about a possible interview. Instead, your call-to-action should be a polite suggestion, such as “I’m excited to hear from you and look forward to the opportunity to provide you with more information.” Additionally, it’s best to thank the reader for reading your letter at this point in time, with something like “Thank you for taking the time to read this letter, and I look forward to discussing this position with you more in the future.” Call-To-Action 84.0.3 What to Avoid We’ve hinted on a few things to avoid in cover letters throughout this lesson, but we figured it would be best to summarize them all in one place here: Do your best to avoid: fonts or colors that are hard to read instead: stick to dark colors and simple fonts starting your letter with “To Whom It May Concern” or “Dear Sir/Madam” instead: take the time to figure out who will be reading this letter using generalized statements like “I’d be a perfect fit” without explanation instead: use specific examples to back up that statement instead: tell a story from your experience that explains why this is true instead: use keywords from the job description and specific examples from your experience being overly-aggressive instead: express enthusiasm and interest while still being professional overused phrases instead: be honest and specific in all your statements including unnecessary information instead: edit your letter to only include important information instead: pare down to just what’s essential typos or grammatical errors instead: proofread yourself instead: have someone else also proofread plagiarism - never copy anyone else’s writing instead: use your own words and experiences to let your personality shine through instead: avoid copy and pasting from examples of others’ cover letters 84.0.4 Summary In this lesson, we’ve reviewed the goals, format and content of a cover letter that will make you stand out. If you let your personality come through the letter and stick to using examples and stories that are specific to your experience, you’ll have a cover letter that is unique and that is not simply glossed over by the individual in charge of hiring. Take the time to ensure that your cover letter is neither boring nor generic. 84.0.5 Additional Resources Cover Letters Templates (from this lesson) - skeleton; content Data Science Cover Letters, by Jordan Goldmeier Even for Data and Tech Jobs, a Cover Letter is the Best Way to Sell Your Human Skills, by Stephen Goldmeier 84.0.6 Slides and Video Cover Letters Slides "],["make-your-own-website.html", "Chapter 85 Make Your Own Website 85.1 More customization of your website", " Chapter 85 Make Your Own Website Follow these instructions to make your own professional website using GitHub. Make a new repository and name it username.github.io But put your own GitHub username where it says “username”. Make sure this repository is public and initialize it with a README. Go to RStudio cloud. Open up a new project using the steps to create a project from a GitHub repository. You can see these instructions here to refresh your memory: https://datatrail-jhu.github.io/DataTrail/creating-a-repository.html In this new RStudio cloud project, create a RMarkdown document. Save the R Markdown document as the name “index.Rmd” and put it in a folder called “docs”. This is important for publishing purposes! Put the content you want on the website in this RMarkdown. You might want to use a template like this: ## About Describe who you are. For example, what you are currently studying. Summarize your trajectory. You could mention what you&#39;ve done. Like what you&#39;ve studied or where you&#39;ve worked. ## Interests * Interest 1 * Interest 2 * etc ## Projects * List some of your recent projects * You could include this website as a project! ## Profiles * [LinkedIn](https://www.linkedin.com/in/yourprofile/) * ... * [GitHub](http://github.com/username) ## Contact * [youremail@email](mailto:youremail@email) Edit the theme to your liking by following these instructions: https://cran.r-project.org/web/packages/prettydoc/vignettes/tactile.html Knit the R Markdown document to see how it will look. When you have it close enough to what you’d like it to look like, add, commit, and push the changes to your GitHub repository. Here’s the instructions on how to do that if you need a refresher: https://datatrail-jhu.github.io/DataTrail/pushing-and-pulling-changes.html Go to your GitHub repository. Go to Settings &gt; Pages. You should see a URL to your new website. Scroll down and underneath “Branch” change the folder to “docs” and click save! Go to the site (it will take a few minutes for it to properly render). The URL will be something like this: https://username.github.io/ 85.1 More customization of your website In this lesson, now that you have these skills and are more comfortable, we’ll update the look of your website using the blogdown package. In this lesson, we’ll cover both how to add more information to your website and how to make it more professional. In doing this, we’ll provide examples from others’ websites. We hope that by the end of this lesson you’ll have a website is helpful to those interested in hiring you! This will be a long lesson, but it will be worth it. Go through each step in RStudio Cloud as you read through the lesson to update your professional website! 85.1.1 Blogdown blogdown is an R package that helps create websites with R Markdown. There is a whole book dedicated to helping users use this package, so if you want to learn more about blogdown beyond what is covered in this lesson - that book is a great place to start. blogdown Simply though, blogdown allows users to generate static websites. This means that you will generate HTML files within blogdown that visitors to your website will be able to view. The content will appear the same to the viewer no matter who it is visiting your site. This is perfect for a personal website! 85.1.2 Getting Started In an earlier chapter, you developed a basic website within RStudio Cloud. At the end of that lesson, you had a website that looked something like this: basic personal website It contained some basic information about you and had information about where to contact you; however, it was pretty basic and not very visually appealing. In this lesson, we’ll improve both the content and look of your personal website. To get started, create a new project in RStudio Cloud! Once you’ve got a new project, you’ll want to run the following code in the R Console to install blogdown and start a new site. (Note: it may take a few minutes for this code to run and for all the dependencies to install.) ## install packages install.packages(&quot;blogdown&quot;) blogdown::install_hugo() ## create a new site ## use the academic theme blogdown::new_site(theme = &quot;gcushen/hugo-academic&quot;) For this site, we’re using the Hugo theme “Academic”, specified as an argument within the blogdown::new_site() function. At the end of this lesson, you’ll have a website that looks something like what you see here! Hugo Academic There are many other themes that can be used within the blogdown package. We’ve chosen to use this one because it does a good job displaying information about your skills and qualifications as well as your projects, which will be important when you are looking for jobs. Once you run this code, an “Edit” window will pop up. Edit Window Replace the text you see in this document, with a brief introduction about yourself, something similar to what you see here. Click Save. Note that nothing here is permanent. You’ll be able to edit all of this later! Welcome text A new tab will open up in your browser with a (somewhat ugly) preview of your website. We’ll make this look better right away! First website preview Ok, so this doesn’t exactly look like that website we were looking at before…what’s going on? Well, in order for the preview to appear correctly, we have to make a slight change to how the website looks for links on the website. To do this, open up the config.toml file within the project directory. open config.toml Search for the line baseurl = \"/\". After this line, add the following two lines of code: relativeurls = true canonifyurls = true Additionally, change title to include your name. config.toml edits Save these changes. The preview of your website should update automatically in that tab that appeared previously. However, if it doesn’t, you’ll want to run the following in the R Console: ## continually show updates to site blogdown::serve_site() When blogdown::serve_site() is run, every time you save a change to the files of your website, the preview will update and you’ll be able to see what changes have been made and how they’ll appear on your website! Now that you’ve edited those two lines of code in config.toml, your preview should look something like this: website preview 85.1.3 Website Content Now that we have the skeleton of the website ready and our theme preview looks as we expected, we’re ready to start editing the content of the website to make it your own. This is where you should edit the text to match your information. Thus, where it says “Jane Doe” in this lesson, put your name in your files. When a description is included, make sure you’re writing information about yourself, and not word for word what’s in this lesson. You really want employers to know about you! 85.1.3.1 Author Details With that all settled, let’s start adding information about you to the website. To do this open the file index.md in the directory /cloud/project/content/author/admin. Now and in the future, you’ll want to edit this file so that it’s always as up-to-date as possible. In this file you’ll want to edit the following sections: * bio : include a brief 1 line about yourself. You’ll write a more complete bio later. * education : feel free to add DataTrail and include the URL to the DataTrail website: https://www.datatrail.org/. Include all of your educational experience here. * email : include your professional email here. Be sure it’s in quotes * interests : add a few interests here. They don’t have to be the same as those included here * name &amp; organizations : be sure to update your name and add any organizations you’re involved with. content/author/admin/index.md edits Once you save this file, the changes should be visible on your website preview! You can scroll down to this section or click the “Home” section on the navigation bar at top to see the changes to your website. Home preview While we’ve got that same _index.md file open, let’s just go ahead and update the rest of your Contact information now. You’ve already added your email, but you’ll want to update your Twitter, and GitHub links. Scroll down to the section labeled “social.” Change the link argument to the link to your twitter. Do the same for the github section. If you don’t have a Google Scholar account to display, just comment out those lines. Contact icon edits We only have two more things to do to finish up with this section - write a bio and add a picture. 85.1.3.2 Biography Your biography section should a little bit of information briefly describing your current positions and data science interests! To see exactly what we mean, let’s look at a few examples from the websites from people currently working in data science. Here we’re looking at the about section from Nathan Yau’s website. In his about me section, Nathan briefly explains what he does followed by where to find some of his work. He finishes with some of his interests outside of work. If you want to see more of Nathan’s work, check out flowingdata.com. Nathan Yau Here we have another example from Mona Chalabi, a journalist who generates illustrations from data. Mona introduces what she does, describes where to find her work, and then provides some background information about her professional work. Mona Chalabi Our final example comes from someone whose work you’ve seen throughout this course, David Robinson. In his about me, he mentions his current position and interests, describes briefly some of his work, and provides a bit of background. David Robinson These three examples should give you an idea of what to include in the “About Me” section of your website. Generally, consider including: current position (if applicable) background information where to find projects you’ve worked on Having looked at a few other individuals’ websites, write your biography text at the end of the _index.md file we’ve been working with so far. Bio section edits Once you save, these changes to the icons and bio displayed on your home page will be visible. Homepage icon update preview 85.1.3.3 Picture While there is currently an avatar image on your site, you want this to be a picture of you! Within the same directory (/cloud/project/content/author/admin) upload a file containing a picture of you. Upload image Once the file is uploaded, we’ll have to go specify to your website that we want to use this file, and not avatar.jpg. To do so, navigate to cloud/project/config/_default and open the params.toml file. Look for the line that says gravatar = false. Under that line add the line avatar = 'female.png', where female.png is replaced with the filename of the file you uploaded. params.toml image edits Once these changes are saved, the preview of your website should have your image on it! your image preview 85.1.3.4 Contact Information With the basic information for our website edited and a picture of ourselves now visible on the preview, it’s time to edit all our contact information. To do this, we’ll continue to make edits within the params.toml file. Update the email address to include your email address but remove the phone and address information, leaving just the empty quotation marks. Similarly, set office hours = \"\" and appointment_url = \"\". Finally, within the contact_links, comment out all the lines so that they are not displayed. If you change your mind in the future and want any of this information displayed in your contact section, you can uncomment any of these lines and update the links to your pages. edits to params.toml By editing this text, the Contact section of your website will only display a form by which someone can email you to contact you. This is sufficient for now, but you can add additional information in the future, should you need to. Once these changes are saved, your preview will be edited and you can see what your Contact page looks like. contact information edits 85.1.3.5 Website Tabs On the last preview, you see that there are currently six tabs at the top of the preview. However, we won’t need all of those tabs. We’re going to pare down these tabs to only include the most important information. But, in the future, should you want to add any of these back in, you’ll know how to do so. To start customizing these tabs, we’ll want to be sure that your most up-to-date resume is available on your website! To get started on this, you’ll need to upload your resume into the cloud/project/static directory. Here, our resume is saved as resume.pdf. resume.pdf in Files Now, we’re ready to start tweaking the tabs available on our website! To do this, we’ll still be making edits to a file we haven’t yet worked with. Open cloud/project/config/_default/menus.toml. Navigate to the portion of the file shown here. Comment out the four lines related to “Publications” and the four lines related to “Tutorials”. This will remove these links from the top of your website. The lines you should delete are highlighted here: tabs to comment out Then, make the following changes: Change the “weight” for each tab so that they match what you see in the image below (This specifies the order in which the tabs will appear on your website) remove the comments from the four lines related to your “Resume” change the name to “Resume” Specify the filename of the resume you updated. Include your filename in the url line. menus.toml tab edits Now, once these changes are saved, you’ll have five tabs on your website preview. Publications and Talks will have been removed, but Resume will have been added! tabs preview 85.1.4 Website Appearance Things are really coming along! We have all of our contact and necessary information included on our homepage now! But, there are lots of changes we will still want to make before our website is ready. Now, we’ll focus on how to improve the overall appearance of our website! 85.1.4.1 Hero Widget While the contact information looks great, there’s information at the top of our website that doesn’t need to be there (and that we’ve kind of been ignoring up to now). Unnecessary information We’ll remove this text by opening up hero.md within /cloud/project/content/home. open hero.md In this file, set active = false. hero.md edits Once the file looks as you see above, you’re ready to save your changes and preview! The blue box with information about widgets will no longer be there! hero.md preview We’ll repeat the same process for the demo widget. Open demo.md in /cloud/project/content/home. Set active = false. demo.md edits Once you save this file, you can see the changes in your preview. No more extra information at the top of your website! Things are really coming along! demo.md preview 85.1.4.2 Skills With those changes, let’s edit the skills section of the website. Navigate to /cloud/project/content/home and open skills.md This is where you highlight all of your job-pertinent skills! Here, we’re suggesting you highlight your “R”, “data visualization”, and “Data Wrangling” skills. But, you could highlight more than three skills or three different skills! Think about what skills you want employers to know you have and include them here. skills.md edits To add the three skills you see here, edit the text in the [[feature]] section of the document to include the following: [[feature]] icon = &quot;r-project&quot; icon_pack = &quot;fab&quot; name = &quot;R&quot; description = &quot;&quot; [[feature]] icon = &quot;chart-line&quot; icon_pack = &quot;fas&quot; name = &quot;Data Visualization&quot; description = &quot;&quot; [[feature]] icon = &quot;table&quot; icon_pack = &quot;fas&quot; name = &quot;Data Wrangling&quot; description = &quot;&quot; Note that the text that will be displayed on your website underneath each icon is specified in name. We’ve removed the text from the description variable for each skill, but you could choose to include text here explaining the skill. Then, the icons displayed are specified in icon and icon_pack. Which icon to display on the website is defined in the icon argument and refers to the names of the icons found at Font Awesome Brand, Font Awesome Standard, and Academic Icons. You can search here for other icons you’d like to use to highlight your skills. If you use an icon from Font Awesome Brand, you would then specify fab in the icon_pack variable. From Font Awesome Standard, you’d specify fas. And, from Academic Icons, you’d specify ai. Once these changes are saved, you’ll be able to see the edits on your website preview: skills.md preview 85.1.4.3 Removing Content While demonstrating your skills and interests are important on your website, at this point, it’s not important to include all of the sections included by default on this theme. So, at this point, we’re going to go in and turn a bunch of these sections “off.” We won’t delete the content. This way, in the future, if you’re interested in adding any of this information back in, you’ll be able to do so! We’ll remove the “Publications,” “Featured Publications,” “Talks,” “Experience”, “Accomplishments” and “Gallery” sections from your website in this section. The process will be very similar for each. First, navigate to /cloud/project/content/home and open publications.md. In this document, where you see active = true, set that to be active = false. This will remove this section from your homepage. Save these changes. Turn off Publications In that same directory, open featured.md. Set active = false and save your changes. Turn off Featured Publications In that same directory, open talks.md. Set active = false and save your changes. Turn off Talks In that same directory, open experience.md. Set active = false and save your changes. Turn off Experience In that same directory, open accomplishments.md. Set active = false and save your changes. Turn off Accomplishments Finally, to turn off Gallery go into cloud/project/content/home/gallery, open index.md. Set active = false and save your changes. Turn off Gallery Now, when you save and preview your website, you can scroll through to see that all these sections will have been removed. Your site will be minimal, but it’s a great place to start! You can always add back in any of these sections later. 85.1.5 Posts We opted not to turn off the posts section of your website. Now, we won’t specify in this lesson what to specifically include in this section, but we will encourage you to write blog posts and include them in this section. For now, we’ll delete the posts that are already in there, since they’re not your blog posts and show you how to write blog posts in the future. Posts section Navigate to /cloud/project/content/post/getting-started. In this directory, you’ll see a few files. One of them will be index.md If you were to open this file, you’d see all the contents used to write that “Getting Started” post currently on your website. getting-started.md While there’s lots of helpful information in this post, you didn’t write it, so you’ll want to delete this directory. The same goes for the jupyter directory. Select both directories and delete the contents. delete getting-started and jupyter directory Another file in there will be a file with a name similar to: 2015-07-23-r-markdown.Rmd. This will contain the text we included at the beginning of the lesson in the “Edit” box. You can leave this file for now. There will be time to edit and write new posts later! After these changes are saved, your Recent Posts section of your website should look like this: Welcome post preview The last thing we’ll note is the following. When you are ready to write a blog post, you’ll want to use the “New Post” Add-in. To find this, click on “Addins” in RStudio Cloud. Then select “New Post” from the drop-down menu. New Post This will open up a box where you’ll enter the title, author, date, and other information about your post. You can specify whether or not you want this file to be an .Rmd or a Markdown file (if you want it to include R code, choose .Rmd). After entering all the necessary information and clicking “Done,” the Add-in will create the file for you to edit it within the /cloud/project/content/post directory and name it in a consistent manner. This is where you will write your blog post! New Post Information 85.1.6 Projects Aside from including blog posts, you’ll also want to include information on projects you’ve worked on and completed in the projects section. By default, there are two projects included in /cloud/project/content/project. One is in the directory internal-project. The content for this post is included inindex.md. The second is an external-project (meaning the content is hosted elsewhere on the Internet). The content here is again specified in the index.md file within this directory. We’ll remove this content from the site for now, but it’s good to note that it’s possible to include external projects on your site. Delete both directories. delete current projects Now, we’ll create a new project using the “Addins” approach we reviewed previously for creating new posts. Click on “Addins” at the top of RStudio. From the drop-down menu, select “New Post.” In the New Post box that appears, fill out the information as you see here: New Project Be sure that “Subdirectory” is project (not post!), and select “project” from the “Archetype” menu. Specify that you want an .Rmd document. Once information is complete, click “Done.” A new file will appear within /cloud/project/content/project and this file will open up. You’ll see that it’s an empty RMarkdown document. It’s up to you to add in the text and code you used to analyze the ATUS Survey data in your final project. Rmd document The skeleton for your project will then be visible on the preview of your website in the Projects section! Projects preview But, what are those tags above your project? We’ll want to customize those as well! To do so, navigate to /cloud/project/content/home and open projects.md. Find the section of this file where you see [[content.filter_button]]. Leave the first filter alone, but edit the second filter to look for the tag “R” in your projects. The last one can be “Other” for now. Use the syntax you see here: Tags edit Note that these tags correspond to the tags specified in the YAML of your post. These filters will search for any projects with the specified tag. You can have more than three tags as you include more projects over time! Once these changes are saved, the new tags will be visible on your preview! Tags preview 85.1.6.1 Website Tailoring Okay, we’ve done a lot, but there’s one last thing we want to do. At this point, it’s more important that future employers see the projects you’ve worked on than the blog posts you’ve written. Thus, we want projects to show up before posts. To do this we’ll edit the weight argument. Return to posts.md within /cloud/project/content/home. Toward the top, change weight = 60 to weight = 65. Save these changes. Projects weight Then, within projects.md within /cloud/project/content/home, change weight = 65 to weight = 60. Save these changes. Posts weight Recent posts will now be displayed after projects on your website: website preview 85.1.7 Deployment Your website is now ready for prime time. The only problem is it’s only visible on your RStudio Cloud project. So, we have to deploy your website. There are a number of different ways to do this, but we’re going to use the workflow with which we’re most familiar: GitHub Pages. This is how your website is currently deployed. Your current website is hosted on GitHub at username.github.com and the file structure should look something like what you see here: old website GitHub repo We’re going to delete the current website and replace it with this updated website! To do so, within your current project on RStudio Cloud (where your new blogdown website files are), go to the Terminal and run the following…but replace username with your GitHub username: git clone https://github.com/username/username.github.com.git This will clone your current website repo into the RStudio Cloud project where your new website files are. cloned repo Click on the directory of the repo where your old website contents are. Then, click on More and select “Show Hidden Files” Show Hidden Files Within this directory, you’ll see two hidden files .git and .nojekyll. Do NOT delete these files, but select everything else, and delete the files from your old website. Delete most of your old website files Now, we’ll need to add all the new files! But, you should NOT add every file you just generated. You only want to add the contents of /cloud/projects/public. What’s great about blogdown is that every time blogdown::serve_site() runs and generates a new preview of your website, all the files needed to deploy your website are updated and added to /cloud/projects/public. Thus, everything you need to deploy your website is right there. Move new website files from public Move all the files from /cloud/projects/public to the repository you just cloned (username.github.com) by selecting “Move…” from the “More” drop-down menu. Select destination directory All of your new website files should be within the username.github.com directory. new files within username.github.com We’re ready to push these changes to GitHub. To do so, change username in the code below to your GitHub username: cd username.github.com git add -A git commit -m &quot;Rmd to blogdown website&quot; git push In this code, you’re changing your directory to the version controlled directory you just cloned. You’re then staging the files using git add -A, which will stage all new, modified and deleted files. You’re then committing these changes and pushing to GitHub. On GitHub, these changes will all be visible! GitHub changes And, when you go to username.github.io, you will be able to see all the changes you’ve made to make your website more professional! Awesome! blogdown website! 85.1.8 Additional Resources Blogdown Book, by Yihui Xie, Amber Thomas, Alison Presmanes Hill List of blogdown websites JaneEverydayDoe GitHub commit - end of this lesson 85.1.9 Slides and Video Making Your Website More Professional Slides "],["project-gallery.html", "Chapter 86 Project Gallery", " Chapter 86 Project Gallery When looking for a job in data science, it’s best that those interested in hiring you can get a sense for your work easily on the Internet. By displaying projects you’ve worked on on your website, hiring managers can quickly see what skills you have and what you’re interested in. Arguably, this could be seen on GitHub (we’ll get to this in the next lesson!), and hiring managers will likely look there as well; however, by writing up a short report that tells the whole story for a few of your data science projects, (including visualizations!), and including them on your website, you’re making it even easier on hiring managers to see what you can do! In this lesson, we’ll discuss two different ways of displaying your projects on your website, discuss what you’ll want to include on your website, and walk through step-by-step of turning one of the projects in this course into a project included on your website. 86.0.1 Project List The simplest way to display projects on your website is by including bullet points that link to your projects on GitHub; however, looking through multiple folders on your GitHub to figure out what you did on a project is asking a lot of someone. Thus, while it’s better than nothing to include links to your GitHub, you want to make it easier on your readers. To do this, you’ll want to turn your projects into a report or blog post that you’ll then publish on your website. By telling a story about your project, including the results in figures and tables, and making what you did in the analysis very clear, you’re demonstrating both your technical and communication skills all at once. In the last lesson on updating your website, we looked at David Robinson’s “about me” section. In this lesson, we can see that as he carries out projects, he writes blog posts and posts them to his website. David Robinson’s Projects On his website, his most recent posts are displayed, and the contents of each can be found by clicking on any of the titles. Then, visitors to his site can see the story of his analysis! Blog posts tell analytical stories By including links to summaries (blog posts!) of your work on your website, you’re helping individuals looking to learn more about your work or simply interested in your analyses 86.0.2 Project Gallery Beyond providing links to blog posts, it can sometimes be helpful to provide readers with a visual cue as to what will be included in that link. Rather than a list of your projects, including an image along with the title of the post can be very helpful to attracting readers to your work. For example, on Nathan Yau’s site, FlowingData.com, projects can be searched by topic. Then, images for each post are displayed with the title underneath the project and the first few words from the post displayed underneath the title. Projects on Nathan Yau’s website One caveat here is that on this site, Nathan Yau often links to others’ work to promote and support their work, so these aren’t all his own projects. On your website, when looking for a job, you would want to be sure to include links to your work primarily. You can then use Twitter (we’ll talk about this later!) to support and promote others’ work, at least while you’re looking for a job. Once you’re more established (like Nathan Yau), it’s OK to include links to others’ work on your website, as long as you give them credit, of course! Another great example of a project gallery can be seen on Mona Chalabi’s website. We saw her about me section in the last lesson and learned that she visualizes data using creative and artistic drawings/visualizations. This type of work perfectly lends itself to a project gallery, as the visualizations are the story. Mona Chalabi’s Project Gallery However, Mona is also a writer. Her journalism pieces are also listed on her website, in list format! Both project lists and project galleries can be effective ways to share your work on your website! Links to Mona Chalabi’s Writing 86.0.3 What To Include As you’re preparing your project gallery, you’ll want links to a few blog posts about projects you’ve worked on. These posts should display your ability to: find a dataset wrangle a dataset explore a dataset analyze a dataset visualize a dataset tell a story effective data science communication You’ll want at least a few examples of your work, meaning at least 3 different projects you’ve worked on. And, you’ll want to make sure at least one of these is a project you’ve come up with and worked on on your own – you don’t want the projects you display to all come from the projects done in this Course Set, although some of them can be! 86.0.4 What Not To Include As you’re looking for a job, it’s best to avoid criticizing the work of others in your project gallery. You can certainly use a dataset that someone else has written about previously to do your own analysis; however, your post should not tear apart what that other individual did in theirs. Additionally, remember this is a blog post. This post should tell a story and include details about your analysis, but it shouldn’t include every detail about your analysis. You should do the analysis and then pare down what you have to only what’s necessary to communicate your story to your audience. Finally, remember that this should demonstrate your skills and abilities. Do not take code or work from others without properly citing those individuals. Giving attribution to others’ work when deserved is incredibly important. Be sure that all writing and code portrayed are your own in your project gallery actually are your own work. 86.0.5 Your Final Project Now that we’ve seen a few examples of others’ projects, let’s get down to preparing one project you may want to include in your project gallery! In the last course, one of the quiz questions required you to create a presentation on Google Slides taking what you did in your Final project and turning it into a presentation that you’d present to a technical audience. Thus, you should have already thought a bit about how you would tell a story about this analysis. We’ll use that same project here, but instead of generating a Google Slides presentation, we’ll turn this project into a project that you will include on your website! The link to this page on your website will be required in the quiz for this lesson, so it’s best to follow along, create this post, and update your website as you work through this lesson! 86.0.5.1 New Post Setup In the last lesson, we set up the skeleton for a project on your website, but we didn’t include any content in that project at that time. The goal of this lesson is to fill out that section of your website a little more! We’ll start by adding your Final Project analyzing the American Time Use Survey Data for this Course Set as a project on your website here. Final Project on RStudio Cloud This write up should take what you did in your final project and turn it into a detailed but clear blog post format for inclusion on your website. To get started, return to the project on RStudio Cloud for your blogdown website. Navigate in the Files tab to /cloud/project/content/project. In here you’ll see the atus-survey.md file we created in the last lesson. atus-survey.md But, at this point, if you want to take what you did in your final project and turn it into a blog post, you probably want an R Markdown (.Rmd) file, not a Markdown (.md) file. That’s ok! We’ll walk through how to create a new post now! First, make sure that you are in /cloud/project and not your version controlled website directory before attempting to make a new post. Then, click on the “Addins” button on the menu along the top. Select “New post” from the drop-down menu. New Post In the pop-up “New Post” box that appear, enter the “Title” of your final project. For this lesson, we’ve titled this post “ATUS Survey Data”, but as we’ve discussed in previous lessons, this is not a very good title. A better title would concisely convey the findings of the analysis. Be sure that your project title is better than “ATUS Survey Data.” After deciding on a good title, include your name in the “Author” box and today’s Date in the “Date” box. Then, importantly change Subdirectory to project. This ensures that the new post goes in your project directory and not your post directory. After that, you can choose a few Categories and Tags to include on your website. Leave “Archetype” as is. The name of the file and the slug will fill in automatically. Leave those defaults alone. Finally **change the format to “R Markdown (.Rmd)”. New Post Filled In Form This will generate an R Markdown document where you’ll be able to get started! Once this information is all complete, click “Done” New RMarkdown Post 86.0.5.2 Project Content With the file ready, we’ll need to determine the general framework for this blog post. This means determining what sections to include in the project and what figures/results you’ll present to tell your story. It’s a good idea to place possible section headers in the document before you start adding content. A general framework for this project and most analyses blog posts could be the following: General Framework You could choose to use this framework or modify it to best tell the story of your analysis. But, regardless of what sections you include, you want to be sure to explain why you’re doing the analysis and what question you’ll be answering in this post in the introduction. Following this, it’s often a good idea to explain where the data came from and display the code you used to get the data into RStudio Cloud. The Exploratory Data Analysis section should explain how you wrangled the data and maybe show an explanatory plot or two. How you analyzed the data should be explained briefly after the data are explained. The Results section is the most important - this should summarize your findings, including figures and tables to guide the reader and help them understand your results. The sub-headers within the results section should tell the reader what your results were. Finally, a conclusion should pull everything together. Within your R Markdown document, this framework would be entered as follows: Framework in R Markdown Before we get to adding content, a reminder that you can run blogdown::serve_site() at any point in time to preview the changes to your site that you’ve made. After saving those changes to the R Markdown file, the homepage would look as follows. Post on Homepage If you were to click on this post, you would be able to preview the skeleton of your project that you just made! Project Preview We’ll get to filling in the content for the actual post; however, let’s take a step back to make the project on the homepage look more like the skeleton post we created before. We want to add a picture to the preview on the homepage, include a caption on the homepage post, and include a picture on the post itself. These changes will all be made in the YAML at the top of your R Markdown document. YAML edits summary: &#39;Analysis of 2016 American Time Use Survey (ATUS) Data&#39; image_preview: &#39;bubbles.jpg&#39; header: image: &#39;bubbles.jpg&#39; We’ll explain the YAML edits here: * summary: the caption text on the post on your homepage * image_preview: the image included with your post on your homepage * header: image: the image included in the header of your project post itself. There are a number of other appearance edits you can make in the YAML of your R Markdown document, but these are all we’ll cover for now. We’re including the default bubbles.jpg image for now, but for each project you should change this to an image that makes sense for that project. The image you want to include should be stored in /cloud/project/static/img. where to save images Now when you preview your projects on your homepage, the project looks just like the skeleton post we created previously! Projects updates preview At this point, we can delete the atus-survey.md project file because we’re going to include just the .Rmd file on our website. Delete atus-survey.md 86.0.5.3 Content With the skeleton for your post in your R Markdown document ready and the appearance on your website ready to go, it’s time to include text and code throughout the post. For example, in the data section you’d explain where the data came from, describe how many observations are in the data and what variables you’re most interested in. Data section You’d also include a code chunk that would read the data into R. Note that you’ll likely want to include the data in the project website directory (/cloud/project/content/project). You can create subdirectories within this directory (for example: /cloud/project/content/project/atus_survey) to help keep things organized. If we were to save and preview this file at this point on your website, it would be pretty bare bones, but you would be able to see all the changes you’ve made. Note that you do NOT have to Knit this file, as you would normally. blogdown does that automatically for you when you preview the site! Skeleton with data section started And, as you include more content for what you did in this analysis, it will become a full-fledged post. Be sure that there is text accompanying all code, figures and table, to explain to your readers what is being done throughout the project. Adapt the text and code from your projects into this document by using the code you’ve already written but adding text to explain to a reader what you’ve done. You can copy sentences and code chunks, but do not simply copy everything currently in your project .Rmd document because there is a lot of explanatory text explaining to you what to do in the project. The goal of this post is to tell a story about your analysis. Make sure all text included in this document and every code chunk helps tell that story. Before finishing change the picture included in the header and on your homepage preview to something that matches the project. It could be a figure included in your analysis or a photo of yours (or that is freely-available) that has something to do with your analysis. Make sure that this photo is uploaded into /cloud/project/static/img 86.0.5.4 Proofread After you’re happy with your post, it’s always a good idea to proofread your writing for typos and errors. 86.0.6 Push to GitHub Once you’re happy with these changes, remember that in order for them to be seen on your website, all the contents of your /cloud/project/public/ directory have to be added to your website directory. To do this, first delete all of the current files (except for the hidden files!) in /cloud/project/username.github.com. Delete old website files Then, select all the files in cloud/project/public. Click “More” and select “Move” from the drop down menu. Move files Select the Folder for your website in the pop-up “Choose Folder” window. Click “Choose.” Select website You’re now ready to push all your changes to GitHub. Make sure you’re in the correct version-controlled directory (/cloud/project/username.github.com/) and then add, commit, and push your changes: cd /cloud/project/username.github.com/ git add . git commit -m &quot;add projects tab&quot; git push Your changes will now be viewable on your website! Changes to website You’ll also want to create projects or posts for your Data Tidying and Data Visualization Projects on your website. That won’t be required of this lesson, but it is a good idea to make sure that your work is represented clearly on your website 86.0.7 Your Own Project So far in this course, you’ve been told what data to work with, how to explore it, and what questions to answer. However, when looking for a job, hiring managers will want to see that you have completed projects on your own. At this point, it’s your job to use all the skills you’ve learned throughout this course to add a project that is all your own to your website. To be considered complete, this project must demonstrate your ability to: Form a data science question Get data Clean the data Plot data Get stats Report results Carry out a data science project on your own about a topic you care about or find interesting! Then add this project to your website. Push the changes to GitHub and include the link to your project in the quiz below. 86.0.8 Slides and Video Project Gallery Slides "],["improving-your-github-profile.html", "Chapter 87 Improving Your GitHub Profile", " Chapter 87 Improving Your GitHub Profile After discussing the required application materials, such as a resume and cover letter, and your personal website and the projects you’ve worked on, the next step in getting ready to apply to jobs is to be sure that your professional social media presence is complete and up-to-date. While this may not be a requirement of a job application, it is likely that your social media presence will be looked at during the interview process. Thus, while not required, it is incredibly important. In this and the following few lessons in this course, we’ll walk through the necessary steps to get each of your professional social media platforms up-to-date. In this lesson, our focus will be on ensuring your GitHub presence and profile is ready for the job market. 87.0.1 Profile For each profile, it’s a good idea to include a picture of yourself along with your profile. This does not have to be a glamour shot or professional photo, but it will ideally be a photo of you (without anyone else in the photo) with a simple and ideally solid color background. In fact, the same photo can be used across platforms. For example, if you have a photo on GitHub, it can be helpful to use that same photo on LinkedIn and Twitter, so that people can be sure that the accounts all belong to the same individual - you! Note that if you are not comfortable sharing an image of yourself on the Internet, it can be helpful to add some recognizable image to each of your social media accounts for the aforementioned reason. This could be a picture you’ve taken of a location or of your pet, or something of that nature. You’ll also want to add a brief description to your GitHub profile and a link to your website. To make all of these changes to your GitHub profile, first login to GitHub. Helpfully, if you haven’t completed your profile yet, GitHub will likely prompt you to add profile information. Below, your username on GitHub, you’ll want to first click, “Edit profile.” Edit Profile You’ll want to fill in all the following fields with accurate information: Public profile Name - at least your first name. Include your last name too if you’re comfortable doing so. Bio - a brief bio about your interests and skills URL - the link to your website You can additionally include a public email address, a Company, and your Location, if you’re comfortable doing so. Know that this information will be shared with GitHub and anyone who looks at your GitHub page. This information can be updated or deleted at any point in time. Once you’ve filled in these fields, click on “Upload new picture” at the right-hand side of the screen. Using the window that pops up, navigate to the location of the photo you’d like to use as your profile picture and click “Open.” Adjust the box to position and crop the photo as you desire and click “Set new profile picture.” The new picture will appear on your Public Profile. Upload new picture Once you’ve filled in the necessary information and added the photo you’d like to use, click “Update profile.” Your profile will now be up to date! To see an example of a complete GitHub profile, we’ll take a look at Jenny Bryan’s GitHub: Jenny Bryan’s GitHub Profile Helpfully, Jenny Bryan has included a picture, her name, a brief bio that let’s us know where she works, a link to her website, and her location. She’s also specified six pinned repositories. 87.0.2 Pinned Repos In addition to updating your personal information, you also have the opportunity to choose which repositories are displayed on your personal GitHub page. GitHub will choose six repos to display; however, you can customize which repos are shown here by clicking “Customize your pinned repositories.” Customize your pinned repositories A new box will appear and you’ll have the opportunity to select six repos to remain pinned on your GitHub profile. You’ll want to have the six projects your most proud, that are the most interesting, or that demonstrate your skills best appear. Once you’ve checked six boxes, click “Save pinned repositories.” Save pinned repositories. These repos will now be displayed on your profile. Pinned Repositories 87.0.3 GitHub Contributions When applying to jobs, future co-workers and hiring mangers will almost certainly take a look at your GitHub to get a sense for your programming skills. Additionally, they will also often look to see what type of community member you are. Data scientists are often members of teams, and this means that hiring managers will look for individuals who work well with and are willing to help others. One way to demonstrate this ability is by contributing to others work on GitHub. To get started being a good community member, there are two ways to contribute early on: submitting PRs (pull requests) that fix typos submitting PRs that address issues on others’ repos 87.0.3.1 Fixing Typos Unlike sending an email letting the developer know about a typo and the developer then having to go in and make the edit themselves, the great thing about pull requests on GitHub is that you do the work (fix the typo) and then the developer (the person whose GitHub repo it is) simply has to accept the pull request for the changes to be made. The amount of work on the developer is minimal, but your contribution is important and helps the community by making documentation clearer! Thus, if you see a typo in someone else’s documentation or README file, feel free to edit the typo and submit a pull request! 87.0.3.2 Addressing GitHub Issues Further, for R packages that are hosted on GitHub, there are often many issues that users and other developers submit to the repository. It can be a lot on developers to keep up with these issues. Fortunately, you can take a look at the issues on packages you use to see if there are any issues you can help with. If you figure out how to address the issue, you can again submit a PR to the developers and they can determine if they want to accept your PR. Specifically, many packages also include tags within issues stating where they feel help is wanted on an issue (using the tag “help wanted”) or where developers feel there’s an opportunity for individuals with less experience to contribute (using the tag “good first issue”). ggplot2 Open Issues For example, let’s take a look at ggplot2. On the day this lesson was written, there were 91 open issues. If you were to click on the “good first issue” tag at the top of this page, GitHub would filter to include issues that were specified to be “good first issues” for less experienced developers. You could look through these issues and see if you could contribute. If you could, you could submit a PR to the developers! ggplot2 open issues and “good first issue” Note that addressing issues on GitHub is not required. You should not accept unhelpful pull requests just for the sake of submitting pull requests. Only submit a pull request that solves an issue. In fact, ggplot2 and many other packages have contributing guidelines to help define when and how to best make a contribution to the package’s development. Read these guidelines now to get a sense of when it’s best and how to make a contribution to someone else’s package. 87.0.3.3 StackOverflow Finally, answering others’ question on StackOverflow in a helpful manner is a good way to help the R community. While some hiring managers may not look there, others do. Thus, it’s always good to have a reputation as a team player online. Again, only answer questions where you actually answer the other individual’s question in a helpful, complete, and kind manner. Your reputation on this platform will be assessed by hiring managers and you never want to look like a jerk. 87.0.4 Summary In this lesson we discussed the importance of having a complete GitHub profile and how to be a helpful and productive community member online by fixing typos on others’ repositories, addressing GitHub issues, and answering questions on StackOverflow. 87.0.5 Additional Resources You Do Not Need to Tell Me I Have a Typo in My Documentation, by Yihui Xie Contributing to ggplot2 development "],["improving-your-linkedin-profile.html", "Chapter 88 Improving Your LinkedIn Profile", " Chapter 88 Improving Your LinkedIn Profile Having a LinkedIn profile is important because it allows recruiters and those responsible for hiring to find you. Recruiters who pay to use LinkedIn can search by terms (such as “data scientist” or “R programmer”) and will be able to identify you as a possible job candidate! Additionally, based on your profile, LinkedIn will be able to suggest possible jobs that may be of interest given the information you’ve provided! That said, it’s best to think of your LinkedIn Profile as a more-detailed resume. Thus, every job and skill that is mentioned on your resume should also be on your LinkedIn Profile. However, on your LinkedIn profile, relative to your resume, the amount of information you can include about each position and the projects you completed at each place can include more detail. You should have created a LinkedIn profile in the first section of this course; however, if you haven’t you’ll need to do that before carrying on with this lesson. Similar to your GitHub profile, it’s important to include a professional-looking picture of yourself on your profile (the same one as your GitHub profile works!), contact information, a link to your website, a brief bio, and details about your experience, education, and skills on LinkedIn. We’ll walk through the steps to update your LinkedIn profile in this lesson. 88.0.1 LinkedIn Profile Once you log in to LinkedIn using the account we previously created (in a lesson in an earlier lesson), you’ll be ready to improve upon your current profile. We didn’t previously fill in any information, so there will be a lot to do here! Note that you may be asked after logging in, “What are you most interested in?” Feel free to click on finding a job at a later point in time. However, for now, we’ll click “Not now” to continue on updating our profile. Not Now Navigate to your profile, using the icon at the top right-hand of the screen Go to Profile 88.0.1.1 Picture The first thing we’ll do is update the profile picture on your page. To do so, click on the camera icon. Upload Photo Navigate to where the picture you’ve used for your website and GitHub are located on your computer and click “Open.” A Screen will appear with your photo in it. Adjust the size and position of the circle to best capture you in the photo you’ve uploaded. You have the option to zoom, straighten, adjust the brightness, crop, filter, and adjust this photo. Additionally, you can control who sees this photo by clicking on the “Visibility” icon. Once you’re happy with your profile picture, click “Apply.” Apply 88.0.1.2 Biography After uploading your photo, you’ll be brought to a new screen asking you to Edit your intro. This is the information that individuals will first see when they come to your LinkedIn Page. Empty edit intro Ensure that your First and Last Name are correct. Then, include a Headline. This should very briefly explain your professional skills and interests. For example, Renee Teate, a data scientist, uses the headline “Creative data scientist with strong communication skills” and then lists a number of her strengths underneath that headlines. This quickly gives you an idea of what her strengths and interests are, which is great! Renee Teate’s LinkedIn Headline Be sure that your headline quickly, concisely, and accurately represents your skillset as a data scientist and enter it into the “Headline Box.” Once you’ve updated your photo and Headline, click “Save.” Save Headline You’ll now have the opportunity to update more information about yourself. Be sure that the “Country/Region is correct.” Add a ZIP code if you’re comfortable doing so (this is not required). Then, choose an industry that best matches your skill set from the drop-down menu. “Computer Software” may be the best-fit, depending on your interests and background. Select Industry Finally, include a longer Summary about your interests and skills. This should concisely describe to recruiters what you know and what you’re interested in doing as a data scientist! 88.0.1.3 Links Out On this same screen, you’ll want to be sure to include links to your other professional sites. To do so, click on the pencil icon to the right of the “Contact Info” line. Edit Contact Info On this new screen, no information is required; however, you’ll want to click on “Add website”. Add website Add the URL to your professional website here. Then, click “Apply.” Apply This will bring you back to the “Edit intro” screen. Save We’ll click “Save” on this screen to see the changes to our profile before moving on to include even more information. Updated Profile Your headline and photo will now be visible on your profile. And, if you click “See contact info”, your website will now be there. 88.0.1.4 Education, Experience, and Skills Now that you have a few updates, we’ll discuss what other information should be included on your LinkedIn profile. You’ll definitely want to update your profile to include information about your education, experience, and skills. The information included in these sections should largely recapitulate your resume; however, more detail can be included for each entry on LinkedIn than would be included on your resume. That said, keep it reasonable – humans have short attention spans. Be sure that any information you’re putting down is important or would be important to a potential employer. Better to be short and clear than long and rambling. We’ll return again to Renee Teate’s profile. Her experience entry for her current position as a data scientist at HelioCampus includes her job title, when she started the position, where she works, and what her responsibilities in this job are. Data Scientist Experience To fill out your job experience, click on “Add profile section” and click on “Work experience” from the drop-down menu. Work experience Fill out the information on the “Add experience” window and then click “Save.” As you fill out any experience, be sure to include all relevant jobs and experience you have. Add experience You’ll want to use the same process to update your educational attainments. Click on “Add profile section” but select “Education” this time from the drop-down menu. Education Enter your information into the “Add education” menu. Once complete, click Save. Add education Finally, you’ll want to include skills you have so that others looking for these skills will be able to find you. Click on the “Add profile section” menu and select skills from the drop-down menu. Skills Select the skills you have. As you select them, they will show up with check marks below the search bar. Once you’ve added all relevant skills, click “Add” to add them to your profile. Add Skills Feel free to work through and include more information on your LinkedIn (Volunteer work, Accomplishments, Courses, Certificates, etc.) The more information you provide, the more helpful your profile will become! However, at the bare minimum, be sure you have listed your Educational background, your Experience, and your Skills. 88.0.2 Connecting to Others Networking is an important concept on LinkedIn. By connecting to others in your field, you start to generate a professional network. And, while there is debate about with whom you should connect on LinkedIn, with some people arguing connect to as many people as possible and others saying only to connect with those you know, we’ll argue here that you should start off by only linking with others you know. Maybe you’ve chatted with someone on Twitter or met someone at a conference - these are people to connect with at the beginning. Over time you can then decide how wide you want to cast your LinkedIn net. To connect with someone, search for their name in the search bar. Here, we’ve searched for Jeff Leek, one of the authors of this course. To connect with him, you would click on the “Connect” button to the right of his name. Connect An invitation to connect with Jeff Leek will be sent to him to verify. Once he verifies the connection, you will be connected on LinkedIn. Click “Done” on the pop-up box. LinkedIn will also suggest people you may know. If you have some professional relationship with them at this point, feel free to Connect with them. Hold off on connecting with everyone you see here until you have a better understanding of how LinkedIn will work best for you. 88.0.3 Summary In this lesson we’ve discussed how to use LinkedIn to expand your professional network, improve your online presence, provide information that’s on your resume online, and enable you to be searched by hiring managers and recruiters. It’s essential to keep this up-to-date overtime, especially whenever you’re looking for a job. 88.0.4 Additional Resources Renee Teate’s LinkedIn 88.0.5 Slides and Video Improving Your LinkedIn Profile Slides "],["using-twitter-for-data-science.html", "Chapter 89 Using Twitter for Data Science", " Chapter 89 Using Twitter for Data Science In an earlier chapter, we had you create a professional Twitter account. This is because Twitter is a very active place among data scientists. As a result, Twitter can be a great place to learn about new topics and to meet others in you field. Additionally, people often tweet about open positions at their companies, so Twitter can be a really great place both when you’re looking to learn more and improve your skills and when you’re looking for a job! In this lesson we’ll briefly discuss customizing your Twitter profile and then spend a fair amount of time delving into how to use Twitter effectively as a data scientist. 89.0.1 Profile As with every other platform we’ve discussed so far, you’ll want to add a photo and update your Twitter profile. Your Twitter profile will contain less information than the other profiles we’ve discussed thus far, so we’ll discuss all necessary steps of how to do so right away. To update your photo, log into your Twitter account. If you haven’t yet uploaded a photo, you can do so, by clicking on the camera icon and clicking on “Upload photo” from the drop-down menu. Upload photo Navigate to the file you want to upload on your computer. In the window that appears, position and size your photo. When you’re satisfied, click “Apply.” Apply From here, click on the menu at the top right-hand of the screen and click “Profile” to navigate to your Twitter Profile. Profile Here, at the right, you can click on “Edit profile.” Edit profile This will open up a box where you can write a short bio about yourself. This should just be a few words about your interests! Here, we’ve included a few Twitter bios as examples. (BecomingDataSci?)(https://twitter.com/BecomingDataSci?lang=en) is the Twitter account for the same person whose LinkedIn profile we looked at in the LinkedIn lesson. The other two are from two of the authors of this course. You’ll note that they aren’t very long and just briefly describe the Twitter user. Note that these can be edited at any time and will change over time. Example bios You’ll also want to include a link to your personal website in the “Website” box. Type the URL of your website there. On these same three profiles whose bios we just looked at, you’ll see that each has also helpfully provided their website URL within their individual Twitter profiles. Website links If you want to share your location, you can also include that under “Location,” but this is not required. Once you’ve completed all the information you’d like your profile to include, click “Save changes” on the right-hand side. Save changes 89.0.2 Who &amp; What To Follow Once you’ve got an updated profile, you’re ready to start following people. It’s best to keep this Twitter account as professional as possible. So, you may not want to follow your personal friends, musicians, actors, or other celebrities on this account. Rather, it’s best to follow those who post information that will be helpful to you as a data scientist. Unlike LinkedIn where it may be best to limit who you follow right off the bat, it’s ok to follow people you don’t yet know on Twitter. Your goal should be to follow people you think 1) are interesting, 2) can teach you something, and/or 3) may post about things that would help you in your career! As you search for individuals to follow, you come across someone you think you’d like to hear from, you should click the “Follow” button on their profile. Follow While you are not required to follow anyone, as someone new to the field of data science and who uses R, we’re providing a list of people you may want to follow: Suggested people to follow on Twitter It can also be helpful to look through who these people follow to find other people you may be interested in following. Finally, you can also search for people to follow using common hashtags, such as #rstats or #r4ds. These two will put you in contact with a crowd that codes primarily in R. However, #datascience and #dataviz will connect you with individuals interested in data science and data visualization generally. You can follow and unfollow people as you find helpful, so you aren’t locked into following anyone forever. 89.0.3 Tweeting After you decide on who to follow, you’ll want to check Twitter every so often to see what everyone has to say – this can be a really great way to keep up with the newest data science news and to learn a lot! As you begin to follow along and generate your own tweets, there are a few general guidelines that will make your Twitter experience a positive one: keep it positive avoid fights support others It’s incredibly easy to allow discussions on the Internet to take a negative turn. But, avoid that instinct. Keep what you put on Twitter positive and avoid delving into Twitter feuds. Disagreements happen, but you should never make personal attacks or be unkind or rude to others on Twitter, regardless of what they do. Also, Twitter can be an amazing and supportive place. So, try to promote your own work and support the work of others from your account. We’ll discuss the details of this below! 89.0.3.1 Tweeting for Self-Promotion When tweeting from your professional account, it is a great idea to tweet about work that you’ve done! Tweet about your projects, cool findings, or awesome visualizations. Include pictures of your awesome graphs, images from your blog post, and links to your work! When possible, include images that are compelling. These can help people visually remember your work! One example of this is shown here. Jeff Leek wrote a book about How to be a Modern Scientist. This book includes information about how to use Twitter as a professional, so feel free to check it out for more details. But, as for the tweet, Jeff included an image, linked to where the content is, and stated that he released the book. What a great and informative tweet about his work! Self-promotion 89.0.3.2 Tweeting to Promote Others You don’t need your own, original content to build up a following on Twitter. Twitter can be a great place to promote others’ work. You can gain followers by promoting others, being funny, being interesting, and curating others work effectively. By tweeting about others’ work, you’ll have a record of all the things you find interesting, will be able to be a good community member (by promoting others’ work), and will be able to add your thoughts about their work. Twitter is great b/c you can share others’ work by adding your own commentary in your tweet! In your tweet, you should always always cite the individual and link to their work. For example, here we see a case where Jeff tweets about a talk that (CMU_Stats?)(https://twitter.com/CMU_Stats) was giving at a conference. He explains what was being discussed, includes (CMU_Stats?)(https://twitter.com/CMU_Stats)’ Twitter handle, and includes a picture! Promoting others Additionally, Jeff uses the hashtag #JSM2018. This allows individuals following this hashtag on Twitter to easily find the tweet. Using helpful hashtags are another great way for others to find your twitter and follow you, even when they don’t know you personally! 89.0.3.3 How To Tweet When you’re ready to start tweeting, you’ll want to click the blue “Tweet” button from your profile. Tweet In the box that appears, you’ll compose your tweet. You can add emojis, photos, GIFs, polls, and location to your tweets. The character limit on tweets is now 280 characters. This limit forces you to ensure your tweets are concise. It’s good practice to convey a message clearly in a few short words. Compose new Tweet As you generate tweets, you may have individuals comment on your tweets. Twitter can be a great place for quick discussions! However, here, we’ll leave you with a reminder to keep your tweets professional from this account. Use your data science Twitter account as a professional social media account, which means that you never want to tweet anything inappropriate, rude, or unprofessional. 89.0.4 Summary In this lesson we discussed why Twitter can be a great place to learn more about data science quickly, how to use the platform to grow your network while having a positive experience, how to tweet for self-promotion, and how to tweet about others’ work. 89.0.5 Additional Resources How to be a modern scientist, by Jeff Leek Phrasing: Communicating data science through tweets, gifs, and classic misdirection video, by [Mara Averick] "],["data-science-job-descriptions.html", "Chapter 90 Data Science Job Descriptions", " Chapter 90 Data Science Job Descriptions In this lesson we’ll discuss what data scientists do, what skills companies expect entry-level data scientists to have, and walk through a few data scientist job descriptions. 90.0.1 What Data Scientists Do Very generally, it’s expected that data scientists know how to interpret and extract meaning from data. But, in practice, what does this actually mean? Well, at a company that sells a product on the Internet, it could the job of a data scientist to figure out what changes to the website caused increases in sales. Or at a company that hires many individuals it could be a data scientist’s job to determine what characteristics of their best hire shave ultimately lead to the biggest increase in revenue for that company. In fact, at Airbnb, they have three different tracks for data scientists to cater to individuals’ strengths and the company’s needs. While the problems differ from one company to the next regardless of location, data scientists work with data to solve problems. 90.0.2 Skills for entry-level positions We’ll look at examples of job descriptions for data scientist positions below; however, before we get there, we’ll discuss the three general categories of skills that entry-level applicants are expected to have. Note that having all of these skills would be ideal, but not every candidate will be an expert in each and every one. So, on your application, play up your strengths and in your free time be sure to improve upon the skills where you’re less-expert to make your application even stronger over time! Companies are looking for data scientists to have strong technical skills, that will be good employees, and who have business skills. We’ll walk through what skills fall under each of these categories in this section, but if you’re interested to see others’ thoughts on this, feel free to check out this thread on Quora. 90.0.2.1 Technical Skills No data scientist will be hired without technical skills. Thus, it’s important to be technically skilled before applying for jobs. The most-common required skills are the following: Basic Programming Languages : R, Python, and SQL are three of the most common programming languages used by data scientists. Data Wrangling : Data Scientists spend a great deal of time getting data into a usable format, cleaning the data, and ensuring that the data they have are the data they need. This skill is essential for all data scientists. Data Visualization : Visualizing data effectively for communication is an important skill for data scientists. Statistics : Having a basic understanding of statistics is often expected of data scientists. Machine Learning : Machine Learning is frequently used for predictive analyses. Having a basic understanding of machine learning and familiarizing yourself with various approaches will help you get a job. To demonstrate that you have these technical skills, you’ll want to include them on your resume. However, you should not list every machine learning algorithm you’ve ever heard of. Anyone can Google and list a bunch of algorithms. Instead, simply include “machine learning” as a skill on your resume if it’s one you possess. Then, let your projects and experience do the explaining. By this we mean, the projects you’ve worked on and experience you’ve listed on your resume should demonstrate how you’ve used machine learning to solve a problem. Additionally, having a Project Gallery and GitHub here are critical. The company can go to your GitHub and/or website to see how you’ve applied your technical skills to answer interesting questions. 90.0.2.2 Work Principles In addition to technical skills, companies will be looking for data scientists who will make good employees. Thus, there are a number of work principles that hiring managers will be looking for. Good data scientists are dedicated to their work and determined to succeed. No company wants to hire someone, train them, and then lose them right away. As such, companies will be looking for individuals who are dedicated to their job. Good employees are also dependable. The company must be able to count on you showing up and completing assigned projects. Finally, data scientists in particular are always learning. Technologies change. Data Formats are altered. New approaches are developed. Thus, data scientists must demonstrate that they are able to learn new things and are adaptable. Being set in one’s ways is not really an option for a data scientist, as the field is always changing and constantly moving forward. It makes for an exciting job, but is not for individuals who struggle to adapt. These principles should not just be listed on your resume. Again, anyone can write these words in a list. Rather, these qualities should be demonstrated on your resume through your experience and come through in the stories you tell in your cover letter. Examples are the way to demonstrate that you’ll be a good employee. 90.0.2.3 Business Skills Finally, data scientists very rarely work in isolation. Instead, they’re part of a team. This means that as a data scientist you’ll likely be working with other data scientists and other individuals at the company. Thus, it’s incredibly important that data scientists possess a number of business skills: effective communicators problem solvers knowledgeable curious &amp; interested It’s expected that data scientists be able to talk with others to help hone the question that they’re trying to ask and to effectively communicate the results of their analyses. Similarly, to do the job of a data scientist, it requires one to solve problems in creative and new ways. Being a problem solver who is knowledgeable about how to work with data and the data the company has is an important skill. Finally, it will be expected that you’re curious to learn more and interested in what you’re doing. These qualities can also come through on your resume, website, and cover letter through examples. Additionally, in an interview (which we’ll discuss in a later lesson), it’s important that those interviewing you see that you possess these qualities. Data Science Skills 90.0.3 Job Descriptions Now that we’ve discussed the general qualities and skills expected of data scientists, let’s get down to actually discussing what you’ll see in data science job descriptions. 90.0.3.1 General Job Description In the following lesson, we’ll discuss where to find open jobs, but for now we’ll just make sure we’re all on the same page about what you will generally find in job descriptions! Typically, a job description will have a few components: Logo - the company’s logo will typically be at the top of the job description Job Title - the job title of the open position Location - where this job is located Introduction - Brief introduction to the company and position Job Description - this section will describe the responsibilities of the posted position as well as some specifics on the required and expected skills a successful applicant would possess Job Qualifications - this section will bullet point out the educational, knowledge, abilities, skills, and experience requirements they’re looking for in the individual who fills this position Preferred Qualifications - Sometimes, a company will choose to include some optional skills or abilities they’d like from a candidate, but that aren’t required. General Job Description Now that we have a general understanding of what information is included in a job description, let’s walk through five actual job postings from Airbnb, Claire’s, and Allstate to see what information each company is looking for in a “Data Scientist” 90.0.3.2 Data Science at Airbnb At Airbnb, there are three tracks for data science positions. The Analytics Track, the Algorithms Track, and the Inference Track. Very briefly here, data scientists on the: analytics track focus on monitoring metrics of the company, building tools, and helping the company make data-driven decisions. algorithms track are responsible for building and interpreting algorithms that will help to power data products at the company inference track use statistical techniques to determine causal relationships For each of these three categories, we’ll first walk through a job description from Airbnb. We’ll break the description down by what technical, employee, and business skills they company is looking for in each position. After the Airbnb examples, we’ll walk through two more examples of a data science job descriptions from other companies and do the same. This way, you’ll have a fairly complete picture of what job descriptions look like and what skills companies are looking for from their data scientists. The way we’ll break down each post is by first highlighting what technical skills the company is looking for in green. We’ll then highlight the type of employee they’re looking for in purple. Finally, we’ll highlight the business skills they’re seeking in orange. 90.0.3.3 Airbnb: Analytics At Airbnb, data scientists on the analytics team have a number of responsibilities and are particularly skilled at asking really good questions, automating analyses and visualizing data. Data scientists at Airbnb on the analytics team are responsible for providing the company with recommendations that drive changes to how things at Airbnb are done. Here we have an excerpt from a job posting at Airbnb for a data science - analytics position: Analytics As mentioned earlier, data scientist positions will always require technical skills of the applicant. In this job posting we see a number of technical skill requirements, including programming, statistics, and data visualization skills. Technical Skills - Analytics Additionally, Airbnb expects that the individuals they hire will be good employees. As such, they have to take ownership of their work, be able to strategize with others, and be open to learning new things. Work Principles - Analytics Finally, there are a number of business skills that a data scientist must possess. In this job description we see that Airbnb’s data science analytics team really prioritizes the ability to communicate their findings effectively. Business skills - Analytics 90.0.3.4 Airbnb: Algorithms Data scientists on the algorithms track are responsible for understanding and working with different types of data, developing machine learning algorithms, and both producing and managing data products to help the company’s users’ experiences. In their job description they explain a bit about the position and then include a number of sample projects to give prospective applicants the type of work they would do in this position as well as list a number of qualifications of the position. Among these are a number of technical skills. For this position, applicants are expected to have a deep understanding of machine learning, statistics, and programming at scale. The strongest candidates will also have experience in natural language processing (NLP). Technical Skills - Algorithms This particular job description explicitly states fewer work principles than the analyst job description, but does state that the ideal candidates will be team players and open to leadership roles. That said, while not explicitly stated, it’s still implied that anyone they hire will be dedicated, dependable, and interested in their work. Work Principles - Algorithms Finally, the successful applicant will be able to take a project, manage it, and move it from start to finish. This is an important quality and expected skill of data scientist on the algorithm track at Airbnb. Business Skills - Algorithms 90.0.3.5 Airbnb: Inference Data scientists on the inference track are responsible for generating hypotheses, carrying out experiments that will help determine causality, and refine strategies to drive decisions within the company. For this position (as with the last two positions), the job description states a number of desired technical skills, including programming, data wrangling and statistical knowledge. Technical Skills - Inference Additionally, this job description states that successful applicants will be focused, detail-oriented, team players who are passionate about their work. Work Principles - Inference Finally, the job description explicitly states that written and verbal communication skills are valued for individuals in this position. Business Skills - Inference 90.0.3.6 Airbnb job descriptions Across all three job descriptions, there are technical skills, work principles, and business skills stated directly in the job description. While the technical skills vary from one position to the next, the expectations of good employees, and business skills expected from employees overlap a great deal across positions. It’s expected that all data scientists and Airbnb will be accountable for their work and able to take a project from start to finish successfully. These qualities help make individuals good employees. Additionally, on the business front, communication skills, the ability to work on teams, and being a successful problem solver are expectations of all data scientists. 90.0.3.7 Claire’s However, data scientists are being hired at companies across the world. Airbnb is certainly not the only company. Thus, here we’ll look at a job description for a “Junior Data Scientist” at Claire’s. For this position, the job description starts by explaining that the person in this position is responsible for using data to make decisions on “promotions, space planning, competitive landscape and ad hoc analyses.” The description then lists “Responsibilities”, “Qualifications + Experience”, and “Preferred Qualifications.” Among these are a number of required technical skills, including programming, statistics, and data visualization. Technical Skills - Claire’s Beyond technical skills, this position is looking for an individual who is a strong team member and who is willing to teach others. Work Principles - Claire’s Finally, this data scientist would be able to communicate their findings via reports that could be understood by partners and business users. Business Skills - Claire’s Here, while a different company than Airbnb, in this data scientist job posting at Claire’s, we can see that technical skills, work principles, and business skills are all still required for this position. 90.0.3.8 Allstate For one final example, we’ll look at a job posting from Allstate. The job posting for this “Data Scientist” position begins by explaining the benefits of working in the insurance field and describes the impact this individual would have. After this, the key responsibilities and job qualifications are listed. Among these, many technical skills are required, including statistical and machine learning knowledge and experience, programming skills, and the ability to work with large datasets. Technical Skills - Allstate For work principles, this job description is looking for an individual will to teach others, to be flexible in and capable of learning new technologies, and to be dependable. Work Principles - Allstate Finally, Allstate is looking for someone who is able to communicate written and oral data-centric information effectively and to present well to others. Business Skills - Allstate 90.0.4 Which Job is Right For You? Of the job descriptions above, which job is right for you? Well, at this point, if your skills are limited to what’s been taught in this course, you aren’t yet fully-versed in machine learning. Thus, the analytics position at Airbnb or the junior data scientist position at Claire’s may be the best fits. As you continue your education and garner more experience in machine learning or artificial intelligence, you can work toward acquiring the skills required by the other data scientist positions. Thus, looking for jobs now (which we’ll discuss in detail in the next lesson!) that fit your current qualifications will likely mean looking for “data analytics” or “data science” positions that don’t focus on machine learning, but more work can help you move into those other positions! 90.0.5 Summary In this lesson we’ve discussed the basic parts of job descriptions and walked through five actual data science job descriptions. We’ve demonstrated that having technical skills, adhering to important work principles, and having solid business skills (such as being an effective communicator) are required for every position. The details of which technical skills are required differs from one position to the next, so it’s ideal to apply to positions that best fit your current skill set. Then, with experience and more training, you’ll increase your skills and open the doors to additional positions. Summary 90.0.6 Additional Resources 7 Types of Data Science Job Profiles What Do Companies Look for In Entry-Level Data Scientists /Analysts? One Data Science Job Doesn’t Fit All What Data Scientists Really Do "],["where-to-look-for-data-science-jobs.html", "Chapter 91 Where to Look for Data Science Jobs", " Chapter 91 Where to Look for Data Science Jobs Now that we’ve discussed what you’ll need to apply to jobs, including a resume and cover letter, what steps to take to update your professional social media presence, including an updated website, GitHub, LinkedIn and active Twitter, and what a job description looks like, we’re ready to discuss where you actually find data science jobs. There are a number of locations where jobs are posted. We’ll walk through them now and discuss the strengths of each platform. We’ll finish out this lesson discussing the steps to take when applying to a position. These will vary from one job to the next, so we won’t discuss specifics in this process, but we’ll walk you through generally. 91.0.1 What Jobs to Look For We mentioned this briefly in the last lesson, but there will be data scientist positions for which you will not be qualified if this program is the extent of your data science training. That’s ok! It makes sense that individuals who have been learning and gaining experience for years will be better suited for higher level or more advanced data science positions! With more training and experience, you’ll get there too. For now though, it’s best to read job descriptions carefully. If it’s a machine learning-focused position and you are unfamiliar with most of the words in the job description, that’s probably not the right position for you. There’s no need to apply to that job. However, you do not need to have exactly every qualification and skill listed on the job posting. If you have most of the skills they’re requesting and are willing to learn the few with which you’re less familiar, it is ok to apply to that position! Thus, given the training in this course, you’ll want to focus on positions where data wrangling, data visualization, basic analysis, and report generation and presentations are the focus. These may have the title “data analyst”, “junior data scientist”, “entry level data scientist” or “data scientist.” Read the job descriptions carefully and decide if the job may be right for you! 91.0.2 Additional Considerations A final note on looking for jobs, you may have limitations that should be considered before you apply to a position. For example, if you don’t live in San Francisco and find a job posting for a job is in San Francisco, CA that cannot be done remotely, only apply to that job if you’re willing to move to San Francisco. If you’re limited geographically, there’s no need to waste your time applying or the company’s time reading your application if it’s a job you’ll never take. Consider whether or not you’re actually interested in the position or if working at the position is actually feasible before applying. 91.0.3 Job Titles to Search In the rest of this lesson we’ll focus in on where to search for positions and the strengths of each; however, before we do so, it’s important to note that data scientist positions do not always have the title data scientist. We’ve mentioned above that “data scientist,” “data analyst”, “junior data scientist”, and “entry level data scientist” may be great terms to search. But, data science positions are also sometimes “product analyst”,“quantitative analyst,” or “research scientist.” While you may not be qualified for every position with each of these titles, it’s good to search broadly. Search each of these terms, read a few job descriptions, and determine if there are positions with these job titles that match your skills. It’s best to have the widest pool of possible jobs before deciding where you’ll apply! 91.0.4 LinkedIn Jobs are also posted on LinkedIn. Similar to StackOverflow’s jobs, positions can be searched by title and Location. However, the jobs posted on LinkedIn will span many more fields, so there will be a larger pool of total jobs to go through. LinkedIn Jobs One advantage of jobs on LinkedIn is that your full LinkedIn profile will help LinkedIn identify jobs that may be of interest to you. You can search through their suggestions to see if any of them are a good fit! Jobs you may be interested in If you carry out a search, the results can be further filtered by “Location”, “Date Posted”, “Job Type”, “Industry”, and “Company” to help hone in on the best position for you. As with StackOverflow, it’s possible to create an alert that will update you whenever applicable positions are posted in the future! LinkedIn search results 91.0.5 Job Boards The final official platform to search for positions are on job boards. There are a number of websites where you should search for data science positions. A few or the most popular are indeed, Glassdoor, ZipRecruiter, and Monster. There will likely be overlap between the jobs posted on these sites and even with LinkedIn and StackOverflow; however, you wouldn’t want to miss the position that’s right for you because you didn’t do a quick search. Gather all the information you can about open positions on all the job boards as you start your search! 91.0.6 Company websites In addition to searching on these websites where jobs are posted, if there is a company you’re particularly interested in working for, check their website directly for position openings. They may not have anything open, but it doesn’t hurt to check! 91.0.7 Twitter The last suggestion we have for hearing about new positions is to keep an eye out on Twitter. There isn’t an official platform; however, data scientists and companies that hire data scientists will often tweet about open positions. By following data scientists on Twitter, being active on Twitter, and checking Twitter, you may hear about a position that you would have missed otherwise. Twitter for positions 91.0.8 Remote Work 91.0.9 Applying Once you’ve identified a number of positions to which you’ll apply, you’ll need to actually apply. The platform may look different from one site to the next but you’ll generally have to provide: Contact Information Materials (resume + cover letter) Additional information about the job + posting General Application 91.0.9.1 Contact Information Despite the fact that your contact information is on your resume, most online platforms will require you to enter this information separately 91.0.9.2 Materials These forms will also have a place for you to upload your resume and cover letter. A reminder here that these should ideally be in PDF format. 91.0.9.3 Additional Information You’ll also be asked frequently about how you heard about this position and information about whether or not you’re authorized to work in the United States. Provide this information along with any other optional information that they’ve requested and that you’re comfortable sharing. 91.0.9.4 Proofread Once you’ve entered all the information into the form and uploaded all necessary documents, look carefully over all the information you’ve provided. Verify that it’s correct and check for typos. Then, hit the “Apply” button to submit! 91.0.10 Summary In this lesson we covered what types of terms to use for job searches, where to carry out those searches, and generally how to apply to a job once you’ve found a good fit! 91.0.11 Additional Resources Advice For Applying To Data Science Jobs "],["where-to-find-remote-data-science-jobs.html", "Chapter 92 Where to Find Remote Data Science Jobs", " Chapter 92 Where to Find Remote Data Science Jobs One of the barriers to a job in data science has been that data science positions have historically required individuals to move to one of the tech hubs throughout the world, such as Silicon Valley in California, for their job. However, over time, there has been a move by many companies to allow for more remote work. This has sometimes meant that individuals could work from home a few days a week while requiring that individual physically be in the office for the rest of the time. But, there has also been a shift to allowing individuals to work remotely full-time. The course work has been developed so that it can be done from anywhere as we recognize that most individuals cannot simply uproot their lives and move to where the courses are being taught. Similarly, we wanted to dedicate a lesson to how to find remote data science jobs! 92.0.1 General Postings We’ll start this lesson by noting that remote jobs can be listed on any of the platforms discussed in the last lesson. The job description may mention “remote work possible” or may state the location of the job to be “Anywhere.” In these cases, a move will likely not be required should you get the job. For example, for the RStudio careers discussed in the last lesson, the position for each position posted is either anywhere in the USA or is not specified. While many companies have not yet shifted to allow remote work, some have. Identifying companies that do allow remote work, if that’s what you’re looking for, can be incredibly helpful. RStudio remote jobs Additionally, if you’re using any of the platforms discussed in the last lesson (i.e. StackOverflow, LinkedIn, etc.), you can often specify that you’re looking for remote positions in your search. For example, on StackOverflow you specify that the location is “remote,” you will get a list of remote positions that meet the rest of your search criteria. remote StackOverflow search The same goes for LinkedIn, where you can specify that you’re looking for remote positions. remote LinkedIn search 92.0.2 Remote OK In addition to finding remote work positions on the platforms we’ve previously discussed, RemoteOK is a site dedicated to hosting job postings that allow individuals to work remotely. Specifically, remote data science jobs can be found on this site at https://remoteok.io/remote-data-science-jobs. remoteok.io For example, here we have a job posting for a “Data Analyst” position at Komoot. The job posting looks pretty typical. There is an introduction to the company and a list of key responsibilities for the position. Komoot Data Analyst However, there is an additional section explaining why working for this company is a great experience. Among these is listed the fact that you can work from wherever you want. Work from home The job posting finishes as the others we’ve looked at have. In includes a section on the technical and business skills along with personality traits you’ll need to be successful at their company and finishes with what documents you’ll need to apply. skills, abilities, and application materials Thus, there won’t be much difference between the job postings that allow remote work and those positions that do not. However, if being able to work from home matters to you or if you are unable to re-locate, it is important that you focus your search to identify positions and companies that will allow this to be possible. 92.0.3 Summary In this lesson we discuss how and where to look for remote positions, should you be unable to relocate or prefer working from your own home. "],["data-science-interviews.html", "Chapter 93 Data Science Interviews", " Chapter 93 Data Science Interviews After applying to jobs, there’s a waiting period. The company has to look over the application materials they’ve received and decide who amongst their applicant pool they want to interview. After they decide if you’re one of the people they’ve decided to interview, they’ll reach out to you via email or phone call. During this phone call or in this email they’ll likely describe their interview process and schedule the next step. If they don’t explain the interview process on this initial call, you should ask. Understand the next steps Before moving on, it’s important to note that much of this lesson is based on this post from Emily Robinson, a data scientist at DataCamp. And, if you want to learn even more about the information in this lesson, read her entire post as well as the information in the other links provided at the end of this lesson! Emily Robinson’s Post 93.0.1 Pre-Interview Phone Calls Sometimes there are one or more phone interviews before the in-person interview. These calls are part of the interview process. You should prepare for them as best you can, just like you would for an in-person interview. The purpose of these phone calls is to ensure that you are who you say you are on your resume, to gauge your interest in this position, and to decide if the company wants to bring you in for an interview. Thus, before the phone call, be sure you know who it is you’ll be talking to, what that person does at the company, and do your best to prepare for questions they may ask you. We’ll discuss the types of interview questions you may get more in the next section. Additionally, always have questions to ask the person you’re talking to. If you have no questions at all, it will look like you don’t care about and are not that interested in the position. We’ll discuss these later in the lesson as well. If there are phone calls and they go well, then they’ll invite you for an interview! Be sure to write down the date and time of the interview and get details about where the interview will be taking place. If it’s in a big office building, be sure they’ll provide you with details on how to get to them within the building. 93.0.2 Interview Preparation Before the interview, you should be sent a schedule or at least explained the structure of the interview. Ideally you’ll have the names of and times at which you’ll be meeting people in the company. If you don’t receive this information a day or two in advance of your interview, you can reach out to your point of contact at the company (the person you chatted or emailed with to plan the interview, typically from HR (human resources)) and ask if one is available. 93.0.2.1 Practice With a schedule of who you’ll be meeting with or at least some idea of the structure of their interview process, it’s time to practice! While there’s no way to know exactly what you’ll be asked in the interview, there are a few general types of questions: Personality Questions - questions asked to learn more about your background, your behaviors, how you would handle certain situations, and about how you work with others Technical Questions - questions asked to gauge your technical competence For personality questions, practice by answering questions out loud. It’s always best to practice interviews the way they would actually happen. Writing the answers down or thinking about them is a good start, but it’s best to actually say them out loud to get more comfortable. These questions range from “Tell me about yourself” and “How did you get interested in data science?” to “What’s your greatest strength” and “How would you handle a conflict with a colleague?” Practice out loud As for technical questions, there are many different ways companies assess technical skills. They may explain a technical issue out loud and ask how you would solve it. They may give you a take-home analysis (we’ll discuss these in detail in the next lesson). They may have you live-code, meaning they may ask you to code in front of them. They may ask you to explain a concept (such as regression) to them. These are all fair game. Hopefully, with some idea as to the structure of the interview, you’ll have some idea as to what to expect. Many people have written and spoken about the types of data science questions they’ve asked and been asked in data science interviews previously. Here we list a few types of questions you could be asked: “Talk to me about [fill in the bank]…” “Write an algorithm that does [fill in the blank].” “What problems should you solve with [fill in the blank]?” “How would you approach this problem given this dataset?” Thus, it’s a really good idea to look at resources that are already out there. These are included again at the end of the lesson, but we’re including them directly in the lesson because they’re really important to read over and use when you’re preparing for interviews. Read and listen through these resources - there will be a few quiz questions from these resources at the end of the lesson. Twitter Thread about what questions to ask, from Jensen Harris Questions I’m Asking in Interviews, by Julia Evans VIDEO: Live Breakdown of Common Data Science Interview Questions &lt;- This one is almost an hour. Give yourself time to listen to this video. When you’re asked a question to which you don’t know the answer, don’t make one up. Rather, acknowledge that you don’t know the answer, but think critically about how you may go about solving the issue raised or question asked. Being able to think through a problem is an important skill for a data scientist, so don’t panic if there’s a question with which you’re less familiar. Finally, remember that it’s fair game for your interviewers to ask about anything on your resume. So, if there’s a project you worked on a while ago that’s listed, be sure you’re prepared to answer questions about or discuss it. Practice answering questions out loud. Practice coding. Practice explaining technical topics out loud. Be prepared to say “I don’t know.” This will all make the actual interview much easier. 93.0.2.2 Avoid Giving a Salary Number Sometimes HR or others will ask you what salary you’re looking for. Never volunteer your current salary or what you’re looking to make. If this number you offer is lower than what they intended to offer you, you may be getting a lower salary than you would have otherwise. If you’re asked to give a salary number say that you’re interested in learning about the position, the company, and responsibilities of the job and would rather discuss salary later. If you’re asked again, you can always offer a very big range based on industry standards but state that you’re really most interested in making sure the fit is right for you and would rather consider the entire compensation package rather than salary alone. Avoid giving a salary 93.0.2.3 Get Knowledge In addition to practicing your responses and practicing how to avoid giving a salary number if asked, you also need to gain even more knowledge about the company with which you’re interviewing. You probably learned a bit when deciding to apply to the job, but now is the time to go much deeper. Learn as much as you can about what the company does from their website. Search other websites (such as Glassdoor) to see what others have to say about the company and about working at the company (For example, information from hundreds of individuals who have interviewed for data science positions at Airbnb can be found here.) In addition to company knowledge about the company, you also want to know as much about the people you’ll be meeting with as possible. Check out their profiles on the company website, look at their Twitters, check their GitHub accounts, look for presentations they’ve given. Learn as much as you can about what each person does and have different questions ready for each individual. These questions can be about their experience at the company or about that individual’s work – if you listen to a presentation someone gave and have a question, you can ask that question! Learn about the company &amp; people 93.0.2.4 Ask Questions In your research about the company and its employees you should start to gather a list of questions that you want answers to on the day of the interview. These questions can discuss more technical aspects of the position, such as details of how work is done at the company (i.e.”How is code shared?” or “What languages are used by the team?” “When something goes wrong, how is it handled?”) and how projects are managed (i.e “Who assigns projects? How are they assigned?” or “How will my performance be evaluated?”). Alternatively, they could be questions about quality of life and work culture aimed at getting a better understanding of how much vacation time comes with the job, how often people work late or on the weekends, and whether or not working from home is an option. For a more complete list of questions one could ask at an interview, check out Julia Evans’ list here. 93.0.3 The Day of the Interview Once you’ve practiced answering questions and have a number of questions you’re interested in getting answered about the position and company, you’re almost ready for the big day! When it comes to the interview day, your goal should be to learn as much about them as they’re learning about you. Ultimately, the position should be a good fit for both of you. 93.0.3.1 Dress Deciding what to wear for a data science interview is not as straightforward as it is for other positions, where it’s often expected interviewees wear a suit. For data science positions, you likely do not have to wear a suit. That said, it’s always better to be overdressed than underdressed, so if you’re unsure, wear something more formal rather than less. Ideally, you’d be able to ask HR (or the person you first chatted with at the company) how applicants typically dress for interviews at their company. They probably get that question frequently and will likely be very helpful. If they say jeans are ok for interviews, they probably mean it. It’s up to you at that point to then decide what to wear. Ultimately, you want to put your best foot forward, so if they say casual is fine, believe them, but maybe go one step above jeans. Slacks of some sort (or a skirt or dress, if you’re more comfortable) may be the safest bet. Clothing should be clean and ironed if necessary. If they say casual, you likely do not need a tie, unless that’s what you’re most comfortable in! At the end of the day, you want to be comfortable and look professional in whatever you wear. So, only wear clothing that you’re comfortable in and that makes you feel good! Feel good in what you’re in 93.0.3.2 One-on-one Interviews After the initial phone call(s) and once you’re interviewing in person, you’ll likely have one or more one-on-one interviews. In these interviews, the person interviewing you will likely explain a little about the company, the position, and/or themselves before trying to gauge your background, your technical capabilities, and fit for the position. They’ll ask you questions and should give you time to ask them questions as well. In your questions, you should demonstrate that you have knowledge of the company and have done your homework, but should also get answers to everything you still want to know about the company and position. 93.0.3.3 Technical Assessment While technical questions will be asked in one-on-one interviews, there should be some coding required during a data science interview. This may require live-coding where another person will sit with you and watch you code or it may be a take-home analysis. Regardless of the format, this is where you want to show off your technical skills! Live-coding can be intimidating, but do your best to relax and just complete the task at hand. As for take-home analyses, we’ll spend more time discussing the details of these in the next lesson. While these may seem daunting, it’s an important part of a data science interview. In fact, if the interview process doesn’t require you to code at all, that may be a red flag that the position is maybe not exactly what you thought it was. Coding required 93.0.3.4 Communication In addition to learning about your background, interests, and technical skills, data science positions often require a lot of written and oral communication. As such, the interview may require you to generate a report or presentation using the results from the take-home analysis you did. Or, they may ask you to give a presentation on some of your past work. For this, prepare as much as possible. Practice your presentation if you’re given time to prepare at home. Proofread your report carefully before sending it back to the company. Know that this is an important part of their assessment of you and do your best to communicate your analysis and findings Effective communication 93.0.3.5 The End of the Interview At the end of your interview day (or two days), you’ll likely be exhausted. Your brain will be mostly used up and you’ll be excited about the position (if all went well!) but ready to go home. This is all normal. But, one piece of information you’ll want before leaving is what the next steps are. You’ll want to know whether or not you should expect to hear from them within the next two days, the next week, or if you should expect it to be longer than that. Get this information. If they don’t tell you, it’s ok to ask! It will bring you piece of mind during the waiting after the interview!. Next steps 93.0.4 After The Interview So, you’ve had the interview and now you’re waiting to see if you’ll get an offer. During this time, you have more to do than just wait. This is a time for self-reflection. If they offer you a position, do you want it? Is this somewhere you want to work? Will this be a good fit? Are you excited about the position and company? These are all important questions to ask yourself about the position before an offer or a rejection comes. 93.0.4.1 Rejection While we’re on the topic, rejection is normal. If you get rejected from a position or from many positions, know that data science is incredibly competitive. Everyone gets rejections. They aren’t fun, especially if they come from a company and position that was really interesting and exciting to you, but know that they are normal. If you are emailed a rejection, handle it appropriately. It’s ok to express disappointment “I was excited at the prospect of working with you all so I’m sorry to hear that” but always thank them for their time and consideration. Rejection 93.0.4.2 The Offer However, it’s not all gloom and doom! Sometimes, there’s a job offer at the end of the waiting game. In these cases, the offer may come by either phone or email. However it comes, always respond with excitement and thanking the individual for providing you with the offer. Never accept right away. If the offer comes on the phone verbally, always ask for the full offer in writing. And, after being given the full offer, always say that you need time to review it. This will give you time to think about it fully and in detail. Additionally, this will give you time to negotiate. Job Offer If, after this conversation, you realize you’re missing some details in the offer (health insurance, start date, etc.) you can ask for those details to have all the possible information you may need before responding to the offer. 93.0.4.3 Negotiations Once you have all the information you need to make a decision, it’s time for negotiations. While some people enjoy this stage, many people don’t. So, if you’re uncomfortable negotiating, know that you’re not alone and then negotiate anyway. Negotiate Tech companies anticipate negotiations. It is generally possible to get a 5-10% salary increase from the initial offer. Additionally, if stock options or a signing bonus are possibilities, these are also negotiable. Beyond money, there may be life benefits you’re interested in. Maybe you need to leave early on Fridays or want an additional week in vacation. These are all the types of things you can ask for during negotiations. That said, you should think hard about what you actually would want or need in order to accept their offer. If you ask for more salary and the ability to work from home once a week and they give both to you, there is an expectation that you’ll accept the offer. It would be in bad taste to come back with a new list of demands after they’ve already given you everything you initially asked for. Of course, you don’t have to accept at this point, but only negotiate for a position you’re actually considering accepting. At the end of the negotiations process, if you accept, you’ll have a new job at a new company! Congratulations! It’s a lot of work trying to get a job in data science and the road will often be filled with a lot of rejection. But, when it’s successful, you can end up in a position that is incredibly exciting and fulfilling! 93.0.5 Summary In this lesson, we’ve covered a lot about interviewing for a data science position. Some of the tips discussed are general interviewing guidelines while other are specific to data science positions. However, despite all the information in this lesson, we’ve only touched on the tip of the iceberg. In order to learn more about interviews, check out and read the information in the links below. And, good luck in your job search! 93.0.6 Additional Resources Advice for Applying to Data Science Jobs Red Flags in Data Science Interviews Twitter Thread about what questions to ask Questions I’m Asking in Interviews Ten Rules for Negotiating a Job Offer 15 Rules for Negotiating a Job Offer One person’s experience on the data science job market -Part 1 and Part 2 Two Sides of Getting a Job as a Data Scientist What We Learned Analyzing Hundreds of Data Science Interviews VIDEO: Live Breakdown of Common Data Science Interview Questions "],["data-science-meetups.html", "Chapter 94 Data Science Meetups", " Chapter 94 Data Science Meetups A lesson on attending data science meetups is included in this course since these gatherings can be great places to meet people in the field, to network, and possibly to find jobs. However, the main purpose of meetups is not to find a job, thus, we’ve included this lesson at the end of the course. Here, we suggest attending meetups if you’re looking for a job but more importantly before you start looking for a job. Having connections and a network to help you before you actually need one can be incredibly helpful. In this lesson, we’ll discuss where to look for data science meetups and what you can expect if you attend one. We’ll finish off by briefly discussing what data science meetups have to do with job searches 94.0.1 Where to Look Data Science Meetups in your area will generally be searchable on Meetup.com and more specifically at https://www.meetup.com/topics/data-science/. Data Science meetups are happening worldwide! Data Science Meetups If you sign up for an account on the site, Meetup.com will suggest groups for you to join and from upcoming meetups to attend! Suggested groups If you find a meeting you’re interested in going to at a time when you’re free, you can let the organizers know you’re going. Note that if you say you’re going and later decide you won’t be able to make it, be sure to go back in and change your response. It’s not fair to organizers to have people say they’re going and not be able to make it. Attend a meetup 94.0.2 What to Expect Once you’ve decided to attend a meetup, you’ve completed step 1. The next step is to show up. The final and most important step is to network and talk to others once you’re at the meetup. At most of these, there will be a lot of people who don’t know anyone, so know that you’re in good company. It can be difficult to strike up a conversation with others there, but remember, people are at these to meet others in their field or people with similar interests – they want to meet you and are probably also feeling awkward about striking up a conversation! But, since you’re already there, it’s best to put yourself out there and meet people! Often there will be a time to network and chat in the beginning. Then, there may be a presentation or two. Hopefully at the end there’s some more time to chat with others. Meetups are a great place to meet new people and to learn new things! 94.0.3 Meetups and Jobs So far we’ve discussed where to find out about what meetups there are in your area and about how you can learn new information and meet people with similar interests. However, this is a course about getting jobs. Well, admittedly, meetups are not the best place to look for jobs, but they are a great place to build a network and meet people. Meetups are most helpful in your job search if you start going to them and building up your network before you really need a job. After meeting people at these meetups, you can follow them on Twitter or exchange emails to discuss project ideas! This is a great step in networking! However, if you’re at a meetup and are looking for a job, that’s a good thing to bring up in conversation. If the people you’re chatting with know of any leads or know that their company is hiring, they’ll pass that information along to you! 94.0.4 Summary In this brief lesson we discussed the benefits of building your network, attending meetups, and getting to know people in your field in person! Check out the meetups in your area and start to meet people! "],["create-your-portfolio.html", "Chapter 95 Create Your Portfolio 95.1 Your objectives!", " Chapter 95 Create Your Portfolio In this chapter, we aren’t creating a formal project per se, but we are gathering all the work you’ve done through this course as well as other professionally relevant information about yourself and putting it together into a website! This will allow you to easily share with potential employers or collaborators the excellent work you have done! 95.1 Your objectives! To complete this project you’ll need to do a few things 95.1.1 Get the rest of your projects online on GitHub Create a new GitHub repository following the steps describe in the Creating a repository chapter. Keep this window open and available. Go to your DataTrail project RStudio project. Go to the Terminal pane and run the following commands: git init git add . These steps have now made this project a git project. Now we need to set up credentials so we can put your projects online! Follow the credentials set up described in this chapter. Return to the Terminal tab and run this: git commit -m &quot;add all files&quot; Tell your RStudio git project where to push your project online by running this command in the terminal, but replace the URL_TO_YOUR_REPOSITORY with the repository URL of your own GitHub repository: git remote set-url --push origin URL_TO_YOUR_REPOSITORY To push all the files here to your GitHub remote repository do this: git push --force Go to your GitHub repository to make sure the files got there. Turn on your GitHub pages for this repository by going to Settings &gt; Pages &gt; and Underneath Branch choose main. Click Save. All of your project htmls should be able to be seen using a URL that will look like this: https://cansavvy.github.io/datatrail_projects/01_Forming_Questions/first_project.nb.html To break this down, it will be like this: https://username.github.io/name_of_your_project_repository/section_folder/project_html_file_name.nb.html Have these links all together because now we will want to post them to your professional website! 95.1.1.1 Add links to your projects to your website! Go to the GitHub repository for the website you created in the Make your own website! chapter. Add a section that describes the DataTrail projects you have done. Make sure to feature your final project that you worked on in the previous chapter. Put links to all the projects you worked on for this course following the URL template we described above and using R Markdown syntax. Feel free to change them how you see fit! These are your projects now! [Description of project](URL YOUV&#39;E PIECED TOGETHER) Be proud of all the excellent work you have done! And now you have a way to show it off! "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Content Author(s) Shannon Ellis, Candace Savonen, Aboozar Hadavand, Leslie Myint, Leonardo Collado-Torres Content Contributor(s) Sarah McClymont, Leah Jager Content Editor(s)/Reviewer(s) Candace Savonen, Katherine Cox, Davon Person Content Director(s) Jeff Leek Production Content Publisher(s) Candace Savonen Technical Course Publishing Engineer(s) Candace Savonen Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright Package Developers (swirlify) Sean Kross Art and Design Illustrator(s) Shannon Ellis, Candace Savonen, Aboozar Hadavand Funding Funder(s) Johns Hopkins Bloomberg School of Public Health, Bloomberg Philanthropies Funding Staff Ashley K Johnson   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2023-01-04 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## bookdown 0.24 2022-02-15 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.0.2) ## hms 0.5.3 2020-01-08 [1] RSPM (R 4.0.0) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## knitr 1.33 2022-02-15 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.0.2) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## ottrpal 0.1.2 2022-02-15 [1] Github (jhudsl/ottrpal@1018848) ## pillar 1.4.6 2020-07-10 [1] RSPM (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.0.3) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## readr 1.4.0 2020-10-05 [1] RSPM (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2022-02-15 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2022-02-15 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2022-02-15 [1] Github (R-lib/testthat@e99155a) ## tibble 3.0.3 2020-07-10 [1] RSPM (R 4.0.2) ## usethis 2.1.5.9000 2022-02-15 [1] Github (r-lib/usethis@57b109a) ## vctrs 0.3.4 2020-08-29 [1] RSPM (R 4.0.2) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2022-02-15 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
